---
title: PyTorch - Neural Network Regression
date: 2022-04-17 23:06:17
permalink: /pages/9d7f40/
categories:
  - 机器学习八股文
  - PyTorch 101
tags:
  - 
---
### 1. Build dataset
我们创建一些假数据来模拟真实的情况. 比如一个一元二次函数: `y = a * x^2 + b`, 我们给 `y` 数据加上一点噪声来更加真实的展示它.

#### APIs:
- `torch.linspace()`: 类似 `np.linspace()`:
![](https://raw.githubusercontent.com/emmableu/image/master/202204172313700.png)
- `torch.unsqueeze()`:  把一维数据变成一个二维数据，例如：
```python
x = torch.tensor([1, 2, 3, 4])
torch.unsqueeze(x, 0) # return tensor([[ 1,  2,  3,  4]])
torch.unsqueeze(x, 1) 
# return 
# tensor([[ 1],
#         [ 2],
#         [ 3],
#         [ 4]])
```
- `torch.rand(*size)`: Returns a tensor filled with random numbers from a uniform distribution on the interval $[0, 1)$
	- The shape of the tensor is defined by the variable argument `size`.
```python
torch.rand(4) # tensor([ 0.5204,  0.2503,  0.3525,  0.5673])
torch.rand(2, 3)
# tensor([[ 0.8237,  0.5781,  0.6879],
# [ 0.3816,  0.7249,  0.0998]])
```

- `torch.randn(*size)`: Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution): $\text{out}_{i} \sim \mathcal{N}(0, 1)$
	- The shape of the tensor is defined by the variable argument `size`.

#### code:
```python
import torch
import matplotlib.pyplot as plt

x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)
y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)

# 画图
plt.scatter(x.data.numpy(), y.data.numpy())
plt.show()
```

### 2. Build Neural Network
建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性(`__init__()`), 然后再一层层搭建(`forward(x)`)层于层的关系链接.

#### API knowledge
- `torch.nn.Linear`: 
	- `net = torch.nn.Linear(2,1)`: This creates a network as shown below. Weight and Bias is set automatically.
	![](https://raw.githubusercontent.com/emmableu/image/master/202204172343665.png)
	- `print(m.weight, m.bias)` 会得到自动设置的weight 和 bias的值：
```python
Parameter containing:
tensor([[-0.0527, -0.0429]], requires_grad=True) 
Parameter containing:
tensor([0.2802], requires_grad=True)
```

- 如果是 neural network intro 里面的图，就是对应 
```python
self.hidden = torch.nn.Linear(1, 2)   # 隐藏层线性输出
self.predict = torch.nn.Linear(2, 1)  # 输出层线性输出
```

#### Code
```python
import torch
import torch.nn.functional as F     # 激励函数都在这

class Net(torch.nn.Module):  # 继承 torch 的 Module
	# 每一个 torch.nn.Module 都有init 和forward这两个功能
    def __init__(self, n_feature, n_hidden, n_output):
		# init的作用：提供搭建每一层所需要的信息。
        super(Net, self).__init__()     # 继承 __init__ 功能
        # 定义每层用什么样的形式
        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # 隐藏层线性输出
        self.predict = torch.nn.Linear(n_hidden, n_output)   # 输出层线性输出

    def forward(self, x):   # 这同时也是 Module 中的 forward 功能
        # forward的作用：搭流程图，把init中的信息转成流程图中的信息
        # 正向传播输入值, 神经网络分析出输出值
        x = F.relu(self.hidden(x))      # 激励函数(隐藏层的线性值)
        x = self.predict(x)             # 输出值
        return x

net = Net(n_feature=1, n_hidden=10, n_output=1)


print(net)  # net 的结构
"""
Net (
  (hidden): Linear (1 -> 10)
  (predict): Linear (10 -> 1)
)
"""
```

### 3. Train the Neural Network
```python
# optimizer 是训练的工具
optimizer = torch.optim.SGD(net.parameters(), lr=0.2)  # 传入 net 的所有参数, 学习率
loss_func = torch.nn.MSELoss()      # 预测值和真实值的误差计算公式 (均方差)

for t in range(100):
    prediction = net(x)     # 喂给 net 训练数据 x, 输出预测值

    loss = loss_func(prediction, y)     # 计算两者的误差

    optimizer.zero_grad()   # 清空上一步的残余更新参数值，把所有参数的梯度全部都设为0，重新做反向传递
    loss.backward()         # 误差反向传播, 计算参数更新值
    optimizer.step()        # 以学习率0.2 将参数更新值施加到 net 的 parameters 上
```