---
title: Graph in PyTorch
date: 2022-04-10 17:45:41
permalink: /pages/8976a5/
categories:
  - 机器学习八股文
  - PyTorch 101
tags:
  - 
---
resource: https://zhuanlan.zhihu.com/p/33378444

## **一、构建计算图**
**pytorch**是动态图机制，所以在训练模型时候，每迭代一次都会构建一个新的计算图。而计算图其实就是代表程序中变量之间的关系。举个列子：$$y = (a + b)(c + d)$$在这个运算过程就会建立一个如下的计算图：
![](https://raw.githubusercontent.com/emmableu/image/master/202204101748967.png)
在这个计算图中，节点就是参与运算的变量，在**pytorch**中是用[Variable()](https://link.zhihu.com/?target=http%3A//pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html%23sphx-glr-beginner-blitz-autograd-tutorial-py)变量来包装的，而图中的边就是变量之间的运算关系，比如：[torch.mul()](https://link.zhihu.com/?target=http%3A//pytorch.org/docs/master/torch.html%3Fhighlight%3Dmul%23torch.mul)，[torch.mm()](https://link.zhihu.com/?target=http%3A//pytorch.org/docs/master/torch.html%3Fhighlight%3Dmm%23torch.mm)，[torch.div()](https://link.zhihu.com/?target=http%3A//pytorch.org/docs/master/torch.html%3Fhighlight%3Ddiv%23torch.div) 等等。

**注意**图中的 [leaf_node](https://link.zhihu.com/?target=http%3A//pytorch.org/docs/master/autograd.html%3Fhighlight%3Dis_leaf)，叶子结点就是由用户自己创建的**Variable**变量，在这个图中仅有**a，b，c** 是 [leaf_node](https://link.zhihu.com/?target=http%3A//pytorch.org/docs/master/autograd.html%3Fhighlight%3Dis_leaf)。为什么要关注leaf_node？因为在网络backward时候，需要用链式求导法则求出网络最后输出的梯度，然后再对网络进行优化，如下就是网络的求导过程。
![](https://raw.githubusercontent.com/emmableu/image/master/202204101749158.png)

## **二、图的细节**。

最简单的backward算法举例：
```python
from torch.autograd import Variable  
tensor = torch.FloatTensor([[1,2]])  
var = Variable(tensor, requires_grad=True)
# requires_grad: 如果需要为张量(tensor)计算梯度(gradient)，则为True，否则为False。
v_out = torch.mean(torch.mm(var))  
# print(v_out): v_out = (1 * 1 + 2 * 2)/2 = 2.5
v_out.backward() # 也就是求gradient，公式见如下, .backward() 返回值为None，但会更改var
# print(var.data) 得到gradient
```

`backward`: 计算gradient
$$
x_1 = 1, x_2 = 2 
$$
f 即`torch.mean(torch.mm(var))`这个函数
$$
f = (x_1 ^2 + x_2^2)/ 2
$$
v_out.backward()








pytoch构建的计算图是动态图，为了节约内存，所以每次一轮迭代完之后计算图就被在内存释放，所以当你想要多次**backward**时候就会报如下错：

```python
net = nn.Linear(3, 4)  # 一层的网络，也可以算是一个计算图就构建好了
input = Variable(torch.randn(2, 3), requires_grad=True)  # 定义一个图的输入变量
output = net(input)  # 最后的输出
loss = torch.sum(output)  # 这边加了一个sum() ,因为被backward只能是标量
loss.backward() # 到这计算图已经结束，计算图被释放了
```


上面这个程序是能够正常运行的，但是下面就会报错

```python
net = nn.Linear(3, 4)
input = Variable(torch.randn(2, 3), requires_grad=True)
output = net(input)
loss = torch.sum(output)

loss.backward()
loss.backward()

RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed.
```

之所以会报这个错，因为计算图在内存中已经被释放。但是，如果你需要多次**backward**只需要在第一次反向传播时候添加一个标识，如下：

```python
net = nn.Linear(3, 4)
input = Variable(torch.randn(2, 3), requires_grad=True)
output = net(input)
loss = torch.sum(output)
loss.backward(retain_graph=True) # 添加retain_graph=True标识，让计算图不被立即释放
loss.backward()
```

这样在第一次backward之后，计算图并不会被立即释放。

读到这里，可能你对计算图中的**backward**还是一知半解。例如上面提过**backward**只能是标量。那么在实际运用中，如果我们只需要求图中某一节点的梯度，而不是整个图的，又该如何做呢？下面举个例子，列子下面会给出解释。

```python
x = Variable(torch.FloatTensor([[1, 2]]), requires_grad=True)  # 定义一个输入变量
y = Variable(torch.FloatTensor([[3, 4],        
                                [5, 6]]))
loss = torch.mm(x, y)    # 变量之间的运算
loss.backward(torch.FloatTensor([[1, 0]]), retain_graph=True)  # 求梯度，保留图                                    
print(x.grad.data)   # 求出 x_1 的梯度
x.grad.data.zero_()  # 最后的梯度会累加到叶节点，所以叶节点清零
loss.backward(torch.FloatTensor([[0, 1]]))   # 求出 x_2的梯度
print(x.grad.data)        # 求出 x_2的梯度
```

结果如下：

```py3tb
3  5
[torch.FloatTensor of size 1x2]

 4  6
[torch.FloatTensor of size 1x2]
```

可能看到上面例子有点懵，用数学表达式形式解释一下，上面程序等价于下面的数学表达式：
![](https://raw.githubusercontent.com/emmableu/image/master/202204101757026.png)
