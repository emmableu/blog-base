


## RNN - 循环神经网络与前馈神经网络相比有什么特点？ 236 ★☆☆☆☆

![](https://raw.githubusercontent.com/emmableu/image/master/202209220117377.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209220117767.png)
## RNN - 循环神经网络为什么会出现梯度消失或梯度爆炸？有哪些改进方案？ 238 ★★☆☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209220118768.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209220119075.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209220119783.png)
下面讲的是梯度爆炸/消失导致的一个后果：

### The Problem of Long-Term Dependencies

RNNs 其中一个有吸引力的地方是能够将以前的信息和现在的任务联系起来，例如使用视频中的前几帧信息可能会对当前帧的理解有帮助。如果 RNNs 能够做到这些，那么他们就是非常有用的。但是他们能吗？这不一定。

有时，我们可能只需要最近的信息来完成当前任务。例如，考虑一个试图基于前面的词来预测下一个词的语言模型，如果我们试图预测 “the clouds are in the _sky_” 这句话中的最后一个词，那么我们就不需要更多的信息，很明显下一个词就是 sky。在这种情况下，相关信息和需要的地方（_the place that it’s needed_）之间的差距很小，那么这时候 RNNs 就可以学习到使用过去的信息。（_译者注：也就是短期依赖_）

![](https://i.imgur.com/HAvvUQV.png)

但是也有其他情况是我们需要更多信息的。考虑我们需要预测 “I grew up in France… I speak fluent _French_” 这句话中的最后一个词。最近的信息表明这个词应该是一个语言的名字，但是如果我们想要知道哪个语言，那么我们需要结合更前面的 France 这个背景。这时相关信息和需要的点（_the point where it is needed_）之间的差距就会变得非常大。

然而不幸的是，随着这个差距的增大，RNNs 越来越难以学习使用以前的信息。

![](https://i.imgur.com/Whfo6UB.png)

理论上来说，RNNs 完全可以处理这种“长期依赖”（_long-term dependencies_）。一个人可以很仔细的选择参数来解决这种形式的小问题（_toy problems_）。不过实际上，RNNs 似乎并不能学习到这种长期依赖。[Hochreiter (1991) [German]](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf) 和 [Bengio, et al. (1994)](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf) 曾经深入探讨了这个问题，发现了一些相当根本的原因。
　  


## RNN 原理

每一个层：用了昨天的prediction和事实两个信息

![](https://raw.githubusercontent.com/emmableu/image/master/202209251553805.png)


举例：用下面的input，写一个good children's book

![](https://raw.githubusercontent.com/emmableu/image/master/202209251555275.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209251556495.png)

rnn 会强调 人名 + saw， saw + 人名 的 pattern，所以会得到这样的结果：

![](https://raw.githubusercontent.com/emmableu/image/master/202209251602869.png)



