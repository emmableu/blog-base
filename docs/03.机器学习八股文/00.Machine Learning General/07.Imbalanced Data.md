---
title: Imbalanced Data
date: 2021-11-08 21:56:10
permalink: /pages/57fdff/
categories:
  - 机器学习八股文
  - Machine Learning General
tags:
  - 
---
## Ways to deal with imbalanced data:
这里assume 正例为少的那个data

### 1. Undersampling: 去除一些反例
下采样(under-sampling)通过减少分类中多数样本的数量来均衡样本的结构。最简单粗暴的方法是随机删掉一些多数类样本，代价是删除样本的同时也有可能删除了一些信息，通常使用的方法有：

#### clustering
对于多数类样本，计算K近邻的空间距离(如欧式距离)，用K近邻的重心(centroid)代替原本的K个样本，形成新的多数类样本组。因此聚类最终的样本量由少数类样本的数量决定。

#### Tomek links

Tomek links是指相反类样本的配对，这样的配对距离非常近，也就是说这样的配对中两个样本的各项指标都非常接近，但是属于不同的类。如图所示，这一方法能够找到这样的配对，并删除配对中的多数类样本。经过这样的处理，两类样本之间的分界线变得更加清晰，使少数类的存在更加明显。
![](https://raw.githubusercontent.com/emmableu/image/master/imbalanced-data-0.png)
### 2. Oversampling: 增加一些正例
上采样(over-sampling)是常用的应对不均衡数据的方法,通过增加分类中少数类样本的数量来均衡数据结构。因为其常用，也发展出了很多不同的上采样技术。
#### 复制
最简单的上采样技术是随机有放回地抽取少数量样本，复制一份后加入总样本中。但是如果数据的特征维度较小，简单的抽取复制容易造成过拟合(over-fitting)。经过多年的发展，上采样有以下几种常用的技术来避免过拟合：
#### 合成新样本：SMOT (Synthetic Minority Over-sampling Technique)及其衍生技术
严格来讲，SMOT不应该放在重采样中，因为操作不涉及重复采样，而是依据现有的少数类样本人为制造一些新的少数类样本，放在这里是因为SMOT及其衍生技术也是上采样的一种方式。

对于少数类样本，SMOT在p个维度上找到K近邻，利用这K个近邻的各项指标，乘上一个0到1之间的随机数，就能组成一个新的少数类样本。因此也很容易发现，SMOT只能够生成少数类样本凸包(convex hall)内的新样本，而永远不可能生成“离群”的样本。
### 3. 调整损失函数
调整损失函数的目的本身是为了使模型对少数量样本更加敏感。训练任何一个机器学习模型的最终目标是损失函数(loss function)的最小化，如果能够在损失函数中加大错判少数类样本的损失，那么模型自然而然能够更好地识别 出少数类样本。

在损失函数上给错判的少数类样本加上一个惩罚系数，加入损失函数中。这样在训练过程中，模型会自然而然地对少数类样本更为敏感，比如penalized-SVM和penalized-LDA这样的方法。
### 4. Threshold-moving: 阈值移动
如题
