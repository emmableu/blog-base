[source](https://everdark.github.io/k9/notebooks/ml/learning_to_rank/learning_to_rank.html)

In this session, we introduce learning to rank (LTR), a machine learning sub-field applicable to a variety of real world problems that are related to ranking prediction or candidate recommendation.
We will walk through the evolution of LTR research in the past two decades, illustrate the very basic concept behind the theory.
In the end we will also live-demo how we can quickly build a proof-of-concept LTR model using real world data and two of the state-of-the-art machine learning open source libraries.

## Outline

+ Introduction to Learning-to-Rank (LTR)
    + What is LTR and what's the difference between it and other ML models?
    + The classical problem (And also the non-classical ones)
    + Different types of LTR modeling approach
+ How to Evaluate a Ranking Model?
+ The Evolution of mainstream LTR
    + RankNet -> LambdaNet -> LambdaMART -> LambdaLoss
+ Demo with the go-to open source libraries
    + LambdaMART with `lightgbm` (Gradient Boosting Trees)
    + Listwise LTR with `tensorflow` (Deep Neural Nets)


## What is Learning to Rank (LTR)?
> Learning to rank refers to machine learning techniques for training a model to solve a ranking task. Usually it is a supervised task and sometimes semi-supervised.

We try to learn a function $f(q, D)$,
given a query $q$ and a relevant list of items $D$,
to predict the order (ranking) of all items within list.

## A Classical Problem in LTR

### Web Search Ranking

*Given a search query, rank the relevance of the resulting matched document URLs, such that more relevant document should be presented first to the user.*

More formally, we depict the above problem as the following task:

Given a query $q$, 
and the resulting $n$ documents $D = {d_1, d_2, ..., d_n}$,
we'd like to learn a function $f$ such that $f(q, D)$ will predict the relevance of any given document associated with a query.
Ideally, $f(q, D)$ should return an ordered list of documents $D^*$, ranked from the most to least relevant to the given query $q$.

Popular web search datasets for large-scale LTR benchmark:
+ [Microsoft LTR Datasets](https://www.microsoft.com/en-us/research/project/mslr/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fprojects%2Fmslr%2F)
+ [Yahoo LTR Datasets](https://webscope.sandbox.yahoo.com/catalog.php?datatype=c)


## Non-Classical Problems in LTR

LTR is a general approach for solving ranking task.
Here are some examples other than just web search ranking.
Note that not all of them are obviously a ranking task in the first glance.

+ **Recommender system** (Solve personal product perference ranking)
+ **Stock portfolio selection** (Solving equity return ranking)
+ **Message auto reply** (Solving best-candidate ranking in email/message reply recommendation)
+ **Image to text** (Solving best-candidate contextual feature)

## General Types of LTR algorithm:

+ Pointwise
+ Pairwise
+ Listwise

They are distinguished by how we formulate the **loss function** in the underlying machine learning task.

**The Ranking Task**

Given a query $q$, 
and the resulting $n$ document $D = {d_1, d_2, ..., d_n}$,
we'd like to learn a function $f$ such that $f(q, D)$ will predict the relevance of any given document associated with a query.

## Pointwise LTR

In pointwise approach, the above ranking task is re-formulated as a regression (or classification) task.
The function to be learned $f(q, D)$ is simplied as $f(q, d_i)$.
That is, the relevance of each document given a query is scored independently.
In recent literature $f(q, d_i)$ is called pointwise scoring function, 
while $f(q, D)$ is refered to as groupwise scoring function.

If we have two queries associated with 2 and 3 resulting matching documents, respectively:

$$
\begin{align}
q_1 & \rightarrow d_1, d_2 \\
q_2 & \rightarrow d_3, d_4, d_5
\end{align}
$$

Then the training examples $x_i$ in a pointwise framework will decouple them into every query-document pair:

$$
x_1: q_1, d_1 \\ 
x_2: q_1, d_2 \\
x_3: q_2, d_3 \\
x_4: q_2, d_4 \\
x_5: q_2, d_5
$$

Since each document is indeed scored independently with the absolute relevance as the target label (could be real-valued in order or simply binary), 
**the task is entirely no difference than a traditional regression or classification task.**
Any such machine learning algorithm can be applied to pointwise solution.

+ Pros: 
    + Simplicity. Existing ML models are ready to apply.
+ Cons: 
    + The result is usually sub-optimal due to not utilizing the full information in the entire list of matching documents for each query.
    + Explicit pointwise labels are required to constitute the training dataset.

**The Ranking Task**

Given a query $q$, 
and the resulting $n$ document $D = {d_1, d_2, ..., d_n}$,
we'd like to learn a function $f$ such that $f(q, D)$ will predict the relevance of any given document associated with a query.

## Pairwise LTR

In pairwise approach, we are still trying to learn the *pointwise* scoring function $f(q, d_i)$, 
however, our training examples are now consructed by pairs of documents within the same query:

$$
x_1: q_1, (d_1, d_2) \\ 

x_2: q_2, (d_3, d_4) \\

x_3: q_2, (d_3, d_5) \\

x_4: q_2, (d_4, d_5)
$$

Given such setup, a new set of pairwise BINARY labels can be derived, by simply comapring the individual relevance score in each pair.
For example, given the first query $q_1$, if $y_1 = 0$ (totally irrelevant) for $d_1$ and $y_2 = 3$ (highly relevant) for $d_2$, then we have a new label $y_1 < y_2$ for the document pair $(d_1, d_2)$.
**Now the problem has become a binary classification learning task.**

In order to learn the still-pointwise function $f(q, d_i)$ in a pairwise manner, we model the score difference probablistically:

$$
Pr(i \succ j) \equiv \frac{1}{1 + exp^{-(s_i - s_j)}}
$$

In plain words, if document $i$ is better matched than document $j$ (which we denote as $i \succ j$), 
then the probability of the scoring function to have scored $f(q, d_i) = s_i$ higher than $f(q, d_j) = s_j$ should be close to 1.
Put it differnetly, the model is trying to learn, given a query, how to score a pair of document such that a more relevant document should be scored higher.

+ Pros: 
    + The model is learning how to rank directly, even though only in a pairwise manner, but in theory it can approximate the performance of a general ranking task given N document in a matched list.
    + We don't need explicit pointwise labels. Only pairwise preferences are required. This is an advantage because sometimes we are only able to infer the pairwise preference from collected user behavior.
+ Cons: 
    + Scoring function itself is still pointwise, meaning that relative information in the feature space among different documents given the same query is still not fully exploited.