## Definition:
- What t-SNE does is find a way to project data into a low dimensional space, so that the clustering in the high dimensional space is preserved

## process

![](https://raw.githubusercontent.com/emmableu/image/master/202210061845422.png)

 1. we start with putting the points to random points in low dimensional space (on the line)
 2. at each step, a point in the new dimensional space (on the line) is attracted to points it is near in the original space (the scatter point), and repelled by points it is far from.

## detailed process
# How t-SNE works?

## Probability Distribution

Let’s start with **SNE** part of t-SNE. I’m far better with explaining things visually so this is going to be our dataset:

![](https://miro.medium.com/max/1400/0*NYve_va3wHU4zNnj.png)

It has 3 different classes and you can easily distinguish them from each other. The first part of the algorithm is to create a **probability distribution** that represents similarities between neighbors. What is “similarity”? [Original paper](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) states “ **similarity of datapoint** xⱼ **to datapoint** xᵢ **is the conditional probability** p_{j|i}**, that** xᵢ **would pick** xⱼ **as its neighbor** “.

![](https://miro.medium.com/max/1400/0*ETuCH0wXgSyit1i3.png)

We’ve picked one of the points from the dataset. Now we have to pick another point and calculate Euclidean Distance between them |xᵢ — xⱼ|

![](https://miro.medium.com/max/1400/0*g1bTTv6Wu632piCu.png)

The next part of the original paper states that it has to be **proportional to probability density under a Gaussian centered at** xᵢ. So we have to generate Gaussian distribution with mean at xᵢ_,_ and place our distance on the X-axis.

![](https://miro.medium.com/max/1400/0*j6P77qstfwQ6mkT8.png)

Right now you might wonder about _σ²_ (variance) and that’s a good thing. But let’s just ignore it for now and assume I’ve already decided what it should be. After calculating the first point we have to do the same thing for every single point out there.

![](https://miro.medium.com/max/1400/0*Afr8xsKrl6dwZ10Q.png)

You might think, we’re already done with this part. But that’s just the beginning.

## Scattered clusters and variance

Up to this point, our clusters were tightly bounded within its group. What if we have a new cluster like that:

![](https://miro.medium.com/max/1400/0*nZrA0hPQqM1Ce7-j.png)

We should be able to apply the same process as before, shouldn’t we?

![](https://miro.medium.com/max/1400/0*0MzS6wydzd5Dr02Y.png)

We’re still not done. You can distinguish between similar and non-similar points but absolute values of probability are much smaller than in the first example (compare Y-axis values).

![](https://miro.medium.com/max/1400/0*kpctqZwCMDkqJK3u.png)

We can fix that by dividing the current projection value by the sum of the projections.

![](https://miro.medium.com/max/526/1*1gBOzGPwWEN4L_HhYLN-VQ.png)

Which if you apply to the first example will look sth like:

![](https://miro.medium.com/max/1228/1*r3tuMMndpFZswRfIVNCEaw.png)

And for the second example:

![](https://miro.medium.com/max/1262/1*9XfcJTE-gFjGxFgdqOQKsQ.png)

This scales all values to have a sum equal to 1. It’s a good place to mention that p_{i|i}​ is set to be equal to 0, not 1.

![](https://miro.medium.com/max/228/1*2oCxcLq7hvxQcrjmFiClMA.png)

## Dealing with different distances

If we take two points and try to calculate conditional probability between them then values of p_{i|j}​ and p_{j|i}​ will be different:

![](https://miro.medium.com/max/1400/0*pTTqRArwYV_tGnF0.png)

![](https://miro.medium.com/max/1400/0*-JXLaDNYjjSSGVdf.png)

The reason for that is because they are coming from two different distributions. Which one should we pick to the calculation then?

![](https://miro.medium.com/max/336/1*gFbUxuUZlcRhIzYXvYzUCA.png)

Where _N_ is a number of dimensions.

## The lie :)

Now when we have everything scaled to 1 (yes, the sum of all equals 1), I can tell you that I wasn’t completely honest about while the process with you :) Calculation all of that would be quite painful for the algorithm and that’s not what exactly is in [t-SNE paper](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf).

![](https://miro.medium.com/max/736/1*J4lRX3F6qR9TF9VgH6Vd1Q.png)

This is an original formula to calculate p_{j|i}. Why did I lie to you? First, because it’s easier to get an intuition about how it works. Second, because I was going to show you the trough either way.

## Perplexity

If you look at this formula. You can spot that our

![](https://miro.medium.com/max/226/1*2q0ECctHTmnbCop71LlJOQ.png)

is

![](https://miro.medium.com/max/446/1*3_qQH7KjQR89ymcDk0Y5Yw.png)

If I would show you this straight away, it would be hard to explain where _σ²_ is coming from and what is a dependency between it and our clusters. Now you know that variance depends on Gaussian and the number of points surrounding the center of it. This is the part where **perplexity** value comes. A perplexity is more or less a target number of neighbors for our central point. Basically, the higher the perplexity is the higher value variance has. Our “red” group is close to each other and if we set perplexity to 4, it searches the right value of to “fit” our 4 neighbors. If you want to be more specific then you can quote the original paper:

> _SNE performs a binary search for the value of sigma that produces probability distribution with a fixed perplexity that is specified by the user_

![](https://miro.medium.com/max/496/1*Csv1yl-2zOxC42wV-WGVkQ.png)

Where

![](https://miro.medium.com/max/292/1*jhLo78eY9Jky4vMP42RC8Q.png)

​ is **Shannon entropy**. But unless you want to implement t-SNE yourself, the only thing you need to know is that perplexity you choose is positively correlated with the value of \mu_i_μi_​ and for the same perplexity you will have multiple different \mu_i_μi_​, base on distances. Typical perplexity value ranges between 5 and 50.

## Original formula interpretation

![](https://miro.medium.com/max/736/1*J4lRX3F6qR9TF9VgH6Vd1Q.png)

When you look on this formula you might notice that our Gaussian is converted into

![](https://miro.medium.com/max/436/1*xi6CUrwPLG4-1CZwt-hX0Q.png)

Let me show you how that looks like:

![](https://miro.medium.com/max/1400/0*sNHrck20Xt7uS7X9.png)

If you play with _σ²_ for a while you can notice that the blue curve remains fixed at point _x_=0. It only stretches when _σ²_ increases.

![](https://miro.medium.com/max/1400/0*SQWPC3TlgUqnsuSu.png)

That helps distinguish neighbor’s probabilities and because you’ve already understood the whole process you should be able to adjust it to new values.

## Create low-dimensional space

The next part of t-SNE is to create low-dimensional space with the same number of points as in the original space. Points should be spread randomly on a new space. The goal of this algorithm is to find similar probability distribution in low-dimensional space. The most obvious choice for new distribution would be to use Gaussian again. That’s not the best idea, unfortunately. One of the properties of Gaussian is that it has a “short tail” and because of that it creates a **crowding problem**. To solve that we’re going to use **Student t-distribution** with a single degree of freedom. More of how this distribution was selected and why Gaussian is not the best idea you can find in the [paper](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf). I decided not to spend much time on it and allow you to read this article within a reasonable time. So now our new formula will look like:

![](https://miro.medium.com/max/632/1*ZDiRzmCfK1xuCzldJ8-tvQ.png)

instead of:

![](https://miro.medium.com/max/690/1*Hax1tT4LMqH9RqUN4d7ulA.png)

If you’re more “visual” person this might help (values on X-axis are distributed randomly):

![](https://miro.medium.com/max/1400/0*z_xyrUJsAlkdKNjX.png)

![](https://miro.medium.com/max/1400/0*SEhDsFAK-qH6fLJv.png)

Using Student distribution has exactly what we need. It “falls” quickly and has a “long tail” so points won’t get squashed into a single point. This time we don’t have to bother with _σ²_ because we don’t have one in q_{ij} formula. I won’t generate the whole process of calculating q_{ij} because it works exactly the same as p_{ij}. Instead, just leave you with those two formulas and skip to sth more important:

![](https://miro.medium.com/max/726/1*l4Gjd2F_cnZQDvSi6zlu1w.png)

# Gradient descent

To optimize this distribution t-SNE is using [**Kullback-Leibler divergence**](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the conditional probabilities p_{j|i} and q_{j|i}

![](https://miro.medium.com/max/846/1*xi-IjvMSJmNu-jfHlZ0qvA.png)

I’m not going through the math here because it’s not important. What we need is a derivate for (it’s derived in **Appendix A** inside the [original paper](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)).

![](https://miro.medium.com/max/938/1*WK-kP2JJsAbYgw49hQENhA.png)

You can treat that gradient as repulsion and attraction between points. A gradient is calculated for each point and describes how “strong” it should be pulled and the direction it should choose. If we start with our random 1D plane and perform gradient on the previous distribution it should look like this.

![](https://miro.medium.com/max/1400/0*gx5m_CS7gVUn8WLH.gif)

Ofc. this is an exaggeration. t-SNE doesn’t run that quickly. I’ve just skipped a lot of steps in there to make it faster. Besides that, the values here are not completely correct, but it’s good enough to show you the process.

## Tricks (optimizations) done in t-SNE to perform better

t-SNE performs well on itself but there are some improvements allow it to do even better.

## Early Compression

To prevent early clustering t-SNE is adding L2 penalty to the cost function at the early stages. You can treat it as standard regularization because it allows the algorithm not to focus on local groups.

## Early Exaggeration

This trick allows moving clusters of (q_{ij}​) more. This time we’re multiplying p_{ij}​ in early stages. Because of that clusters don’t get in each other’s ways.


# Conclusions

t-SNE is a great tool to understand high-dimensional datasets. It might be less useful when you want to perform dimensionality reduction for ML training (cannot be reapplied in the same way). It’s not deterministic and iterative so each time it runs, it could produce a different result. But even with that disadvantages it still remains one of the most popular method in the field.