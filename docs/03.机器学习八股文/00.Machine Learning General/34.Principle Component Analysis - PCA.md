---
title: Principle Component Analysis - PCA
date: 2022-09-29 13:39:59
permalink: /pages/0d8e27/
categories:
  - æœºå™¨å­¦ä¹ å…«è‚¡æ–‡
  - Machine Learning General
tags:
  - 
---
## Definition of PCA
A method to reduce dimension (reduce the amount of features) by finding the top few axis that capture the maximum amount of variance in the data.  The top few new axis are found by performing eigendecomposition of the covariance matrix, and find the eigenvectors that corresponds to the highest few eigenvalues. 

## A Summary of the PCA Approach

-   Standardize the data.
-   Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Value Decomposition.
-   Sort eigenvalues in descending order and choose the k eigenvectors that correspond to the k largest eigenvalues where k is the number of dimensions of the new feature subspace (kâ‰¤d).
-   Construct the projection matrix W from the selected k eigenvectors.
-   Transform the original dataset X via W to obtain a k-dimensional feature subspace Y.

## Eigenvectors, Eigenvalues
[source](https://guzintamath.com/textsavvy/2018/05/26/eigenvalues-and-eigenvectors/)

![](https://raw.githubusercontent.com/emmableu/image/master/202209300052071.png)

Î» å°±æ˜¯ eigenvalue., [r1, r2] å°±æ˜¯eigenvector

## eigenvalue decomposition
eigendecomposition, or eigenvalue decomposition, is the factorization of a matrix A, using the function Ax = Î»x, where Î» is eigenvalue and x is eigen vector.


å†ä¸¾å¦å¤–ä¸€ä¸ªä¾‹å­ï¼š
![](https://raw.githubusercontent.com/emmableu/image/master/202209300057131.png)
we calculate the eigenvalues to be Î»1=âˆ’2 and Î»2=âˆ’1. And the corresponding eigenvectors are of the form (1,âˆ’2) and (âˆ’1,1), respectively.

![](https://raw.githubusercontent.com/emmableu/image/master/202209300058950.png)


## eigendecomposition v.s. SVD
[source](https://math.stackexchange.com/questions/320220/intuitively-what-is-the-difference-between-eigendecomposition-and-singular-valu)


Consider the eigendecomposition ğ´=ğ‘ƒğ·ğ‘ƒâˆ’1 and SVD ğ´=ğ‘ˆÎ£ğ‘‰âˆ—

Some key differences are as follows:
-   The vectors in the eigendecomposition matrix ğ‘ƒ are not necessarily orthogonal, so the change of basis isn't a simple rotation. On the other hand, the vectors in the matrices ğ‘ˆ and ğ‘‰-   in the SVD are orthonormal, so they do represent rotations (and possibly flips).
-   In the SVD, the nondiagonal matrices ğ‘ˆ and ğ‘‰ are not necessairily the inverse of one another. They are usually not related to each other at all. In the eigendecomposition the nondiagonal matrices ğ‘ƒ and ğ‘ƒâˆ’1-   are inverses of each other.
-   In the SVD the entries in the diagonal matrix Î£ are all real and nonnegative. In the eigendecomposition, the entries of ğ·-   can be any complex number - negative, positive, imaginary, whatever.
-   The SVD always exists for any sort of rectangular or square matrix, whereas the eigendecomposition can only exists for square matrices, and even among square matrices sometimes it doesn't exist.