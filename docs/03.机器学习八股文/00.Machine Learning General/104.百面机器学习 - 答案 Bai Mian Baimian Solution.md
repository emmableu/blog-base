---
title: 百面 答案 Bai Mian Baimian Solution
date: 2022-09-14 00:15:10
permalink: /pages/e1bf02/
categories:
  - 机器学习八股文
  - Deep Learning Problems
tags:
  - 
---
## 第1章 特征工程

### 为什么需要对数值类型的特征做feature scaling？002 ★☆☆☆☆
Feature scaling，常见的提法有“特征归一化”(normalization)、“标准化”(standardization)，是数据预处理中的重要技术，有时甚至决定了算法能不能work以及work得好不好。谈到feature scaling的必要性，最常用的2个例子可能是：

- 特征间的单位（尺度）可能不同，比如身高和体重，比如摄氏度和华氏度，比如房屋面积和房间数，一个特征的变化范围可能是[1000, 10000]，另一个特征的变化范围可能是[-0.1, 0.2]，在进行距离有关的计算时，单位的不同会导致计算结果的不同，尺度大的特征会起决定性作用，而尺度小的特征其作用可能会被忽略，为了消除特征间单位和尺度差异的影响，以对每维特征同等看待，需要对特征进行归一化。
- 原始特征下，因尺度差异，其损失函数的等高线图 (Contour map) 可能是椭圆 (oval) 形，梯度方向垂直于等高线，下降会走zigzag路线，而不是指向local minimum。通过对特征进行zero-mean and unit-variance变换后，其损失函数的等高线图更接近圆形，梯度下降的方向震荡更小，收敛更快，如下图所示
![](https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-0.png)
feature scaling的方法可以分成2类，逐行进行和逐列进行。逐行是对每一维特征操作，逐列是对每个样本操作，上图为逐行操作中特征标准化的示例。

具体地，常用feature scaling方法如下
![](https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-1.png)
上述4种feature scaling方式，前3种为逐行操作，最后1种为逐列操作。

### 何时做feature scaling?
涉及或隐含距离计算的算法，比如K-means、KNN、PCA、SVM等，一般需要feature scaling，因为
- zero-mean一般可以增加样本间余弦距离或者内积结果的差异，区分力更强，假设数据集集中分布在第一象限遥远的右上角，将其平移到原点处，可以想象样本间余弦距离的差异被放大了。在模版匹配中，zero-mean可以明显提高响应结果的区分度。
- 就欧式距离而言，增大某个特征的尺度，相当于增加了其在距离计算中的权重，如果有明确的先验知识表明某个特征很重要，那么适当增加其权重可能有正向效果，但如果没有这样的先验，或者目的就是想知道哪些特征更重要，那么就需要先feature scaling，对各维特征等而视之。
- 增大尺度的同时也增大了该特征维度上的方差，PCA算法倾向于关注方差较大的特征所在的坐标轴方向，其他特征可能会被忽视，因此，在PCA前做Standardization效果可能更好，
![](https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-2.png)
- 要用到gradient descent方法的算法，e.g.，神经网络。
    - 注意，文章开篇的椭圆形和圆形等高线图，仅在采用均方误差(mean square error)的线性模型上适用，其他损失函数或更复杂的模型，如深度神经网络，损失函数的error surface可能很复杂，并不能简单地用椭圆和圆来刻画，所以用它来解释feature scaling对所有损失函数的梯度下降的作用是一个简化后的解释，
- batch normalization: 对于传统的神经网络，对输入做feature scaling也很重要，因为采用sigmoid等有饱和区 (saturation point, 导数接近0） 的激活函数，如果输入分布范围很广，参数初始化时没有适配好，很容易直接陷入饱和区，导致梯度消失，所以，需要对输入做Standardization或映射[0,1]、[-1,1]，配合精心设计的参数初始化方法，对值域进行控制。但自从有了Batch Normalization，每次线性变换改变特征分布后，都会重新进行Normalization，似乎可以不太需要对网络的输入进行feature scaling了？但习惯上还是会做feature scaling。


### 何时不做 feature scaling?
- 与距离计算无关的概率模型，不需要feature scaling，比如Naive Bayes；
- 与距离计算无关的基于树的模型，不需要feature scaling，比如决策树、随机森林等，树中节点的选择只关注当前特征在哪里切分对分类更好，即只在意特征内部的相对大小，而与特征间的相对大小无关。（因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率）



###  imbalanced data 处理:
这里assume 正例为少的那个data

#### 1. Undersampling: 去除一些反例
下采样(under-sampling)通过减少分类中多数样本的数量来均衡样本的结构。最简单粗暴的方法是随机删掉一些多数类样本，代价是删除样本的同时也有可能删除了一些信息，通常使用的方法有：

#### clustering
对于多数类样本，计算K近邻的空间距离(如欧式距离)，用K近邻的重心(centroid)代替原本的K个样本，形成新的多数类样本组。因此聚类最终的样本量由少数类样本的数量决定。

#### Tomek links

Tomek links是指相反类样本的配对，这样的配对距离非常近，也就是说这样的配对中两个样本的各项指标都非常接近，但是属于不同的类。如图所示，这一方法能够找到这样的配对，并删除配对中的多数类样本。经过这样的处理，两类样本之间的分界线变得更加清晰，使少数类的存在更加明显。
![](https://raw.githubusercontent.com/emmableu/image/master/imbalanced-data-0.png)
#### 2. Oversampling: 增加一些正例
上采样(over-sampling)是常用的应对不均衡数据的方法,通过增加分类中少数类样本的数量来均衡数据结构。因为其常用，也发展出了很多不同的上采样技术。
#### 复制
最简单的上采样技术是随机有放回地抽取少数量样本，复制一份后加入总样本中。但是如果数据的特征维度较小，简单的抽取复制容易造成过拟合(over-fitting)。经过多年的发展，上采样有以下几种常用的技术来避免过拟合：
#### 合成新样本：SMOTE (Synthetic Minority Over-sampling Technique)及其衍生技术
严格来讲，SMOT不应该放在重采样中，因为操作不涉及重复采样，而是依据现有的少数类样本人为制造一些新的少数类样本，放在这里是因为SMOT及其衍生技术也是上采样的一种方式。

对于少数类样本，SMOT在p个维度上找到K近邻，利用这K个近邻的各项指标，乘上一个0到1之间的随机数，就能组成一个新的少数类样本。因此也很容易发现，SMOT只能够生成少数类样本凸包(convex hall)内的新样本，而永远不可能生成“离群”的样本。
#### 3. 调整损失函数
调整损失函数的目的本身是为了使模型对少数量样本更加敏感。训练任何一个机器学习模型的最终目标是损失函数(loss function)的最小化，如果能够在损失函数中加大错判少数类样本的损失，那么模型自然而然能够更好地识别 出少数类样本。

在损失函数上给错判的少数类样本加上一个惩罚系数，加入损失函数中。这样在训练过程中，模型会自然而然地对少数类样本更为敏感，比如penalized-SVM和penalized-LDA这样的方法。
#### 4. Threshold-moving: 阈值移动
如题

### overfitting 处理:

2. Train with more data， 即根据一些先验知识，对原始数据进行适当变换达到扩充数据集的效果。
4. Feature selection，或者feature reduction, e.g., PCA
5. Early stop. (e.g., in decision tree)
6. Regularization.
7. Bagging. [ensemble learning](/pages/63f233/)
8. dropout (in Neural network) [link](/pages/63f233/)

### 缺失值 missing data 处理
处理缺失值的方法：

首先了解数据缺失的原因，根据原因判断缺失的数据是否具有特定的商业意义。如果不具备业务意义，那么可以进行以下操作：

1. 删除整条记录（list-wise deletion）：适用于缺失值随机分布，且缺失值非常少，不影响整体数据的情况。这种方法的优点是简单，缺点是减少了样本数量。
2. 删除含有大量缺失值的变量：如果某个变量包含大量的缺失值，我们可以直接删除这个变量来保留更多的观测，除非这个变量对于模型而言特别重要。应用这个方法需要我们在变量的重要性和观测的数量之间做权衡。
3. 用标量插补（single imputation）：如果缺失值比较少，那么可以使用平均值，中位数，众数等进行插补。
4. 插值法（interpolation）：先求得插值函数，然后将缺失值对应的点代入插值函数得到缺失值的近似值。常见插值方法有 linear interpolation, 拉格朗日插值法(Lagrange polynomial)、 Newton Polynomial Interpolation。 
5. 用模型预测（model-based imputation）：通过模型来估计缺失值，是处理缺失值比较复杂的方法。 如果缺失值很多，但是比较适用模型预测。在这种情况下，我们将数据集分为两组：一组没有缺失值，另一组有缺少值。 第一个数据集成为模型的训练数据集，而有缺失值的第二个数据集是测试数据集，有缺失值的变量被视为目标变量。 接下来，我们创建一个模型，根据训练数据集的特征预测目标变量，并填充测试数据集的缺失值。我们可以使用线性回归，随机森林，最近邻法，逻辑回归等各种建模技术来执行此操作。  
这种方法有两个缺点：  
    - 如果数据集中的特征与有缺少值的特征之间没有关系，那么模型估计将不精确。
### Word2Vec是如何工作的？013
![](https://raw.githubusercontent.com/emmableu/image/master/202209221139166.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209221140179.png)





## 第5章 非监督学习
### K均值聚类算法的步骤是什么？ 093 ★★☆☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209221219602.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209221220836.png)


### K均值聚类的优缺点是什么？如何对其进行调优？ 094 ★★★☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209221222240.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209221222136.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209221223056.png)


### K均值聚类缺点有哪些？ 097 ★★★☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209221224865.png)
### 以聚类算法为例，如何区分两个非监督学习算法的优劣？ 111 ★★★☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209221257114.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209221257496.png)







## 第9章 前向神经网络
### Effect of batch size, sgd, mini batch
[link](/pages/1f2a0f/#三类梯度下降算法概述/)
### 写出常用激活函数及其导数。 207 ★☆☆☆☆

![](https://raw.githubusercontent.com/emmableu/image/master/202209160031253.png)


### 神经网络训练时是否可以将参数全部初始化为0？ 217 ★☆☆☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209220044455.png)
### 为什么Sigmoid和Tanh激活 函数会导致梯度消失的现象？ 208 ★★☆☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209160032877.png)
### 解释卷积操作中的稀疏交互和参数共享及其作用。 223 ★★☆☆☆

![](https://raw.githubusercontent.com/emmableu/image/master/202209220045822.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209220046942.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209220046455.png)


### ReLU系列的激活函数的优点是什么？他们有什么局限性以及如何改进？ 209 ★★★☆☆

下面的稀疏性主要指的是 类似 l1 regularization 相比于l2的优点 - 可以产生真正的稀疏解

![](https://raw.githubusercontent.com/emmableu/image/master/202209160034612.png)
[source](https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks)

Two additional major benefits of ReLUs are sparsity and a reduced likelihood of vanishing gradient. But first recall the definition of a ReLU is ℎ=max(0,𝑎), where 𝑎=𝑊𝑥+𝑏

One major benefit is the reduced likelihood of the gradient to vanish. This arises when 𝑎>0. In this regime the gradient has a constant value. In contrast, the gradient of sigmoids becomes increasingly small as the absolute value of x increases. The constant gradient of ReLUs results in faster learning.

The other benefit of ReLUs is sparsity. Sparsity arises when 𝑎≤0. The more such units that exist in a layer the more sparse the resulting representation. Sigmoids on the other hand are always likely to generate some non-zero value resulting in dense representations. Sparse representations seem to be more beneficial than dense representations.

### 平方误差损失函数和交叉熵损失函数分别适合什么场景？ 214 ★★★☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209220044662.png)


### 常用的池化操作有哪些？池化的作用是什么？ 225 ★★★☆☆


平移（translation）、旋转（rotation）、缩放（scaling）

![](https://raw.githubusercontent.com/emmableu/image/master/202209220048800.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209220049614.png)
### 卷积神经网络如何用于文本分类任务？ 227 ★★★☆☆

![](https://raw.githubusercontent.com/emmableu/image/master/202209220049987.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209220050458.png)
### ResNet的提出背景和核心理论是什么？ 230 ★★★☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209220050403.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209220052733.png)


## 第10章 循环神经网络
### 循环神经网络与前馈神经网络相比有什么特点？ 236 ★☆☆☆☆

![](https://raw.githubusercontent.com/emmableu/image/master/202209220117377.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209220117767.png)
### 循环神经网络为什么会出现梯度消失或梯度爆炸？有哪些改进方案？ 238 ★★☆☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209220118768.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209220119075.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209220119783.png)

　  

### LSTM是如何实现长短期记忆功能的？ 243 ★★☆☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209221338297.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209221339348.png)

### 在循环神经网络中能否使用ReLU作为激活函数？ 241 ★★★☆☆

![](https://raw.githubusercontent.com/emmableu/image/master/202209221341254.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209221341473.png)


### LSTM里各模块分别使用什么激活函数？可以用其它的激活函数吗？ 245 ★★★☆☆

![](https://raw.githubusercontent.com/emmableu/image/master/202209221342449.png)

