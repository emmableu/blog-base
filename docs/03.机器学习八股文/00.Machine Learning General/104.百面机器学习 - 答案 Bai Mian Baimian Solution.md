---
title: 百面 答案 Bai Mian Baimian Solution
date: 2022-09-14 00:15:10
permalink: /pages/e1bf02/
categories:
  - 机器学习八股文
  - Deep Learning Problems
tags:
  - 
---
## 第1章 特征工程

### 为什么需要对数值类型的特征做feature scaling？002 ★☆☆☆☆
Feature scaling，常见的提法有“特征归一化”(normalization)、“标准化”(standardization)，是数据预处理中的重要技术，有时甚至决定了算法能不能work以及work得好不好。谈到feature scaling的必要性，最常用的2个例子可能是：

- 特征间的单位（尺度）可能不同，比如身高和体重，比如摄氏度和华氏度，比如房屋面积和房间数，一个特征的变化范围可能是[1000, 10000]，另一个特征的变化范围可能是[-0.1, 0.2]，在进行距离有关的计算时，单位的不同会导致计算结果的不同，尺度大的特征会起决定性作用，而尺度小的特征其作用可能会被忽略，为了消除特征间单位和尺度差异的影响，以对每维特征同等看待，需要对特征进行归一化。
- 原始特征下，因尺度差异，其损失函数的等高线图可能是椭圆形，梯度方向垂直于等高线，下降会走zigzag路线，而不是指向local minimum。通过对特征进行zero-mean and unit-variance变换后，其损失函数的等高线图更接近圆形，梯度下降的方向震荡更小，收敛更快，如下图所示
![](https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-0.png)
feature scaling的方法可以分成2类，逐行进行和逐列进行。逐行是对每一维特征操作，逐列是对每个样本操作，上图为逐行操作中特征标准化的示例。

具体地，常用feature scaling方法如下
![](https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-1.png)
上述4种feature scaling方式，前3种为逐行操作，最后1种为逐列操作。

### When do we need feature scaling?
涉及或隐含距离计算的算法，比如K-means、KNN、PCA、SVM等，一般需要feature scaling，因为
- zero-mean一般可以增加样本间余弦距离或者内积结果的差异，区分力更强，假设数据集集中分布在第一象限遥远的右上角，将其平移到原点处，可以想象样本间余弦距离的差异被放大了。在模版匹配中，zero-mean可以明显提高响应结果的区分度。
- 就欧式距离而言，增大某个特征的尺度，相当于增加了其在距离计算中的权重，如果有明确的先验知识表明某个特征很重要，那么适当增加其权重可能有正向效果，但如果没有这样的先验，或者目的就是想知道哪些特征更重要，那么就需要先feature scaling，对各维特征等而视之。
- 增大尺度的同时也增大了该特征维度上的方差，PCA算法倾向于关注方差较大的特征所在的坐标轴方向，其他特征可能会被忽视，因此，在PCA前做Standardization效果可能更好，
![](https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-2.png)
- 要用到gradient descent方法的算法，e.g.，神经网络。
    - 注意，文章开篇的椭圆形和圆形等高线图，仅在采用均方误差(mean square error)的线性模型上适用，其他损失函数或更复杂的模型，如深度神经网络，损失函数的error surface可能很复杂，并不能简单地用椭圆和圆来刻画，所以用它来解释feature scaling对所有损失函数的梯度下降的作用是一个简化后的解释，
- batch normalization: 对于传统的神经网络，对输入做feature scaling也很重要，因为采用sigmoid等有饱和区的激活函数，如果输入分布范围很广，参数初始化时没有适配好，很容易直接陷入饱和区，导致梯度消失，所以，需要对输入做Standardization或映射[0,1]、[-1,1]，配合精心设计的参数初始化方法，对值域进行控制。但自从有了Batch Normalization，每次线性变换改变特征分布后，都会重新进行Normalization，似乎可以不太需要对网络的输入进行feature scaling了？但习惯上还是会做feature scaling。


### When do we not need feature scaling?
- 与距离计算无关的概率模型，不需要feature scaling，比如Naive Bayes；
- 与距离计算无关的基于树的模型，不需要feature scaling，比如决策树、随机森林等，树中节点的选择只关注当前特征在哪里切分对分类更好，即只在意特征内部的相对大小，而与特征间的相对大小无关。（因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率）


### 什么是组合特征？如何处理高维组合特征？ 006 ★★☆☆☆
### 怎样有效地找到组合特征？ 009 ★★☆☆☆
### 有哪些文本表示模型？它们各有什么优缺点？ 011 ★★☆☆☆
### 如何缓解图像分类任务中训练数据不足带来的问题？ 016 ★★☆☆☆
### Word2Vec是如何工作的？它和隐狄利克雷模型有什么区别与联系？ 013 ★★★☆☆

## 第2章 模型评估

### 准确率的局限性。 022 ★☆☆☆☆
### 精确率与召回率的权衡。 023 ★☆☆☆☆
### 平方根误差的“意外”。 025 ★☆☆☆☆
### 什么是ROC曲线？ 027 ★☆☆☆☆
### 为什么要进行在线A/B测试？ 037 ★☆☆☆☆ 
### 如何进行线上A/B测试？ 038 ★☆☆☆☆
### 过拟合和欠拟合具体是指什么现象？ 045 ★☆☆☆☆
### 如何绘制ROC曲线？ 028 ★★☆☆☆
### 如何计算AUC？ 030 ★★☆☆☆
### 为什么在一些场景中要使用余弦相似度而不是欧氏距离？ 033 ★★☆☆☆
### 如何划分实验组和对照组？ 038 ★★☆☆☆
### 模型评估过程中的验证方法及其优缺点。 040 ★★☆☆☆
### 能否说出几种降低过拟合和欠拟合风险的方法？ 046 ★★☆☆☆
### ROC曲线相比P-R曲线有什么特点？ 030 ★★★☆☆
### 余弦距离是否是一个严格定义的距离？ 034 ★★★☆☆
### 自助法采样在极限情况下会有多少数据从未被选择过？ 041 ★★★☆☆
### 超参数有哪些调优方法？ 043 ★★★☆☆


## 第3章 经典算法
### 逻辑回归相比线性回归，有何异同？ 058 ★★☆☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209140016581.png)
### 决策树有哪些常用的启发函数？ 062 ★★☆☆☆
### 线性可分的两类点在SVM分类超平面上的投影仍然线性可分吗？ 051 ★★★☆☆
### 证明存在一组参数使得高斯核SVM的训练误差为0。 054 ★★★☆☆
### 加入松弛变量的SVM的训练误差可以为0吗？ 056 ★★★☆☆
### 用逻辑回归处理多标签分类任务的一些相关问题。 059 ★★★☆☆
### 如何对决策树进行剪枝？ 067 ★★★☆☆
### 训练误差为0的SVM分类器一定存在吗？ 055 ★★★★☆


## 第4章 降维
### 从最大方差的角度定义PCA的目标函数并给出求解方法。 074 ★★☆☆☆
### 从回归的角度定义PCA的目标函数并给出对应的求解方法。 078 ★★☆☆☆
### 线性判别分析的目标函数以及求解方法。 083 ★★☆☆☆
### 线性判别分析与主成分分析的区别与联系 086 ★★☆☆☆


## 第5章 非监督学习
### K均值聚类算法的步骤是什么？ 093 ★★☆☆☆
### 高斯混合模型的核心思想是什么？它是如何迭代计算的？ 103 ★★☆☆☆
### K均值聚类的优缺点是什么？如何对其进行调优？ 094 ★★★☆☆
### 针对K均值聚类的缺点，有哪些改进的模型？ 097 ★★★☆☆
### 自组织映射神经网络是如何工作的？它与K均值算法有何区别？ 106 ★★★☆☆
### 怎样设计自组织映射神经网络并设定网络训练参数？ 109 ★★★☆☆
### 以聚类算法为例，如何区分两个非监督学习算法的优劣？ 111 ★★★☆☆
### 证明K均值聚类算法的收敛性。 099 ★★★★☆


## 第6章 概率图模型
### 写出图6.1（a）中贝叶斯网络的联合概率分布。 118 ★☆☆☆☆
### 写出图6.1（b）中马尔可夫网络的联合概率分布。 119 ★☆☆☆☆
### 解释朴素贝叶斯模型的原理，并给出概率图模型表示。 121 ★★☆☆☆
### 解释最大熵模型的原理，并给出概率图模型表示。 122 ★★☆☆☆
### 常见的主题模型有哪些？试介绍其原理。 133 ★★☆☆☆
### 如何确定LDA模型中的主题个数？ 136 ★★☆☆☆
### 常见的概率图模型中，哪些是生成式的，哪些是判别式的？ 125 ★★★☆☆
### 如何对中文分词问题用隐马尔可夫模型进行建模和训练？ 128 ★★★☆☆
### 如何用主题模型解决推荐系统中的冷启动问题？ 137 ★★★☆☆
### 最大熵马尔可夫模型为什么会产生标注偏置问题？如何解决？ 129 ★★★★☆

## 第7章 优化算法
### 有监督学习涉及的损失函数有哪些？ 142 ★☆☆☆☆
### 训练数据量特别大时经典梯度法存在的问题，如何改进？ 155 ★☆☆☆☆
### 机器学习中哪些是凸优化问题？哪些是非凸优化问题？ 145 ★★☆☆☆
### 无约束优化问题的求解。 148 ★★☆☆☆
### 随机梯度下降法失效的原因。 158 ★★☆☆☆
### 如何验证求目标函数梯度功能的正确性？ 152 ★★★☆☆
### 随机梯度下降法的一些变种。 160 ★★★☆☆
### L1正则化使得模型参数具有稀疏性的原理是什么？ 164 ★★★☆☆
 
 ## 第8章 采样
### 如何编程实现均匀分布随机数生成器？ 174 ★☆☆☆☆
### 简述MCMC采样法的主要思想。 185 ★☆☆☆☆
### 举例说明采样在机器学习中的应用。 172 ★★☆☆☆
### 简单介绍几种常见的MCMC采样法。 186 ★★☆☆☆
### MCMC采样法如何得到相互独立的样本？ 187 ★★☆☆☆
### 简述一些常见的采样方法的主要思想和具体操作。 176 ★★★☆☆
### 如何对高斯分布进行采样？ 180 ★★★☆☆
### 如何对贝叶斯网络进行采样？ 190 ★★★☆☆
### 当训练集中正负样本不均衡时，如何处理数据以更好地训练分类模型？ 194 ★★★☆☆


## 第9章 前向神经网络
### 写出常用激活函数及其导数。 207 ★☆☆☆☆

![](https://raw.githubusercontent.com/emmableu/image/master/202209160031253.png)


### 神经网络训练时是否可以将参数全部初始化为0？ 217 ★☆☆☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209220044455.png)
### 为什么Sigmoid和Tanh激活 函数会导致梯度消失的现象？ 208 ★★☆☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209160032877.png)
### 解释卷积操作中的稀疏交互和参数共享及其作用。 223 ★★☆☆☆

![](https://raw.githubusercontent.com/emmableu/image/master/202209220045822.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209220046942.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209220046455.png)


### ReLU系列的激活函数的优点是什么？他们有什么局限性以及如何改进？ 209 ★★★☆☆

下面的稀疏性主要指的是 类似 l1 regularization 相比于l2的优点 - 可以产生真正的稀疏解

![](https://raw.githubusercontent.com/emmableu/image/master/202209160034612.png)
[source](https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks)

Two additional major benefits of ReLUs are sparsity and a reduced likelihood of vanishing gradient. But first recall the definition of a ReLU is ℎ=max(0,𝑎), where 𝑎=𝑊𝑥+𝑏

One major benefit is the reduced likelihood of the gradient to vanish. This arises when 𝑎>0. In this regime the gradient has a constant value. In contrast, the gradient of sigmoids becomes increasingly small as the absolute value of x increases. The constant gradient of ReLUs results in faster learning.

The other benefit of ReLUs is sparsity. Sparsity arises when 𝑎≤0. The more such units that exist in a layer the more sparse the resulting representation. Sigmoids on the other hand are always likely to generate some non-zero value resulting in dense representations. Sparse representations seem to be more beneficial than dense representations.

### 平方误差损失函数和交叉熵损失函数分别适合什么场景？ 214 ★★★☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209220044662.png)


### 常用的池化操作有哪些？池化的作用是什么？ 225 ★★★☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209220048800.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209220049614.png)
### 卷积神经网络如何用于文本分类任务？ 227 ★★★☆☆

![](https://raw.githubusercontent.com/emmableu/image/master/202209220049987.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209220050458.png)
### ResNet的提出背景和核心理论是什么？ 230 ★★★☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209220050403.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209220052733.png)


## 第10章 循环神经网络
### 循环神经网络与前馈神经网络相比有什么特点？ 236 ★☆☆☆☆

![](https://raw.githubusercontent.com/emmableu/image/master/202209220117377.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209220117767.png)
### 循环神经网络为什么会出现梯度消失或梯度爆炸？有哪些改进方案？ 238 ★★☆☆☆
![](https://raw.githubusercontent.com/emmableu/image/master/202209220118768.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209220119075.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209220119783.png)

　  

### LSTM是如何实现长短期记忆功能的？ 243 ★★☆☆☆
### 什么是Seq2Seq模型？它有哪些优点？ 247 ★★☆☆☆
### 在循环神经网络中能否使用ReLU作为激活函数？ 241 ★★★☆☆
### LSTM里各模块分别使用什么激活函数？可以用其它的激活函数吗？ 245 ★★★☆☆
### Seq2Seq模型在解码时有哪些常用的方法？ 249 ★★★☆☆
### Seq2Seq模型引入注意力机制是为了解决什么问题？为什么选用双向循环神经模型？ 251 ★★★★☆

## 第12章 集成学习
### 集成学习分哪几种？它们有何异同？ 278 ★☆☆☆☆
### 常用的基分类器是什么？ 285 ★☆☆☆☆
### 集成学习有哪些基本步骤？请举几个集成学习的例子。 282 ★★☆☆☆
### 可否将随机森林中的基分类器由决策树替换为线性分类器或K-近邻？ 286 ★★☆☆☆
### 什么是偏差和方差？ 287 ★★☆☆☆
### GBDT的基本原理是什么？ 291 ★★☆☆☆
### 梯度提升和梯度下降的区别和联系是什么？ 293 ★★☆☆☆
### GBDT的优点和局限性有哪些？ 294 ★★☆☆☆
### 如何从减小方差和偏差的角度解释Boosting和Bagging的原理？ 289 ★★★☆☆
### XGBoost与GBDT的联系和区别有哪些？ 295 ★★★☆☆

