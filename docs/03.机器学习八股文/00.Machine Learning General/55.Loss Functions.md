---
title: Loss Functions
date: 2022-04-18 10:33:46
permalink: /pages/2a61bd/
categories:
  - 机器学习八股文
  - Neural Network
tags:
  - 
---


### Cross-Entropy Loss Function

Cross entropy loss is commonly used in classification tasks both in traditional ML and deep learning.

![](https://miro.medium.com/max/1400/0*BxYO1LbZ40uuU2nG.jpeg)

Image from [this post](https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e)

Note: _logit_ here is used to refer to the unnormalized output of a NN, as in [Google ML glossary](https://developers.google.com/machine-learning/glossary/#logits). However, admittedly, this term is overloaded, as discussed in this [post](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow).

In this figure, the raw unnormalized output from a Neural Network is converted into probability by a `softmax` function.

![](https://miro.medium.com/max/882/0*eKHqb8IjKL2oVTBE.jpeg)

Image from [this post](https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e)

Now suppose we have a training sample which is a dog, the target label would be `[1,0,0,0]` , while the NN raw output is `[3.2, 1.3, 0.2, 0.8]` , the softmax probability output is `[0.775, 0.116, 0.039, 0.07]` , what would be the cross entropy loss?

![](https://miro.medium.com/max/1400/1*-2E-C0iqdQL-JHkEiDqBqA.png)

Plugging in the values, we have the loss value:

![](https://miro.medium.com/max/1400/1*GCYvYhq8596_4Aay20Pszg.png)

Note: natural log is used as common practice.

After some weights update, we have new raw outputs for the same sample, the softmax probability becomes `[0.9, 0.05, 0.03, 0.02]` , the new loss value is:

![](https://miro.medium.com/max/1400/1*1yh9oci1e0uGkSNYPlbbHA.png)

This new loss is lower than previous loss, indicating NN is learning. Intuitively, we can also observe that the softmax probability is closer to the the true distribution.

The perfect loss will be 0, when the softmax outputs perfectly matches the true distribution. However, that would mean extreme overfitting.



Also in this example, we only considered a single training sample, in reality, we normally do mini-batches. And **by default PyTorch will use the average cross entropy loss of all samples in the batch**.

One might wonder, what is a good value for cross entropy loss, how do I know if my training loss is good or bad?

Some intuitive guidelines from [MachineLearningMastery post](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) for natural log based for a mean loss:

-   **Cross-Entropy = 0.00**: Perfect probabilities.
-   **Cross-Entropy < 0.02**: Great probabilities.
-   **Cross-Entropy < 0.05**: On the right track.
-   **Cross-Entropy < 0.20**: Fine.
-   **Cross-Entropy > 0.30**: Not great.
-   **Cross-Entropy > 1.00**: Terrible.
-   **Cross-Entropy > 2.00** Something is broken.

**Binary cross entropy** is a special case where the number of classes are 2. In practice, it is often implemented in different APIs. In PyTorch, there are `nn.BCELoss` and `nn.BCEWithLogitsLoss` . The former requires the input to be normalized sigmoid probability, whereas the latter can take raw unnormalized logits.