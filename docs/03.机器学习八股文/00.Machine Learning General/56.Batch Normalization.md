---
title: Batch Normalization
date: 2022-09-22 00:26:04
permalink: /pages/f405d4/
categories:
  - 机器学习八股文
  - Neural Network
tags:
  - 
---


Batch Normalization Normalizing the input or output of the activation functions in a hidden layer. Batch normalization can provide benefits such as:
	- Reduce overfitting.
    - Make neural networks more stable by protecting against outlier weights.
    - Enable higher learning rates.


-   对于传统的神经网络，对输入做feature scaling也很重要，因为采用sigmoid等有饱和区的激活函数，如果输入分布范围很广，参数初始化时没有适配好，很容易直接陷入饱和区，导致梯度消失，所以，需要对输入做Standardization或映射[0,1]、[-1,1]，配合精心设计的参数初始化方法，对值域进行控制。但自从有了Batch Normalization，每次线性变换改变特征分布后，都会重新进行Normalization，似乎可以不太需要对网络的输入进行feature scaling了？但习惯上还是会做feature scaling。



![](https://raw.githubusercontent.com/emmableu/image/master/202209220029918.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209220030828.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209220030346.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209220033400.png)
