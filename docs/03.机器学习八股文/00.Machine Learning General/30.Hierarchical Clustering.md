---
title: Hierarchical Clustering
date: 2021-11-08 10:59:34
permalink: /pages/4d184b/
categories:
  - 机器学习八股文
  - Machine Learning Models
tags:
  - 
---
[notes from MIT](http://web.mit.edu/6.S097/www/resources/Hierarchical.pdf)

## Definition
Hierarchical Clustering is an algorithm that iteratively cluster data samples into a hierarchical structure - a tree of clusters. It clusters data by iteratively merge clusters based on a similarity metric between clusters, such as min or max or average distance. 

## hierarchical clustering v.s. K-means
K-means tend to have the following issues:
1. Convergence time is poor. For example K-means takes worst case exponential number (2^(Ω(n))) of iterations.
2. The final clusters depend heavily on the initialization. Usually a k random points are chosen.
3. The number of clusters is a huge issue. You are forced to specify the number of clusters in the beginning.

Hierarchical clustering solves all these issues and even allows you a metric by which to cluster.   
Hierarchical clustering is polynomial time, the final clusters are always the same depending on your metric, and the number of clusters is not at all a problem.

## Hierarchical clustering steps

[source](https://www.learndatasci.com/glossary/hierarchical-clustering/)

Hierarchical clustering employs a measure of distance/similarity to create new clusters. It includes steps such as: 

-   Step 1: Compute the proximity matrix using a particular distance metric (e.g., min distance)
-   Step 2: Each data point is assigned to a cluster
-   Step 3: Merge the clusters based on a metric for the similarity between clusters
-   Step 4: Update the distance matrix
-   Step 5: Repeat Step 3 and Step 4 until only a single cluster remains

### Computing a proximity matrix

The first step of the algorithm is to create a distance matrix. The values of the matrix are calculated by applying a distance function between each pair of objects. The **Euclidean distance function** is commonly used for this operation. The structure of the proximity matrix will be as follows for a data set with n elements. Here, represent the distance values between p_i and p_j


![](https://raw.githubusercontent.com/emmableu/image/master/202210010143465.png)
### Similarity between Clusters

The main question in hierarchical clustering is how to calculate the distance between clusters and update the proximity matrix. There are many different approaches used to answer that question. Each approach has its advantages and disadvantages. The choice will depend on whether there is noise in the data set, whether the shape of the clusters is circular or not, and the density of the data points.

A numerical example will help illustrate the methods and choices. We'll use a small sample data set containing just nine two-dimensional points, displayed in Figure 1.

![](https://storage.googleapis.com/lds-media/images/Sample-data-plot.width-1200.jpg)

Figure 1: Sample Data

Suppose we have two clusters in the sample data set, as shown in Figure 2. There are different approaches to calculate the distance between the clusters. Popular methods are listed below.

![](https://storage.googleapis.com/lds-media/images/Sample-data-two-clusters-plot.width-1200.jpg)

Figure 2: Two clusters

#### Min (Single) Linkage

One way to measure the distance between clusters is to find the minimum distance between points in those clusters. That is, we can find the point in the first cluster nearest to a point in the other cluster and calculate the distance between those points. In Figure 2, the closest points are p_2 in one cluster and p_5 in the other.  The distance between those points, and hence the distance between clusters, is found as `d(p_2, p_5) = 4`.

![](https://storage.googleapis.com/lds-media/images/Sample-data-min-distance-single-linkage.width-1200.jpg)

Figure 3: Min Linkage Method

The advantage of the Min method is that it can accurately handle non-elliptical shapes. The disadvantages are that it is sensitive to noise and outliers.

#### Max (Complete) Linkage

Another way to measure the distance is to find the maximum distance between points in two clusters. We can find the points in each cluster that are furthest away from each other and calculate the distance between those points. In Figure 3, the maximum distance is between p1 and p6. Distance between those two points, and hence the distance between clusters, is found as d(p1, p6) = 10.


![](https://storage.googleapis.com/lds-media/images/Sample-data-max-distance-complete-linkage.width-1200.jpg)

Figure 4: Max Linkage Method


Max is less sensitive to noise and outliers in comparison to MIN method. However, MAX can break large clusters and tends to be biased towards globular clusters.

#### Centroid Linkage

The Centroid method defines the distance between clusters as being the distance between their centers/centroids. After calculating the centroid for each cluster, the distance between those centroids is computed using a distance function.

![](https://storage.googleapis.com/lds-media/images/Sample-data-centroid-distance-linkage.width-1200.jpg)

Figure 5: Centroid Linkage Method



#### Average Linkage

The Average method defines the distance between clusters as the average pairwise distance among all pairs of points in the clusters. For simplicity, only some of the lines connecting pairs of points are shown in Figure 6.

![](https://storage.googleapis.com/lds-media/images/Sample-data-average-distance-linkage.width-1200.jpg)

Figure 6: Average Linkage Method


#### Ward Linkage

The Ward approach analyzes the variance of the clusters rather than measuring distances directly, minimizing the variance between clusters.

With the Ward method, the distance between two clusters is related to how much the sum of squares (SS) value will increase when combined.

In other words, the Ward method attempts to minimize the sum of the squared distances of the points from the cluster centers. Compared to the distance-based measures described above, the Ward method is less susceptible to noise and outliers. Therefore, Ward's method is preferred more than others in clustering.