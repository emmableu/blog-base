---
title: è¯„ä»·æ¨¡å‹ Model Review
date: 2022-09-20 20:13:28
permalink: /pages/6b4f6c/
categories:
  - æœºå™¨å­¦ä¹ å…«è‚¡æ–‡
  - Machine Learning General
tags:
  - 
---
- data çš„assumption å¤šä¸å¤š (æ¯”å¦‚featureç›¸äº’ç‹¬ç«‹ï¼Œæ®‹å·®æ­£æ€åˆ†å¸ƒä¹‹ç±»çš„)
- data æ˜¯å¦éœ€è¦å…ˆè¿›è¡Œscale
- higher dimension é«˜ç»´ç‰¹å¾ è¡¨ç°èƒ½åŠ› ï¼ˆæ¯”å¦‚featureæ¯”sampleè¿˜å¤šï¼‰
	- feature > sample ä¸èƒ½æ±‚è§£çš„æ–¹æ³•ï¼š linear regression, logistic regression ï¼ˆä½†æ˜¯åŠ äº† L1/L2 regularization å°±å¯ä»¥æ±‚è§£ï¼‰
	- æ˜¯å¦å€¾å‘äºå¾—åˆ°ç¨€ç–è§£ ï¼ˆæ˜¯å¦å¯ä»¥é€šè¿‡æ±‚è§£è¿™ä¸ªregressionçš„ç»“æœå»æ‰ä¸€äº›æ²¡ç”¨çš„featureï¼‰
		- å€¾å‘äºå¾—åˆ°ç¨€ç–è§£çš„æ–¹æ³•: L1 regularization, ReLU activation functionï¼Œ svm with hinge loss
- æ˜¯å¦sensitive to outlier:
	- good model: SVM
- æ›´å€¾å‘äºoverfit (low bias, high variance),  è¿˜æ˜¯underfit
	- overfit: Decision Tree, Neural Network
- è®­ç»ƒé€Ÿåº¦
- hyper parameter tuningçš„éš¾åº¦/éº»çƒ¦ç¨‹åº¦
- å¯¹äºimbalanced datasetçš„å¤„ç†èƒ½åŠ›
- æ¨¡å‹æœ¬èº«æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“ç†è§£ï¼ˆæ¯”å¦‚decision treeï¼Œlogistic regression å°±å¾ˆå®¹æ˜“ç†è§£ï¼‰
- èƒ½ä¸èƒ½å¾—åˆ°interpretableçš„feature importance
- minimum æ˜¯å¦ = global minimum  (æ˜¯å¦æ˜¯convex function)

## Linear Regression è¯„ä»·
- data assumption å¤šï¼š
	- residuals
		- iid
		- normal distribution with mean=0 (å¦‚æœä¸è¿›è¡Œbetaç½®ä¿¡åŒºé—´çš„ä¼°è®¡ï¼Œå°±æ˜¯mean 0, constant variance)
		- independent of X
	- X / features:
		- independent with each other
		- linearity: assuming relationship y = wx + b
	å…³äºresidualçš„è§ä¸‹å›¾
	![](https://raw.githubusercontent.com/emmableu/image/master/202210032015214.png)
- data does not need scaling:å¯¹äºçº¿æ€§æ¨¡å‹ y = wx + b è€Œè¨€ï¼Œxçš„ä»»æ„çº¿æ€§å˜æ¢(å¹³ç§»ï¼Œæ”¾ç¼©)éƒ½ä¼šååº”åœ¨wï¼Œbä¸Šï¼Œæ‰€ä»¥ä¸ä¼šå½±å“æ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›ã€‚
- does not work well with high dimension (many features)
	- feature > sample ä¸èƒ½æ±‚è§£ï¼Œ 
	- å¤§éƒ¨åˆ†betaéƒ½ä¸ä¼šå®Œå…¨æ˜¯0ï¼Œ æ‰€ä»¥ä¸å¯ä»¥é€šè¿‡æ±‚è§£è¿™ä¸ªregressionçš„ç»“æœå»æ‰ä¸€äº›æ²¡ç”¨çš„feature
	- prone to overfit if there are many features / dimensions, causing high variance. 
- sensitive to outliers comparing to models like SVM (å› ä¸º during loss function, treat each sample equallyï¼Œincluding the outliers)
- å¦‚æœassumptionå®Œå…¨æ»¡è¶³çš„è¯ï¼Œæ ¹æ®gaussian-markov theorem, it does pretty good in terms of balancing under- and over- fit. 
	- ä½†æ˜¯ï¼Œå¦‚æœassumption ä¸æ»¡è¶³çš„è¯ï¼Œ æ›´å€¾å‘äºunderfit, å› ä¸ºmodel ç›¸å¯¹decision treeï¼Œ neural network æ›´åŠ ç®€å•
- è®­ç»ƒé€Ÿåº¦å¿«ï¼Œæœ‰ analytical solution, ä¸éœ€è¦ iteratively do gradient descent
- hyper parameter tuning å®¹æ˜“ï¼Œæ²¡æœ‰ä»€ä¹ˆhyperparameter
- ï¼ˆå› ä¸ºæ˜¯continuous target variableï¼Œä¸éœ€è¦è®¨è®ºimbalanceé—®é¢˜ï¼‰
- model is very interpretable
- feature importance is very interpretable - can just look at the p value of the beta. 
- MSE is a convex function, so gradient descent can always reach global minima

## Logistic Regression è¯„ä»·
- data çš„assumption å¤šä¸å¤š (æ¯”å¦‚featureç›¸äº’ç‹¬ç«‹ï¼Œæ®‹å·®æ­£æ€åˆ†å¸ƒä¹‹ç±»çš„)
	- æ¯”linear regressionå°‘å¾ˆå¤šï¼Œæ‰€æœ‰å…³äºresidualçš„éƒ½ä¸éœ€è¦
	- åªéœ€è¦ï¼š
		- 	X / features:
			- independent with each other
			- linearity: assumes linearity of independent variables and log odds.
- data æ˜¯å¦éœ€è¦å…ˆè¿›è¡Œscale
	- è¦ï¼Œå› ä¸ºè¦ç”¨gradient descent æ±‚è§£
- higher dimension é«˜ç»´ç‰¹å¾ è¡¨ç°èƒ½åŠ› ï¼ˆæ¯”å¦‚featureæ¯”sampleè¿˜å¤š
	- ä¸å¥½
	- å¤§éƒ¨åˆ†betaéƒ½ä¸ä¼šå®Œå…¨æ˜¯0ï¼Œ æ‰€ä»¥ä¸å¯ä»¥é€šè¿‡æ±‚è§£è¿™ä¸ªregressionçš„ç»“æœå»æ‰ä¸€äº›æ²¡ç”¨çš„feature
	- prone to overfit if there are many features / dimensions, causing high variance. 
- æ˜¯å¦sensitive to outlier:
	- ä¸å¦‚SVM
- æ›´å€¾å‘äºoverfit (low bias, high variance),  è¿˜æ˜¯underfit
	- more underfit comparing to neural network
	- more overfit comparing to naive bayes. 
- è®­ç»ƒé€Ÿåº¦
	- relatively low comparing to linear regression or naive bayes
- hyper parameter tuningçš„éš¾åº¦/éº»çƒ¦ç¨‹åº¦
	- å¥½åƒæ²¡æœ‰ hyper parameterï¼Œ å¯ä»¥åŠ regularization
- æ¨¡å‹æœ¬èº«æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“ç†è§£ï¼ˆæ¯”å¦‚decision treeï¼Œlogistic regression å°±å¾ˆå®¹æ˜“ç†è§£ï¼‰
	- æ˜¯, å¯ä»¥ç”¨odds ratio The odds ratio represents the odds that an outcome will occur given the presence of a specific predictor,
- èƒ½ä¸èƒ½å¾—åˆ°interpretableçš„feature importance
	- èƒ½
- minimum æ˜¯å¦ = global minimum  (æ˜¯å¦æ˜¯convex function)
	- æ˜¯ï¼Œloss function is convex
- ï¼ˆè·³è¿‡ï¼‰å¯¹äºimbalanced datasetçš„å¤„ç†èƒ½åŠ› 
	- there may not be sufficient patterns belonging to the minority class to adequately represent its distribution.

## Lasso / Ridge (L1 / L2 Regularization) Regression è¯„ä»·
- data assumption æ¯” linear regression å°‘: ä¸éœ€è¦æ®‹å·®æ­£æ€åˆ†å¸ƒï¼Œå› ä¸ºå¾—åˆ°çš„betaä¸æ˜¯unbiasedçš„ï¼Œä¸ç¬¦åˆtåˆ†å¸ƒ
	-  éœ€è¦çš„assumptionï¼š
		- residual:
			- iid
			- 0 mean, constant variance
			- independent of X
		- X / feature:
			- don't need to be completely independent with each other
			- linearity: assuming relationship y = wx + b
- data need scaling
	- ![](https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-2.png)

- works better with high dimension:
	- L1 allows sparcity -> some beta will be fitted to be 0, so, Lasso Regularization can exclude useless variables from the model and, in general, tends to perform well when we need to remove a lot of useless variables from a model.

- sensitive to outliers comparing to models like SVM 
- comparing to LR tends to underfit if too big lambda
- hyper parameter tuning å®¹æ˜“, åªæœ‰lambda
- model is very interpretable
- minimum æ˜¯å¦ = global minimum  (æ˜¯å¦æ˜¯convex function):
	- L2 Ridge regression: loss function is convex,  gradient descent can always reach global minima
	- L1 Lasso: sometimes not convex, so gradient descent don't always reach global minima



## Naive Baiyes è¯„ä»·
- data çš„assumption å¤šä¸å¤šï¼š
	- æœ‰ä¸€ä¸ªï¼š features need to be independent with each other
- data æ˜¯å¦éœ€è¦å…ˆè¿›è¡Œscale
	- no
- higher dimension é«˜ç»´ç‰¹å¾ è¡¨ç°èƒ½åŠ› ï¼ˆæ¯”å¦‚featureæ¯”sampleè¿˜å¤šï¼‰
	- If your data has ğ‘˜ dimensions, then a fully general ML algorithm which attempts to learn all possible correlations between these features has to deal with 2ğ‘˜ possible feature interactions, and therefore needs on the order of 2ğ‘˜ many data points to be performant. 
	- However because Naive Bayes assumes independence between features, it only needs on the order of ğ‘˜ many data points, exponentially fewer.
- æ˜¯å¦sensitive to outlier:
	- sensitive to outliers comparing to SVM
- æ›´å€¾å‘äºoverfit (low bias, high variance),  è¿˜æ˜¯underfit
	- underfit, since the interactions are not modeled, some of the information in the data is ignored. This makes it an inherently high bias model; it has a high approximation error but as a result it also does not overfit.
- è®­ç»ƒé€Ÿåº¦
	- fast
- hyper parameter tuningçš„éš¾åº¦/éº»çƒ¦ç¨‹åº¦
	- easy, only one hyperparameter - pseudocount
- å¯¹äºimbalanced datasetçš„å¤„ç†èƒ½åŠ›
	- not good
	- ![](https://raw.githubusercontent.com/emmableu/image/master/202210050154011.png)
	- Even though the likelihood probabilities are similar to some extent, but the posterior probability is badly affected by prior probabilities. Here in the above example the class +ve prior probability will be 9 times higher than the class -ve, this difference in naive bayes creates a bias for class +ve.
	- One simple solution is to ignore the prior probabilities. (Or) You can do undersampling or oversampling.
- (è·³è¿‡)æ¨¡å‹æœ¬èº«æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“ç†è§£ï¼ˆæ¯”å¦‚decision treeï¼Œlogistic regression å°±å¾ˆå®¹æ˜“ç†è§£ï¼‰è·³è¿‡
- (è·³è¿‡)èƒ½ä¸èƒ½å¾—åˆ°interpretableçš„feature importance è·³è¿‡
- (è·³è¿‡)minimum æ˜¯å¦ = global minimum  (æ˜¯å¦æ˜¯convex function) è·³è¿‡


## Decision Tree è¯„ä»·
- data çš„assumption å¤šä¸å¤š (æ¯”å¦‚featureç›¸äº’ç‹¬ç«‹ï¼Œæ®‹å·®æ­£æ€åˆ†å¸ƒä¹‹ç±»çš„)
	- å¥½åƒæ²¡æœ‰
- data æ˜¯å¦éœ€è¦å…ˆè¿›è¡Œscale
	- no need
- higher dimension é«˜ç»´ç‰¹å¾ è¡¨ç°èƒ½åŠ›
	- tends to become very overfit,  the number of branches grows exponentially with the number of features
- æ˜¯å¦sensitive to outlier:
	- not sensitive to outliers since the partitioning happens based on the proportion of samples within the split ranges and not on absolute values.
	- especially when we use early stopping
- æ›´å€¾å‘äºoverfit (low bias, high variance),  è¿˜æ˜¯underfit
	- overfit,very specific rules
- è®­ç»ƒé€Ÿåº¦
	- not as fast comparing to logistic regression / naive bayes, but still fast enough comparing to neural network
	- test time is fast as it's just linearly traversing the test rules. 
- hyper parameter tuningçš„éš¾åº¦/éº»çƒ¦ç¨‹åº¦
	- å¥½åƒæ²¡æœ‰hyper parameterï¼Œé™¤éå®šä¹‰early stopping
- å¯¹äºimbalanced datasetçš„å¤„ç†èƒ½åŠ›
	- ä¸å¤ªå¥½
	- Decision trees implementations normally use Gini impurity for finding splits. For each rule/condition, when calculating the gini impurity it is a weighted average on. it weights higher on the more prevaling condition (e.g., loves troll when predicting loves popcorn). But more prevaling sample has a higher say on the more prevaling condition. 
- æ¨¡å‹æœ¬èº«æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“ç†è§£ï¼ˆæ¯”å¦‚decision treeï¼Œlogistic regression å°±å¾ˆå®¹æ˜“ç†è§£ï¼‰
	- yes. has interpretable rules
- (è·³è¿‡) èƒ½ä¸èƒ½å¾—åˆ°interpretableçš„feature importance 
- (è·³è¿‡) minimum æ˜¯å¦ = global minimum  (æ˜¯å¦æ˜¯convex function) è·³è¿‡

## SVM è¯„ä»·

- data çš„assumption å¤šä¸å¤š (æ¯”å¦‚featureç›¸äº’ç‹¬ç«‹ï¼Œæ®‹å·®æ­£æ€åˆ†å¸ƒä¹‹ç±»çš„)
	- æ¯”è¾ƒå®½æ¾ï¼Œåªéœ€è¦Data is linearly separable. Even if the linear boundary is in an augmented feature space. for example, after the kernel trick
- data æ˜¯å¦éœ€è¦å…ˆè¿›è¡Œscale
	- è¦ã€€
	- SVM tries to maximize the distance between the separating plane and the support vectors.
	- If one feature (i.e. one dimension in this space) has very large values, it will dominate the other features when calculating the distance. 
	- If you rescale all features (e.g. to [0, 1]), they all have the same influence on the distance metric.
- higher dimension é«˜ç»´ç‰¹å¾ è¡¨ç°èƒ½åŠ› ï¼ˆæ¯”å¦‚featureæ¯”sampleè¿˜å¤šï¼‰
	- è¿˜è¡Œï¼Œå› ä¸ºå®ƒä¼šé€šè¿‡fitè¿™ä¸ªmodelæ¥æ”¾ç¼© c => regularization parameter, the degree to which our model will accept misclassifications in our datasetï¼Œ so that it generalise well over training data.
	- æ˜¯å¦å€¾å‘äºå¾—åˆ°ç¨€ç–è§£ ï¼ˆæ˜¯å¦å¯ä»¥é€šè¿‡æ±‚è§£è¿™ä¸ªmodelçš„ç»“æœå»æ‰ä¸€äº›æ²¡ç”¨çš„featureï¼‰
		- ç”¨hinge loss ä½œä¸ºloss functionï¼Œ å› ä¸ºhinge lossæ˜¯ = max(0, 1-z), æ‰€ä»¥>1æ—¶æ˜¯0ï¼Œæ‰€ä»¥ä¼šæœ‰ 0 è§£
- æ˜¯å¦sensitive to outlier:
	- ä¸sensitive to outlierï¼Œå¾ˆrobustï¼Œè¿˜æ˜¯å› ä¸ºå®ƒä¼šé€šè¿‡fitè¿™ä¸ªmodelæ¥æ”¾ç¼© cï¼Œè§ä¸Š
		- è€Œä¸”åªå…³å¿ƒsoft margin å‘¨å›´çš„ç‚¹
	- good model: SVM
- æ›´å€¾å‘äºoverfit (low bias, high variance),  è¿˜æ˜¯underfit
	- æ¯”èµ·lr å’Œ nn å’Œ decision tree è¿™äº›æ›´å®¹æ˜“ underfit, å› ä¸ºæœ‰soft margin å’Œ regularization parameterã€€
- è®­ç»ƒé€Ÿåº¦
	- ä¸æ˜¯å¾ˆå¿«ï¼Œå› ä¸ºè¿˜æ˜¯è¦ç”¨gradient descent
- hyper parameter tuningçš„éš¾åº¦/éº»çƒ¦ç¨‹åº¦
	- éº»çƒ¦ï¼Œæ¯”å¦‚ç¡®å®šå“ªä¸ªkernel
- æ¨¡å‹æœ¬èº«æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“ç†è§£ï¼ˆæ¯”å¦‚decision treeï¼Œlogistic regression å°±å¾ˆå®¹æ˜“ç†è§£ï¼‰
	- æ¯”è¾ƒéš¾ï¼Œ æœ‰feature weightsï¼Œ but they don't correspond 1-1 to feature importance such as in logistic regression
- minimum æ˜¯å¦ = global minimum  (æ˜¯å¦æ˜¯convex function)
	- æ˜¯convex


## KNN è¯„ä»·

- data çš„assumption å¤šä¸å¤š 
	- assuming similar points situated closely with each other.
- data æ˜¯å¦éœ€è¦å…ˆè¿›è¡Œscale
	- è¦ï¼Œå› ä¸ºè¦è®¡ç®—è·ç¦»
- higher dimension é«˜ç»´ç‰¹å¾ è¡¨ç°èƒ½åŠ› ï¼ˆæ¯”å¦‚featureæ¯”sampleè¿˜å¤šï¼‰
	- ä¸å¤ªè¡Œï¼Œå› ä¸ºé«˜ç»´çš„æ—¶å€™ï¼š
		- Our assumption of similar points being situated closely breaksï¼š high dimension creates exponentially higher space, points tend to never be close together.
		- Â  It becomes computationally more expensive to compute distance and find the nearest neighbors in high-dimensional space

- æ˜¯å¦sensitive to outlier:
	- If â€˜Kâ€™ value is low, the model is susceptible to outliers. => Let take K=1, suppose there is 1 outlier near to our test point and then the model predicts the label corresponding to the outlier.
	-   If â€˜Kâ€™ value is high, the model is robust to outliers.
Â Â 
- æ›´å€¾å‘äºoverfit (low bias, high variance),  è¿˜æ˜¯underfit
	- k å°ï¼Œ é«˜ç»´æ•°æ®ï¼š overfit
	- k å¤§ï¼Œä½ç»´æ•°æ®ï¼š underfit
- è®­ç»ƒé€Ÿåº¦
	- æ…¢Â Â for each testing sample, it requires calculating distance with each training sample
- hyper parameter tuningçš„éš¾åº¦/éº»çƒ¦ç¨‹åº¦
	- k å¾ˆéš¾å†³å®šï¼Œè¦ç”¨hyperparameter tuning
- å¯¹äºimbalanced datasetçš„å¤„ç†èƒ½åŠ›
	- theoretically, unbalanced classes are not a problem at all for the k-nearest neighbor algorithm.
		- Because the algorithm is not influenced in any way by the size of the class, it will not favor any on the basis of size.
	- but:  there may not be sufficient patterns belonging to the minority class to adequately represent its distribution.
- æ¨¡å‹æœ¬èº«æ˜¯ä¸æ˜¯å¾ˆå®¹æ˜“ç†è§£ï¼ˆæ¯”å¦‚decision treeï¼Œlogistic regression å°±å¾ˆå®¹æ˜“ç†è§£ï¼‰
	- not interpretable
