---
title: 评价模型 Model Review
date: 2022-09-20 20:13:28
permalink: /pages/6b4f6c/
categories:
  - 机器学习八股文
  - Machine Learning General
tags:
  - 
---
- data 的assumption 多不多 (比如feature相互独立，残差正态分布之类的)
- data 是否需要先进行scale
- higher dimension 高维特征 表现能力 （比如feature比sample还多）
	- feature > sample 不能求解的方法： linear regression, logistic regression （但是加了 L1/L2 regularization 就可以求解）
	- 是否倾向于得到稀疏解 （是否可以通过求解这个regression的结果去掉一些没用的feature）
		- 倾向于得到稀疏解的方法: L1 regularization, ReLU activation function
- 是否sensitive to outlier:
	- good model: SVM
- 更倾向于overfit (low bias, high variance),  还是underfit
	- overfit: Decision Tree, Neural Network
- 训练速度
- hyper parameter tuning的难度/麻烦程度
- 对于imbalanced dataset的处理能力
- 模型本身是不是很容易理解（比如decision tree，logistic regression 就很容易理解）
- 能不能得到interpretable的feature importance
- minimum 是否 = global minimum  (是否是convex function)

## Linear Regression 评价
- data assumption 多：
	- residuals
		- iid
		- normal distribution with mean=0 (如果不进行beta置信区间的估计，就是mean 0, constant variance)
		- independent of X
	- X / features:
		- independent with each other
		- linearity: assuming relationship y = wx + b
	关于residual的见下图
	![](https://raw.githubusercontent.com/emmableu/image/master/202210032015214.png)
- data does not need scaling:对于线性模型 y = wx + b 而言，x的任意线性变换(平移，放缩)都会反应在w，b上，所以不会影响模型的拟合能力。
- does not work well with high dimension (many features)
	- feature > sample 不能求解， 
	- 大部分beta都不会完全是0， 所以不可以通过求解这个regression的结果去掉一些没用的feature
	- prone to overfit if there are many features / dimensions, causing high variance. 
- sensitive to outliers (因为 during loss function, treat each sample equally，including the outliers)
- 如果assumption完全满足的话，根据gaussian-markov theorem, it does pretty good in terms of balancing under- and over- fit. 
	- 但是，如果assumption 不满足的话， 更倾向于underfit, 因为model 相对decision tree， neural network 更加简单
- 训练速度快，有 analytical solution, 不需要 iteratively do gradient descent
- hyper parameter tuning 容易，没有什么hyperparameter
- （因为是continuous target variable，不需要讨论imbalance问题）
- model is very interpretable
- feature importance is very interpretable - can just look at the p value of the beta. 
- MSE is a convex function, so gradient descent can always reach global minima

## Lasso / Ridge (L1 / L2 Regularization) Regression 评价
- data assumption 比 linear regression 少: 不需要残差正态分布，因为得到的beta不是unbiased的，不符合t分布
	-  需要的assumption：
		- residual:
			- iid
			- 0 mean, constant variance
			- independent of X
		- X / feature:
			- independent with each other
			- linearity: assuming relationship y = wx + b
- data need scaling
	- ![](https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-2.png)

- works better with high dimension:
	- L1 allows sparcity -> some beta will be fitted to be 0, so, Lasso Regularization can exclude useless variables from the model and, in general, tends to perform well when we need to remove a lot of useless variables from a model.
	- 
