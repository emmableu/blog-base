---
title: 评价模型 Model Review
date: 2022-09-20 20:13:28
permalink: /pages/6b4f6c/
categories:
  - 机器学习八股文
  - Machine Learning General
tags:
  - 
---
- data 的assumption 多不多 (比如feature相互独立，残差正态分布之类的)
- data 是否需要先进行scale
- higher dimension 高维特征 表现能力 （比如feature比sample还多）
	- feature > sample 不能求解的方法： linear regression, logistic regression （但是加了 L1/L2 regularization 就可以求解）
	- 是否倾向于得到稀疏解 （是否可以通过求解这个regression的结果去掉一些没用的feature）
		- 倾向于得到稀疏解的方法: L1 regularization, ReLU activation function， svm with hinge loss
- 是否sensitive to outlier:
	- good model: SVM
- 更倾向于overfit (low bias, high variance),  还是underfit
	- overfit: Decision Tree, Neural Network
- 训练速度
- hyper parameter tuning的难度/麻烦程度
- 对于imbalanced dataset的处理能力
- 模型本身是不是很容易理解（比如decision tree，logistic regression 就很容易理解）
- 能不能得到interpretable的feature importance
- minimum 是否 = global minimum  (是否是convex function)

## Linear Regression 评价
- data assumption 多：
	- residuals
		- iid
		- normal distribution with mean=0 (如果不进行beta置信区间的估计，就是mean 0, constant variance)
		- independent of X
	- X / features:
		- independent with each other
		- linearity: assuming relationship y = wx + b
	关于residual的见下图
	![](https://raw.githubusercontent.com/emmableu/image/master/202210032015214.png)
- data does not need scaling:对于线性模型 y = wx + b 而言，x的任意线性变换(平移，放缩)都会反应在w，b上，所以不会影响模型的拟合能力。
- does not work well with high dimension (many features)
	- feature > sample 不能求解， 
	- 大部分beta都不会完全是0， 所以不可以通过求解这个regression的结果去掉一些没用的feature
	- prone to overfit if there are many features / dimensions, causing high variance. 
- sensitive to outliers comparing to models like SVM (因为 during loss function, treat each sample equally，including the outliers)
- 如果assumption完全满足的话，根据gaussian-markov theorem, it does pretty good in terms of balancing under- and over- fit. 
	- 但是，如果assumption 不满足的话， 更倾向于underfit, 因为model 相对decision tree， neural network 更加简单
- 训练速度快，有 analytical solution, 不需要 iteratively do gradient descent
- hyper parameter tuning 容易，没有什么hyperparameter
- （因为是continuous target variable，不需要讨论imbalance问题）
- model is very interpretable
- feature importance is very interpretable - can just look at the p value of the beta. 
- MSE is a convex function, so gradient descent can always reach global minima

## Logistic Regression 评价
- data 的assumption 多不多 (比如feature相互独立，残差正态分布之类的)
	- 比linear regression少很多，所有关于residual的都不需要
	- 只需要：
		- 	X / features:
			- independent with each other
			- linearity: assumes linearity of independent variables and log odds.
- data 是否需要先进行scale
	- 要，因为要用gradient descent 求解
- higher dimension 高维特征 表现能力 （比如feature比sample还多
	- 不好
	- 大部分beta都不会完全是0， 所以不可以通过求解这个regression的结果去掉一些没用的feature
	- prone to overfit if there are many features / dimensions, causing high variance. 
- 是否sensitive to outlier:
	- 不如SVM
- 更倾向于overfit (low bias, high variance),  还是underfit
	- more underfit comparing to neural network
	- more overfit comparing to naive bayes. 
- 训练速度
	- relatively low comparing to linear regression or naive bayes
- hyper parameter tuning的难度/麻烦程度
	- 好像没有 hyper parameter， 可以加regularization
- 模型本身是不是很容易理解（比如decision tree，logistic regression 就很容易理解）
	- 是, 可以用odds ratio The odds ratio represents the odds that an outcome will occur given the presence of a specific predictor,
- 能不能得到interpretable的feature importance
	- 能
- minimum 是否 = global minimum  (是否是convex function)
	- 是，loss function is convex
- （跳过）对于imbalanced dataset的处理能力 
	- there may not be sufficient patterns belonging to the minority class to adequately represent its distribution.

## Lasso / Ridge (L1 / L2 Regularization) Regression 评价
- data assumption 比 linear regression 少: 不需要残差正态分布，因为得到的beta不是unbiased的，不符合t分布
	-  需要的assumption：
		- residual:
			- iid
			- 0 mean, constant variance
			- independent of X
		- X / feature:
			- don't need to be completely independent with each other
			- linearity: assuming relationship y = wx + b
- data need scaling
	- ![](https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-2.png)

- works better with high dimension:
	- L1 allows sparcity -> some beta will be fitted to be 0, so, Lasso Regularization can exclude useless variables from the model and, in general, tends to perform well when we need to remove a lot of useless variables from a model.

- sensitive to outliers comparing to models like SVM 
- comparing to LR tends to underfit if too big lambda
- hyper parameter tuning 容易, 只有lambda
- model is very interpretable
- minimum 是否 = global minimum  (是否是convex function):
	- L2 Ridge regression: loss function is convex,  gradient descent can always reach global minima
	- L1 Lasso: sometimes not convex, so gradient descent don't always reach global minima



## Naive Baiyes 评价
- data 的assumption 多不多：
	- 有一个： features need to be independent with each other
- data 是否需要先进行scale
	- no
- higher dimension 高维特征 表现能力 （比如feature比sample还多）
	- If your data has 𝑘 dimensions, then a fully general ML algorithm which attempts to learn all possible correlations between these features has to deal with 2𝑘 possible feature interactions, and therefore needs on the order of 2𝑘 many data points to be performant. 
	- However because Naive Bayes assumes independence between features, it only needs on the order of 𝑘 many data points, exponentially fewer.
- 是否sensitive to outlier:
	- sensitive to outliers comparing to SVM
- 更倾向于overfit (low bias, high variance),  还是underfit
	- underfit, since the interactions are not modeled, some of the information in the data is ignored. This makes it an inherently high bias model; it has a high approximation error but as a result it also does not overfit.
- 训练速度
	- fast
- hyper parameter tuning的难度/麻烦程度
	- easy, only one hyperparameter - pseudocount
- 对于imbalanced dataset的处理能力
	- not good
	- ![](https://raw.githubusercontent.com/emmableu/image/master/202210050154011.png)
	- Even though the likelihood probabilities are similar to some extent, but the posterior probability is badly affected by prior probabilities. Here in the above example the class +ve prior probability will be 9 times higher than the class -ve, this difference in naive bayes creates a bias for class +ve.
	- One simple solution is to ignore the prior probabilities. (Or) You can do undersampling or oversampling.
- (跳过)模型本身是不是很容易理解（比如decision tree，logistic regression 就很容易理解）跳过
- (跳过)能不能得到interpretable的feature importance 跳过
- (跳过)minimum 是否 = global minimum  (是否是convex function) 跳过


## Decision Tree 评价
- data 的assumption 多不多 (比如feature相互独立，残差正态分布之类的)
	- 好像没有
- data 是否需要先进行scale
	- no need
- higher dimension 高维特征 表现能力
	- tends to become very overfit,  the number of branches grows exponentially with the number of features
- 是否sensitive to outlier:
	- not sensitive to outliers since the partitioning happens based on the proportion of samples within the split ranges and not on absolute values.
	- especially when we use early stopping
- 更倾向于overfit (low bias, high variance),  还是underfit
	- overfit,very specific rules
- 训练速度
	- not as fast comparing to logistic regression / naive bayes, but still fast enough comparing to neural network
	- test time is fast as it's just linearly traversing the test rules. 
- hyper parameter tuning的难度/麻烦程度
	- 好像没有hyper parameter，除非定义early stopping
- 对于imbalanced dataset的处理能力
	- 不太好
	- Decision trees implementations normally use Gini impurity for finding splits. For each rule/condition, when calculating the gini impurity it is a weighted average on. it weights higher on the more prevaling condition (e.g., loves troll when predicting loves popcorn). But more prevaling sample has a higher say on the more prevaling condition. 
- 模型本身是不是很容易理解（比如decision tree，logistic regression 就很容易理解）
	- yes. has interpretable rules
- (跳过) 能不能得到interpretable的feature importance 
- (跳过) minimum 是否 = global minimum  (是否是convex function) 跳过

## SVM 评价

- data 的assumption 多不多 (比如feature相互独立，残差正态分布之类的)
	- 比较宽松，只需要Data is linearly separable. Even if the linear boundary is in an augmented feature space. for example, after the kernel trick
- data 是否需要先进行scale
	- 要　
	- SVM tries to maximize the distance between the separating plane and the support vectors.
	- If one feature (i.e. one dimension in this space) has very large values, it will dominate the other features when calculating the distance. 
	- If you rescale all features (e.g. to [0, 1]), they all have the same influence on the distance metric.
- higher dimension 高维特征 表现能力 （比如feature比sample还多）
	- 还行，因为它会通过fit这个model来放缩 c => regularization parameter, the degree to which our model will accept misclassifications in our dataset， so that it generalise well over training data.
	- 是否倾向于得到稀疏解 （是否可以通过求解这个model的结果去掉一些没用的feature）
		- 用hinge loss 作为loss function， 因为hinge loss是 = max(0, 1-z), 所以>1时是0，所以会有 0 解
- 是否sensitive to outlier:
	- 不sensitive to outlier，很robust，还是因为它会通过fit这个model来放缩 c，见上
		- 而且只关心soft margin 周围的点
	- good model: SVM
- 更倾向于overfit (low bias, high variance),  还是underfit
	- 比起lr 和 nn 和 decision tree 这些更容易 underfit, 因为有soft margin 和 regularization parameter　
- 训练速度
	- 不是很快，因为还是要用gradient descent
- hyper parameter tuning的难度/麻烦程度
	- 麻烦，比如确定哪个kernel
- 模型本身是不是很容易理解（比如decision tree，logistic regression 就很容易理解）
	- 比较难， 有feature weights， but they don't correspond 1-1 to feature importance such as in logistic regression
- minimum 是否 = global minimum  (是否是convex function)
	- 是convex


## KNN 评价

- data 的assumption 多不多 
	- assuming similar points situated closely with each other.
- data 是否需要先进行scale
	- 要，因为要计算距离
- higher dimension 高维特征 表现能力 （比如feature比sample还多）
	- 不太行，因为高维的时候：
		- Our assumption of similar points being situated closely breaks： high dimension creates exponentially higher space, points tend to never be close together.
		-   It becomes computationally more expensive to compute distance and find the nearest neighbors in high-dimensional space

- 是否sensitive to outlier:
	- If ‘K’ value is low, the model is susceptible to outliers. => Let take K=1, suppose there is 1 outlier near to our test point and then the model predicts the label corresponding to the outlier.
	-   If ‘K’ value is high, the model is robust to outliers.
  
- 更倾向于overfit (low bias, high variance),  还是underfit
	- k 小， 高维数据： overfit
	- k 大，低维数据： underfit
- 训练速度
	- 慢  for each testing sample, it requires calculating distance with each training sample
- hyper parameter tuning的难度/麻烦程度
	- k 很难决定，要用hyperparameter tuning
- 对于imbalanced dataset的处理能力
	- theoretically, unbalanced classes are not a problem at all for the k-nearest neighbor algorithm.
		- Because the algorithm is not influenced in any way by the size of the class, it will not favor any on the basis of size.
	- but:  there may not be sufficient patterns belonging to the minority class to adequately represent its distribution.
- 模型本身是不是很容易理解（比如decision tree，logistic regression 就很容易理解）
	- not interpretable
