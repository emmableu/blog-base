---
title: Support Vector Machine (SVM)
date: 2021-11-07 16:28:25
permalink: /pages/12e393/
categories:
  - 机器学习八股文
  - Machine Learning Models
tags:
  - 
---


[svm youtube](https://www.youtube.com/watch?v=efR1C6CvhmE&t=416s)

margin: distance between observation and margin

- maximum margin classifier problem: (problem with hard margins)
	- sensitive to outliers

- soft margin: when we allow misclassifications, the distance between the observations and threshold is soft margin. 
	- how to decide soft margin: use it as a hyperparameter and use cross validation

- soft margin classifier / support vector classifier:
- what's support vector: the observations on the edge and within the soft margin are called support vectors. 

![](https://raw.githubusercontent.com/emmableu/image/master/202209191710708.png)


the kernel trick:
- calculate the relationship between data as if they are in higher dimension (when they are actually not in higher dimension).


## Intuitions, What is Support Vector


<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-0.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-1.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-2.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-3.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-4.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-5.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-6.png" width="100%">

## Definition of Hyperplanes and Margin
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-7.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-8.png" width="100%">

## Using Lagrangian Multipler Method to Maximize the Margin
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-9.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-10.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-11.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-12.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-13.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-14.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-15.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-16.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-17.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-18.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-19.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-20.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-21.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-22.png" width="100%">

## Inner Products, similarity, and SVMs
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-23.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-24.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-25.png" width="100%">

## Non-Linear SVMs
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-26.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-27.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-28.png" width="100%">

举个例子，对于这个 polynomial kernel (a*b + 1/2)^2, 其实也就是 = (a, a^2, 1/2) 点乘 (b, b^2, 1/2)

<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-29.png" width="100%">
<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-30.png" width="100%">

除了上面这三个，还有一个比较常见的是Gaussian kernel 

<img src="https://raw.githubusercontent.com/emmableu/image/master/svm-31.png" width="100%">


## Soft margin, regularization, and Surrogate loss function
![](https://raw.githubusercontent.com/emmableu/image/master/svm-32.png)
![](https://raw.githubusercontent.com/emmableu/image/master/svm-33.png)
![](https://raw.githubusercontent.com/emmableu/image/master/svm-34.png)
![](https://raw.githubusercontent.com/emmableu/image/master/svm-35.png)
![](https://raw.githubusercontent.com/emmableu/image/master/svm-36.png)

## Difference between SVM and Logistic Regression
- LR gives calibrated probabilities that can be interpreted as confidence in a decision.
- LR gives us an unconstrained, smooth objective.
- SVMs don’t penalize examples for which the correct decision is made with sufficient confidence. This may be good for generalization.
- SVMs have a nice dual form, giving sparse solutions when using the kernel trick (better scalability)

## How to choose which kernel to use
- （1）如果特征维数很高，往往线性可分，可以采用LR或者线性核的SVM；
- （2）如果样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM；
- （3）如果不满足上述两点，即特征维数少，样本数量正常，可以使用高斯核/RBF核的SVM。
