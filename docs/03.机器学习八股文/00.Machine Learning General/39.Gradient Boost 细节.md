---
title: Gradient Boost 细节
date: 2022-09-25 15:20:23
permalink: /pages/049472/
categories:
  - 机器学习八股文
  - Machine Learning General
tags:
  - 
---

## gradient boost 和 adaboost 区别
1. 组成部分：
	1) adaboost starts by building a very short tree, called a **stump**, from the training data.
	2) gradient boost starts by making a small leaf, instead of a tree or stump, this leaf represents an initial guess for the weights of all of the samples. 然后gradient boost 会建树，但是会比一个stump大
2. prediction target:
	1. adaboost predicts the final value
	2. gradient boost predicts the residuals


## gradient boost 过程
Suppose, we were trying to predict the price of a house given their age, square footage and location.

![](https://raw.githubusercontent.com/emmableu/image/master/202209302210359.png)


### Step 1: Calculate the average of the target label

When tackling regression problems, we start with a leaf that is the average value of the variable we want to predict. This leaf will be used as a baseline to approach the correct solution in the proceeding steps.

![](https://raw.githubusercontent.com/emmableu/image/master/202209302210774.png)


### Step 2: Calculate the residuals

For every sample, we calculate the residual with the proceeding formula.

> residual = actual value – predicted value

In our example, the predicted value is the equal to the mean calculated in the previous step and the actual value can be found in the price column of each sample. After computing the residuals, we get the following table.

![](https://raw.githubusercontent.com/emmableu/image/master/202209302211021.png)

### Step 3: Construct a decision tree

Next, we build a tree with the goal of predicting the residuals. In other words, every leaf will contain a prediction as to the value of the residual (not the desired label).

![](https://raw.githubusercontent.com/emmableu/image/master/202209302213623.png)

In the event there are more residuals than leaves, some residuals will end up inside the same leaf. When this happens, we compute their average and place that inside the leaf.

Thus, the tree becomes:

![](https://raw.githubusercontent.com/emmableu/image/master/202209302213246.png)

### Step 4: Predict the target label using all of the trees within the ensemble

Each sample passes through the decision nodes of the newly formed tree until it reaches a given lead. The residual in said leaf is used to predict the house price.

例如，对左下角的这个node，就是这样算：

![](https://raw.githubusercontent.com/emmableu/image/master/202209302220441.png)


或者中间的这个node：

predicted_price = 688 + 0.1 * (-208) = 667.2

### Step 5: Compute the new residuals

![](https://raw.githubusercontent.com/emmableu/image/master/202209302301353.png)



### Step 6: Repeat steps 3 - 5 until the number of iterations matches the number specified by the hyperparameter (i.e., # iterations)

![](https://raw.githubusercontent.com/emmableu/image/master/202209302223105.png)

### Step 7: Once trained, use all of the trees in the ensemble to make a final prediction as to the value of the target variable

The final prediction will be equal to the mean we computed in the first step, plus all of the residuals predicted by the trees that make up the forest multiplied by the learning rate.

![](https://raw.githubusercontent.com/emmableu/image/master/202209302223078.png)

## XGBoost step by step
Let’s start with our training dataset which consists of five people. We recorded their ages, whether or not they have a master’s degree, and their salary (in thousands). Our goal is to predict _Salary_ using the XGBoost Algorithm.

![](https://miro.medium.com/max/1400/1*_KUzdQTdTUfjPposrAxSfQ.png)

### Step 1: Make an Initial Prediction and Calculate Residuals

[source](https://towardsdatascience.com/xgboost-regression-explain-it-to-me-like-im-10-2cf324b0bbdb)

This prediction can be anything. But let’s assume our initial prediction is the average value of the variables we want to predict.

![](https://miro.medium.com/max/1400/1*GjxcMQDQQqU-UfgfvBeb1A.png)

We can calculate residuals using the following formula:

![](https://miro.medium.com/max/1400/1*AEwarDLHuDQZmMlB-2VDpw.png)

Here, our Observed Values are the values in the _Salary_ column and all Predicted Values are equal to 70 because that is what we chose our initial prediction to be.

![](https://miro.medium.com/max/1400/1*D5eCwr_TYcwuDP8NBOAOsg.png)

### Step 2: Build an XGBoost Tree

Each tree starts with a single leaf and all the residuals go into that leaf.

![](https://miro.medium.com/max/1036/1*yYq5mmVkYk1Lwl_aspUq6A.png)

Now we need to calculate something called a **Similarity Score** of this leaf.

![](https://miro.medium.com/max/1400/1*ddxctrOTVbqPhppkcxCzFw.png)

λ (lambda) is a regularization parameter that reduces the prediction’s sensitivity to individual observations and prevents the overfitting of data (this is when a model fits exactly against the training dataset). The default value of λ is 1 so we will let λ = 1 in this example.

![](https://miro.medium.com/max/1400/1*3bc39LtxpRTqQ0kivyrQlw.png)

Now we should see if we can do a better job clustering the residuals if we split them into two groups using thresholds based on our predictors — _Age_ and _Master’s Degree?._ Splitting the _Residuals_ basically means that we are adding branches to our tree.

First, let’s try splitting the leaf using _Master’s Degree_**?**

![](https://miro.medium.com/max/1400/1*iofkdqDDgFDhOHRxjmjz5w.png)

And then calculate the **Similarity Scores** for the left and right leaves of the above split:

![](https://miro.medium.com/max/1400/1*rNlX9qaQqc8xGSUXvkwkKg.png)

Now we need to quantify how much better the leaves cluster similar _Residuals_ than the root does. We can do this by calculating the **Gain** of splitting the _Residuals_ into two groups. If the **Gain** is positive, then it’s a good idea to split, otherwise, it is not.

![](https://miro.medium.com/max/1222/0*rIeNeoesHLox_5p8.png)

Then we compare this **Gain** to those of the splits in _Age_. Since _Age_ is a continuous variable, the process to find the different splits is a little more involved. First, we arrange the rows of our dataset according to the ascending order of _Age_. Then we calculate the average values of the adjacent values in _Age_.

![](https://miro.medium.com/max/550/0*JruyoPHXdM9LxPPW)

Now we split the _Residuals_ using the four averages as thresholds and calculate **Gain** for each of the splits. The first split uses _Age < 23.5_:

![](https://miro.medium.com/max/1222/1*4285F3yPyqgvkDf0a3JtJA.png)

For this split, we find the **Similarity Score** and **Gain** the same way we did for _Master’s Degree?_

![](https://miro.medium.com/max/1400/1*QG-8M5rLc_WFo293kdn32Q.png)

Do the same thing for the rest of the _Age_ splits:

![](https://miro.medium.com/max/1400/1*HrG-WfVT9O5ve7Yso3dL9g.png)

Out of the one _Mater’s Degree?_ split and four _Age_ splits, the _Master’s Degree_ split has the greatest **Gain** value, so we’ll use that as our initial split. Now we can add more branches to the tree by splitting our _Master’s Degree?_ leaves again using the same process described above. But, only this time, we use the initial _Master’s Degree?_ leaves as our root nodes and try splitting them by getting the greatest **Gain** value that is greater than 0.

Let’s start with the left node. For this node, we only consider the observations that have the value ‘Yes’ in _Master’s Degree?_ because only those observations land in the left node.

![](https://miro.medium.com/max/1400/1*ZaDD16j0hBb3kIp4iOgPcA.png)

So we calculate the **Gain** of the _Age_ splits using the same process as before, but this time using the _Residuals_ in the highlighted rows only.

![](https://miro.medium.com/max/1400/1*ArdKYPYzu9oRcJCsoOgOKQ.png)

Since only _Age < 25_ gives us a positive **Gain**, we split the left node using this threshold. Moving onto our right node, we only look at values with ‘No’ values in _Master’s Degree?_

![](https://miro.medium.com/max/1400/1*wT0cScVpEqPSa4OAJSuJLg.png)

We only have two observations in our right node, so the only split possible is _Age < 24.5_ because that is the average of the two _Age_ values in the highlighted rows.

![](https://miro.medium.com/max/1400/1*I4xa7BIP1YcZOyl_K1eBEg.png)

The **Gain** of this split is positive, so our final tree is:

![](https://miro.medium.com/max/1298/1*LOq80DzUJSo3ndw1P9PoAQ.png)

### **_Step 3: Prune the Tree_**

Pruning is another way we can avoid overfitting the data. To do this we start from the bottom of our tree and work our way up to see if a split is valid or not. To establish validity, we use γ (gamma). If **Gain —** γ is positive then we keep the split, otherwise, we remove it. The default value of γ is 0, but for illustrative purposes, let’s set our γ to 50. From previous calculations we know the **Gain** values:

![](https://miro.medium.com/max/1400/1*I1ukSNGxqbB1tAHZBqw7qA.png)

Since **Gain —** γ is positive for all splits except that of _Age < 24.5_, we can remove that branch. So the resulting tree is:

![](https://miro.medium.com/max/1168/1*n0b6WmYcJXw4UCpy0DCGRA.png)

### **_Step 4: Calculate the Output Values of Leaves_**

We are almost there! All we have to do now is calculate a single value in our leaf nodes because we can not have a leaf node giving us multiple outputs.

![](https://miro.medium.com/max/1400/0*XmYMrpfsKZF6XWOb)

This is similar to the formula to calculate **Similarity Score** except we are not squaring the _Residuals_. Using the formula and λ = 1, _*drum roll*_ our final tree is:

![](https://miro.medium.com/max/1400/1*FSIFRB1N8bR52vixSWR19A.png)

### **_Step 5: Make New Predictions_**

Now that all that hard model building is behind us, we come to the exciting part and see how much our predictions improve using our new model. We can make predictions using this formula:

![](https://miro.medium.com/max/1400/1*tIEzTeLj_yHOIQWVVqPQ9w.png)

The XGBoost Learning Rate is ɛ (eta) and the default value is 0.3. So the predicted value of our first observation will be:

![](https://miro.medium.com/max/1400/1*EYwsm84uQB4-WNQfezfwCA.png)

Similarly, we can calculate the rest of the predicted values:

![](https://miro.medium.com/max/1400/1*7tDf4x_6I-WehIG2yI7BMw.png)

### **_Step 6: Calculate Residuals Using the New Predictions_**

![](https://miro.medium.com/max/1400/1*tIqXoaKD_41QeCBfe2xxqQ.png)

We see that the new _Residuals_ are smaller than the ones before, this indicates that we’ve taken a small step in the right direction. As we repeat this process, our _Residuals_ will get smaller and smaller indicating that our predicted values are getting closer to the observed values.

### **_Step 7: Repeat Steps 2–6_**

Now we just repeat the same process over and over again, building a new tree, making predictions, and calculating _Residuals_ at each iteration. We do this until the _Residuals_ are super small or we reached the maximum number of iterations we set for our algorithm. If the tree we built at each iteration is indicated by Tᵢ, where _i_ is the current iteration, then the formula to calculate predictions is:

![](https://miro.medium.com/max/1400/1*MiHVOymdS80S8o_uv96Ubw.png)

And that’s it. Thanks for reading and good luck with the rest of your algorithmic journey!

