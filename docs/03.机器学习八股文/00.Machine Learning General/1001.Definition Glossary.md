---
title: Definition Glossary
date: 2022-09-29 15:24:37
permalink: /pages/1792d7/
categories:
  - 机器学习八股文
  - Machine Learning General
tags:
  - 
---
[source](https://developers.google.com/machine-learning/glossary#a)

webwhiteboard website: https://webwhiteboard.com/

## a
- accuracy: The fraction of predictions that a classification model got right
- activation function: A function (for example, ReLU or sigmoid) that takes in the weighted sum of all of the inputs from the previous layer and then generates and passes an output value (typically nonlinear) to the next layer.
- adaboost: [local link](/pages/63f233/#例子-adaboost-adaptive-boost)
- attention: Any of a wide range of [**neural network**](https://developers.google.com/machine-learning/glossary#neural_network) architecture mechanisms that aggregate information from a set of inputs in a data-dependent manner. A typical attention mechanism might consist of a weighted sum over a set of inputs, where the [**weight**](https://developers.google.com/machine-learning/glossary#weight) for each input is computed by another part of the neural network.
-  AUC (Area under the ROC Curve): An evaluation metric that considers all possible classification thresholds. The Area Under the ROC curve is the probability that a classifier will be more confident that a randomly chosen positive example is actually positive than that a randomly chosen negative example is positive.
## b
- backpropagation: [local link](/pages/2a7490/)
- bagging: [local link](/pages/63f233/#bagging-bootstrap-aggregating-concept/)
- bag of words: 
	- A representation of the words in a phrase or passage, irrespective of order. For example, bag of words represents the following three phrases identically:
		- the dog jumps
		- jumps the dog
		- dog jumps the
	- Each word is mapped to an index in a sparse vector, where the vector has an index for every word in the vocabulary. For example, the phrase the dog jumps is mapped into a feature vector with non-zero values at the three indices corresponding to the words the, dog, and jumps. The non-zero value can be any of the following:
		- A 1 to indicate the presence of a word.
		- A count of the number of times a word appears in the bag.
		- Some other value, such as the logarithm of the count of the number of times a word appears in the bag.
- batch: The set of examples used in one iteration (that is, one gradient update) of model training.
- batch normalization: [local link](/pages/f405d4/)
- batch size: The number of examples in a batch. For example, the batch size of SGD is 1, while the batch size of a mini-batch is usually between 10 and 1000. 
- boosting: [local link](/pages/63f233/#boosting-concept)
## c
- centroid: The center of a cluster as determined by a k-means or k-median algorithm. For instance, if k is 3, then the k-means or k-median algorithm finds 3 centroids.
-  centroid-based clustering: A category of clustering algorithms that organizes data into nonhierarchical clusters. k-means is the most widely used centroid-based clustering algorithm. Contrast with hierarchical clustering algorithms.
- collaborative filtering: Making predictions about the interests of one user based on the interests of many other users. Collaborative filtering is often used in recommendation systems.
- confusion matrix: An NxN table that aggregates a classification model's correct and incorrect guesses. One axis of a confusion matrix is the label that the model predicted, and the other axis is the ground truth. N represents the number of classes. For example, N=2 for a binary classification model.
- convolutional neural network: [local link](/pages/f75af9/)
- cross-entropy: [local link](/pages/710eb0/#cross-entropy-loss)
- cross-validation: A mechanism for estimating how well a model would generalize to new data by testing the model against one or more non-overlapping data subsets withheld from the training set.
## d
- decision tree: [local link](/pages/05b850/) 
- discriminative model: local link
- dropout: [local link](/pages/9e1504/)
## e
- early stopping: A method for reducing overfit that involves ending model training before training loss finishes decreasing. 
- eigendecomposition: [local link](/pages/0d8e27/#eigenvalue-decomposition)
- embeddings: 
	- A categorical feature represented as a continuous-valued feature. Typically, an embedding is a translation of a high-dimensional vector into a low-dimensional space. For example, you can represent the words in an English sentence in either of the following two ways:
	    - As a million-element (high-dimensional) sparse vector in which all elements are integers. 
		    - Each cell in the vector represents a separate English word; 
		    - the value in a cell represents the number of times that word appears in a sentence. 
		    - Since a single English sentence is unlikely to contain more than 50 words, nearly every cell in the vector will contain a 0. 
		    - The few cells that aren't 0 will contain a low integer (usually 1) representing the number of times that word appeared in the sentence.
	    - As a several-hundred-element (low-dimensional) dense vector in which each element holds a floating-point value between 0 and 1. This is an embedding.
	- embeddings are trained by backpropagating loss just like any other parameter in a neural network.
	- The [dot product](https://wikipedia.org/wiki/Dot_product) of two embeddings is a measure of their similarity.
- ensemble: A collection of models trained independently whose predictions are averaged or aggregated. In many cases, an ensemble produces better predictions than a single model. For example, a random forest is an ensemble built from multiple decision trees. Note that not all decision forests are ensembles.
## f
## g
- Gaussian Mixed Model (GMM): [local link](/pages/38f1b5/)
- generalization curve: A loss curve showing both the training set and the validation set. A generalization curve can help you detect possible overfitting.
- generative model:  
	- a model that does either of the following:
	    - Creates (generates) new examples from the training dataset. e.g., create poetry after training on a dataset of poems. (e.g., GAN)
	    - Determines the probability that a new example comes from the training set, or was created from the same mechanism that created the training set. e.g., after training on a dataset consisting of English sentences, a generative model could determine the probability that new input is a valid English sentence.(e.g., Naive Bayes)
	- A generative model can understand the distribution of examples or particular features in a dataset.
- gradient: The vector of partial derivatives with respect to all of the independent variables. In machine learning, the gradient is the vector of partial derivatives of the model function. The gradient points in the direction of steepest ascent.
-  gradient descent: A technique to minimize loss by computing the gradients of loss with respect to the model's parameters, conditioned on training data. In another word, gradient descent iteratively adjusts parameters, gradually finding the best combination of weights and bias to minimize loss.
## h
-  hierarchical clustering: 
	- A category of clustering algorithms that create a tree of clusters. Hierarchical clustering is well-suited to hierarchical data, such as botanical taxonomies. There are two types of hierarchical clustering algorithms:
		- Agglomerative clustering first assigns every example to its own cluster, and iteratively merges the closest clusters to create a hierarchical tree.
	    - Divisive clustering first groups all examples into one cluster and then iteratively divides the cluster into a hierarchical tree.
	- Contrast with centroid-based clustering.
- hinge loss: A family of loss functions for classification designed to find the decision boundary as distant as possible from each training example, thus maximizing the margin between examples and the boundary. SVMs use hinge loss (or a related function, such as squared hinge loss).
## i
## j
## k
-  k-means: 
	- A clustering algorithm that clusters samples by:
		1. Iteratively determines the best k center points.
		2.  Assigns each sample to the closest center points. Those samples nearest the same center points belong to the same group.
	- So, k-means algorithm picks center points to minimize the cumulative square of the distances from each sample to its closest center points
- knn: a classification algorithm, which classifies the new data points based on the similarity measure of the earlier stored data points. 
## l
- L1 regularization: A type of regularization that penalizes weights in proportion to the sum of the absolute values of the weights. In models relying on sparse features, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model. 
- L2 regularization: A type of regularization that penalizes weights in proportion to the sum of the squares of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. (Contrast with L1 regularization.) L2 regularization always improves generalization in linear models.
- learning rate: A scalar used to train a model via gradient descent. During each iteration, the gradient descent algorithm multiplies the learning rate by the gradient. The resulting product is called the gradient step.
-  logistic regression: A classification model that uses a sigmoid function to convert a linear model's raw prediction (y') into a value between 0 and 1. 
-  Long Short-Term Memory (LSTM): A type of cell in a recurrent neural network used to process sequences of data in applications such as handwriting recognition, machine translation, and image captioning. LSTMs address the vanishing gradient problem that occurs when training RNNs due to long data sequences by maintaining history in an internal memory state based on new input and context from previous cells in the RNN.
- loss: A measure of how far a model's predictions are from its label. 
- loss curve: A graph of loss as a function of training iterations. 
## m
-  mini-batch: A small, randomly selected subset of the entire batch of examples run together in a single iteration of training or inference. The batch size of a mini-batch is usually between 10 and 1,000. It is much more efficient to calculate the loss on a mini-batch than on the full training data.
## n
- normalization: The process of converting an actual range of values into a standard range of values, typically -1 to +1 or 0 to 1. 
## o
- overfitting: Creating a model that matches the training data so closely that the model fails to make correct predictions on new data.
## p
- Principle Component Analysis (PCA): [local link](/pages/0d8e27/)
## q
## r
-  random forest: [local link](/pages/63f233/#随机森林算法-random-forest)
- Rectified Linear Unit (ReLU): An activation function with the following rules:
    - If input is negative or zero, output is 0.
    - If input is positive, output is equal to input.
- recurrent neural network (RNN): A neural network that is intentionally run multiple times, where parts of each run feed into the next run. Specifically, hidden layers from the previous run provide part of the input to the same hidden layer in the next run. Recurrent neural networks are particularly useful for evaluating sequences, so that the hidden layers can learn from previous runs of the neural network on earlier parts of the sequence.
- regularization: The penalty on a model's complexity. Regularization helps prevent overfitting. Different kinds of regularization include:
	- L1 regularization
	- L2 regularization
	- dropout regularization
	- early stopping (this is not a formal regularization method, but can effectively limit overfitting)
- regularization rate: A scalar value, represented as lambda, specifying the relative importance of the regularization function. The following simplified loss equation shows the regularization rate's influence:
![](https://raw.githubusercontent.com/emmableu/image/master/202209292333236.png)
-  ROC (receiver operating characteristic) Curve: A curve of true positive rate vs. false positive rate at different classification thresholds. 
-  Root Mean Squared Error (RMSE): The square root of the Mean Squared Error.

## s
- scaling: A commonly used practice in feature engineering to tame a feature's range of values to match the range of other features in the dataset.
- softmax: A function that provides probabilities for each possible class in a multi-class classification model. The probabilities add up to exactly 1.0.
- Squared loss: The loss function used in linear regression. This function calculates the squares of the difference between a model's predicted value for a labeled example and the actual value of the label.
- Support Vector Machines (SVMs): A classification algorithm that seeks to maximize the margin between positive and negative classes by mapping input data vectors to a higher dimensional space. 
- sigmoid function: A function that maps logistic or multinomial regression output (log odds) to probabilities, returning a value between 0 and 1. 

## t
## u
- underfitting: Producing a model with poor predictive ability because the model hasn't captured the complexity of the training data. 
## v
- vanishing gradient problem: The tendency for the gradients of early hidden layers of some deep neural networks to become surprisingly flat (low). 
## w
## x
## y
## z

