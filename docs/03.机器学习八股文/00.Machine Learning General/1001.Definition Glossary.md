---
title: Definition Glossary
date: 2022-09-29 15:24:37
permalink: /pages/1792d7/
categories:
  - 机器学习八股文
  - Machine Learning General
tags:
  - 
---
[source](https://developers.google.com/machine-learning/glossary#a)

webwhiteboard website: https://webwhiteboard.com/

 ## 比较异同
 ### random forest v.s. gradient boost
- random forest: the process to train an ensemble where, each constituent model (a decision tree) trains on a random subset of training examples, sampled with replacement.
- gradient boosting: 
	- A training algorithm where weak models are trained to iteratively improve the quality (reduce the loss) of a strong model. For example, a weak model could be a linear or small decision tree model. The strong model becomes the sum of all the previously trained weak models.
	- at each iteration, a weak model is trained to predict the loss gradient of the strong model. Then, the strong model's output is updated by subtracting the predicted gradient, similar to gradient descent.
- 同： ensembling methods, combine the outputs from individual trees. 
- 异：
	- 从classification 结果来说：
		 - comparing to RF, gradient boost have a lot more modeling capacity. They can model very complex relationships and decision boundaries.
		 - gradient boost: low bias, high variance, can lead to overfitting
		 - decision tree: high bias, low variance, can cause underfitting. 
	 - 从classification 过程来说：
		 - [/pages/63f233/#bagging-和-boosting-的4-点差别](/pages/63f233/#bagging-和-boosting-的4-点差别)

### Generative v.s. Discrimitive Model
from [csdn](https://blog.csdn.net/Oh_MyBug/article/details/104343641)

- discriminative model: A model that predicts labels from a set of one or more features. The goal of a discriminative model is to understand the conditional probability of an output given the features and weights; that is: `p(output | features, weights)`
	- For example, a model that predicts whether an email is spam from features and weights 
	- Contrast with generative model.

generative model:
-   a model that does either of the following:
    -   Creates (generates) new examples from the training dataset. e.g., create poetry after training on a dataset of poems. (e.g., GAN)
    -   Determines the probability that a new example comes from the training set, or was created from the same mechanism that created the training set. e.g., after training on a dataset consisting of English sentences, a generative model could determine the probability that new input is a valid English sentence.(e.g., Naive Bayes)
-   A generative model can understand the distribution of examples or particular features in a dataset. 
<img src="https://raw.githubusercontent.com/emmableu/image/master/generative-model-0.png" width="100%">
simple generative model includes:
- naive bayes 
- LDA (Linear Discrimitative Analysis)
<img src="https://raw.githubusercontent.com/emmableu/image/master/generative-model-1.png" width="100%">

#### generative model pros and cons
#### pros
- 实际上带的信息比判别模型丰富
- 研究单类问题比判别模型灵活性强
- 能用于数据不完整情况, 基于概率分布的假设，所需的training data较少
- 很容易将先验知识考虑进去
- 稳健型好，当数据呈现不同特点时，分类性能不会出现太大的差异对noise比较robust
#### cons
- 容易产生错误分类:Naive Bayes里面假设每个事件都是independent的，比如00|01|10 & 11的分类，样本不均的时候可能会分错，因为model可能会脑补不存在的情况
- 学习和计算过程比较复杂


#### discrimitive model pros and cons
pros
- 分类边界更灵活，比使用纯概率方法或生成模型得到的更高级
- 能清晰的分辨出多类或某一类与其它类之间的差异特征
- 对于多feature的情况，feature之间多有correlation，比起naive bayes，models such as logistic regression is much more robust with correlated features. 
- 判别模型的性能比生成模型要简单，比较容易学习
cons
- 不能反应训练数据本身的特性，只能告诉你的是1还是2，不能把整个场景描述出来




### GMM v.s. K-Means
k-means:
-   A clustering algorithm that clusters samples by:
    1. Assigns each sample to the closest center points.
    2. Iteratively determines the best k center points.
-   So, k-means algorithm picks center points to minimize the cumulative square of the distances from each sample to its closest center points

GMM: 
-   A clustering algorithm that clusters samples by iteratively:
	- assign the probablilities each sample belong to each cluster, and
	- iteratively determine the best miu, and covariance big sigma for each of the clusters
- keep iterating until it reaches convergence. 

相同点  
都是迭代执行的算法，且迭代的策略也相同：算法开始执行时先对需要计算的参数赋初值，然后交替执行两个步骤，一个步骤是对数据的估计（k-means是估计每个点所属簇；GMM是计算隐含变量的期望；）;第二步是用上一步算出的估计值重新计算参数值，更新目标参数（k-means是计算簇心位置；GMM是计算各个高斯分布的中心位置和协方差矩阵）

不同点   
1）需要计算的参数不同：k-means是簇心位置；GMM是各个高斯分布的参数   
2）计算目标参数的方法不同：k-means是计算当前簇中所有元素的位置的均值；GMM是基于概率的算法，是通过计算似然函数的最大值实现分布参数的求解的。


## a
- accuracy: The fraction of predictions that a classification model got right
- activation function: A function (for example, ReLU or sigmoid) that takes in the weighted sum of all of the inputs from the previous layer and then generates and passes an output value (typically nonlinear) to the next layer.
- adaboost:
	1) AdaBoost combines a lot of “weak learners”, learners that are only slightly better at classifying the observations than random guesses, to make classifications. The weak learners are almost aways stumps, (stumps are  classification trees with only a root and two leaves).
	2) The better a stump is at correctly classifying the training data, get more say it gets in the final classification.
	3) Each stump is made by taking the previous stump’s mistakes into account.
- attention: Any of a wide range of [**neural network**](https://developers.google.com/machine-learning/glossary#neural_network) architecture mechanisms that aggregate information from a set of inputs in a data-dependent manner. A typical attention mechanism might consist of a weighted sum over a set of inputs, where the [**weight**](https://developers.google.com/machine-learning/glossary#weight) for each input is computed by another part of the neural network.
-  AUC (Area under the ROC Curve): An evaluation metric that considers all possible classification thresholds. The Area Under the ROC curve is the probability that a classifier will be more confident that a randomly chosen positive example is actually positive than that a randomly chosen negative example is positive.
## b
- backpropagation: The primary algorithm for performing gradient descent on neural networks. First, the output values of each node are calculated (and cached) in a forward pass. Then, the partial derivative of the error with respect to each parameter is calculated in a backward pass through the graph.
- bagging: bagging means bootstrap aggregating. It's a method to train an ensemble where, each constituent model trains on a random subset of training examples, sampled with replacement. For example, a random forest is a collection of decision trees trained with bagging. 
- bag of words: 
	- A representation of the words in a phrase or passage, irrespective of order. For example, bag of words represents the following three phrases identically:
		- the dog jumps
		- jumps the dog
		- dog jumps the
	- Each word is mapped to an index in a sparse vector, where the vector has an index for every word in the vocabulary. For example, the phrase the dog jumps is mapped into a feature vector with non-zero values at the three indices corresponding to the words the, dog, and jumps. The non-zero value can be any of the following:
		- A 1 to indicate the presence of a word.
		- A count of the number of times a word appears in the bag.
		- Some other value, such as the logarithm of the count of the number of times a word appears in the bag.
- batch: The set of examples used in one iteration (that is, one gradient update) of model training.
- batch normalization: Normalizing the input or output of the activation functions in a hidden layer. Batch normalization can provide the following benefits:
    - Make neural networks more stable by protecting against outlier weights.
    - Enable higher learning rates.
    - Reduce overfitting.
- batch size: The number of examples in a batch. For example, the batch size of SGD is 1, while the batch size of a mini-batch is usually between 10 and 1000. 
- boosting: A technique that iteratively combines a set of simple and not very accurate classifiers into a classifier with high accuracy by upweighting the examples that the model is currently misclassifying.
## c
- centroid: The center of a cluster as determined by a k-means or k-median algorithm. For instance, if k is 3, then the k-means or k-median algorithm finds 3 centroids.
-  centroid-based clustering: A category of clustering algorithms that organizes data into nonhierarchical clusters. k-means is the most widely used centroid-based clustering algorithm. Contrast with hierarchical clustering algorithms.
- collaborative filtering: Making predictions about the interests of one user based on the interests of many other users. Collaborative filtering is often used in recommendation systems.
- confusion matrix: An NxN table that aggregates a classification model's correct and incorrect guesses. One axis of a confusion matrix is the label that the model predicted, and the other axis is the ground truth. N represents the number of classes. For example, N=2 for a binary classification model.
- convolutional neural network: A neural network in which at least one layer is a convolutional layer. A typical convolutional neural network consists of some combination of the following layers:
    - convolutional layers
    - pooling layers
    - dense layers
	- Convolutional neural networks have had great success in certain kinds of problems, such as image recognition.
- cross-entropy: A generalization of Log Loss to multi-class classification problems. Cross-entropy quantifies the difference between two probability distributions. See also perplexity.
- cross-validation: A mechanism for estimating how well a model would generalize to new data by testing the model against one or more non-overlapping data subsets withheld from the training set.
## d
- decision tree: A supervised learning model composed of a set of conditions and leaves organized hierarchically. The goal of decision tree is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.
- discriminative model: A model that predicts labels from a set of one or more features. The goal of a discriminative model is to understand the conditional probability of an output given the features and weights; that is: `p(output | features, weights)`
	- For example, a model that predicts whether an email is spam from features and weights 
	- Contrast with generative model.
- dropout: A method to reduce overfitting, when training neural networks. Dropout removes a random selection of a fixed number of the units in a network layer for a single gradient step. The more units dropped out, the stronger the regularization. 
## e
- early stopping: A method for reducing overfit that involves ending model training before training loss finishes decreasing. 
- eigendecomposition: the factorization of a matrix Ax = λx, where λ is eigenvalue and x is eigen vector.
- embeddings: 
	- A categorical feature represented as a continuous-valued feature. Typically, an embedding is a translation of a high-dimensional vector into a low-dimensional space. For example, you can represent the words in an English sentence in either of the following two ways:
	    - As a million-element (high-dimensional) sparse vector in which all elements are integers. 
		    - Each cell in the vector represents a separate English word; 
		    - the value in a cell represents the number of times that word appears in a sentence. 
		    - Since a single English sentence is unlikely to contain more than 50 words, nearly every cell in the vector will contain a 0. 
		    - The few cells that aren't 0 will contain a low integer (usually 1) representing the number of times that word appeared in the sentence.
	    - As a several-hundred-element (low-dimensional) dense vector in which each element holds a floating-point value between 0 and 1. This is an embedding.
	- embeddings are trained by backpropagating loss just like any other parameter in a neural network.
	- The [dot product](https://wikipedia.org/wiki/Dot_product) of two embeddings is a measure of their similarity.
- ensemble: A collection of models trained independently whose predictions are averaged or aggregated. In many cases, an ensemble produces better predictions than a single model. For example, a random forest is an ensemble built from multiple decision trees. Note that not all decision forests are ensembles.
## f
## g
- Gaussian Mixed Model (GMM): 
	-   A clustering algorithm that clusters samples by iteratively:
		- assign the probablilities each sample belong to each cluster, and
		- iteratively determine the best miu, and covariance big sigma for each of the clusters
	- keep iterating until it reaches convergence. 
- generalization curve: A loss curve showing both the training set and the validation set. A generalization curve can help you detect possible overfitting.
- generative model:  
	- a model that does either of the following:
	    - Creates (generates) new examples from the training dataset. e.g., create poetry after training on a dataset of poems. (e.g., GAN)
	    - Determines the probability that a new example comes from the training set, or was created from the same mechanism that created the training set. e.g., after training on a dataset consisting of English sentences, a generative model could determine the probability that new input is a valid English sentence.(e.g., Naive Bayes)
	- A generative model can understand the distribution of examples or particular features in a dataset.
- gradient: The vector of partial derivatives with respect to all of the independent variables. In machine learning, the gradient is the vector of partial derivatives of the model function. The gradient points in the direction of steepest ascent.
- gradient boosting: 
	- A training algorithm where weak models are trained to iteratively improve the quality (reduce the loss) of a strong model. For example, a weak model could be a linear or small decision tree model. The strong model becomes the sum of all the previously trained weak models.
	- at each iteration, a weak model is trained to predict the loss gradient of the strong model. Then, the strong model's output is updated by subtracting the predicted gradient, similar to gradient descent.
-  gradient descent: A technique to minimize loss by computing the gradients of loss with respect to the model's parameters, conditioned on training data. In another word, gradient descent iteratively adjusts parameters, gradually finding the best combination of weights and bias to minimize loss.
## h
-  hierarchical clustering: 
	- A category of clustering algorithms that create a tree of clusters. Hierarchical clustering is well-suited to hierarchical data, such as botanical taxonomies. There are two types of hierarchical clustering algorithms:
		- Agglomerative clustering first assigns every example to its own cluster, and iteratively merges the closest clusters to create a hierarchical tree.
	    - Divisive clustering first groups all examples into one cluster and then iteratively divides the cluster into a hierarchical tree.
	- Contrast with centroid-based clustering.
- hinge loss: A family of loss functions for classification designed to find the decision boundary as distant as possible from each training example, thus maximizing the margin between examples and the boundary. SVMs use hinge loss (or a related function, such as squared hinge loss).
## i
## j
## k
-  k-means: 
	- A clustering algorithm that clusters samples by:
		1. Iteratively determines the best k center points.
		2.  Assigns each sample to the closest center points. Those samples nearest the same center points belong to the same group.
	- So, k-means algorithm picks center points to minimize the cumulative square of the distances from each sample to its closest center points
- knn: a classification algorithm, which classifies the new data points based on the similarity measure of the earlier stored data points. 
## l
- L1 regularization: A type of regularization that penalizes weights in proportion to the sum of the absolute values of the weights. In models relying on sparse features, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model. 
- L2 regularization: A type of regularization that penalizes weights in proportion to the sum of the squares of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. (Contrast with L1 regularization.) L2 regularization always improves generalization in linear models.
- learning rate: A scalar used to train a model via gradient descent. During each iteration, the gradient descent algorithm multiplies the learning rate by the gradient. The resulting product is called the gradient step.
-  logistic regression: A classification model that uses a sigmoid function to convert a linear model's raw prediction (y') into a value between 0 and 1. 
-  Long Short-Term Memory (LSTM): A type of cell in a recurrent neural network used to process sequences of data in applications such as handwriting recognition, machine translation, and image captioning. LSTMs address the vanishing gradient problem that occurs when training RNNs due to long data sequences by maintaining history in an internal memory state based on new input and context from previous cells in the RNN.
- loss: A measure of how far a model's predictions are from its label. 
- loss curve: A graph of loss as a function of training iterations. 
## m
-  mini-batch: A small, randomly selected subset of the entire batch of examples run together in a single iteration of training or inference. The batch size of a mini-batch is usually between 10 and 1,000. It is much more efficient to calculate the loss on a mini-batch than on the full training data.
## n
- normalization: The process of converting an actual range of values into a standard range of values, typically -1 to +1 or 0 to 1. 
## o
- overfitting: Creating a model that matches the training data so closely that the model fails to make correct predictions on new data.
## p
- Principle Component Analysis (PCA): A method to reduce dimension (reduce the amount of features) by finding the top few axis that capture the maximum amount of variance in the data.  The top few new axis are found by performing eigendecomposition of the covariance matrix, and find the eigenvectors that corresponds to the highest few eigenvalues. 
## q
## r
-  random forest: the process to train an ensemble where, each constituent model (a decision tree) trains on a random subset of training examples, sampled with replacement.
- Rectified Linear Unit (ReLU): An activation function with the following rules:
    - If input is negative or zero, output is 0.
    - If input is positive, output is equal to input.
- recurrent neural network (RNN): A neural network that is intentionally run multiple times, where parts of each run feed into the next run. Specifically, hidden layers from the previous run provide part of the input to the same hidden layer in the next run. Recurrent neural networks are particularly useful for evaluating sequences, so that the hidden layers can learn from previous runs of the neural network on earlier parts of the sequence.
- regularization: The penalty on a model's complexity. Regularization helps prevent overfitting. Different kinds of regularization include:
	- L1 regularization
	- L2 regularization
	- dropout regularization
	- early stopping (this is not a formal regularization method, but can effectively limit overfitting)
- regularization rate: A scalar value, represented as lambda, specifying the relative importance of the regularization function. The following simplified loss equation shows the regularization rate's influence:
![](https://raw.githubusercontent.com/emmableu/image/master/202209292333236.png)
-  ROC (receiver operating characteristic) Curve: A curve of true positive rate vs. false positive rate at different classification thresholds. 
-  Root Mean Squared Error (RMSE): The square root of the Mean Squared Error.

## s
- scaling: A commonly used practice in feature engineering to tame a feature's range of values to match the range of other features in the dataset.
- softmax: A function that provides probabilities for each possible class in a multi-class classification model. The probabilities add up to exactly 1.0.
- Squared loss: The loss function used in linear regression. This function calculates the squares of the difference between a model's predicted value for a labeled example and the actual value of the label.
- Support Vector Machines (SVMs): A classification algorithm that seeks to maximize the margin between positive and negative classes by mapping input data vectors to a higher dimensional space. 
- sigmoid function: A function that maps logistic or multinomial regression output (log odds) to probabilities, returning a value between 0 and 1. 

## t
## u
- underfitting: Producing a model with poor predictive ability because the model hasn't captured the complexity of the training data. 
## v
- vanishing gradient problem: The tendency for the gradients of early hidden layers of some deep neural networks to become surprisingly flat (low). 
## w
## x
## y
## z

