---
title: Feature Scaling
date: 2021-11-06 12:40:41
permalink: /pages/303e21/
categories:
  - 机器学习八股文
  - Machine Learning General
tags:
  - 
---
Feature scaling，常见的提法有“特征归一化”(normalization)、“标准化”(standardization)，是数据预处理中的重要技术，有时甚至决定了算法能不能work以及work得好不好。谈到feature scaling的必要性，最常用的2个例子可能是：

- 特征间的单位（尺度）可能不同，比如身高和体重，比如摄氏度和华氏度，比如房屋面积和房间数，一个特征的变化范围可能是[1000, 10000]，另一个特征的变化范围可能是[-0.1, 0.2]，在进行距离有关的计算时，单位的不同会导致计算结果的不同，尺度大的特征会起决定性作用，而尺度小的特征其作用可能会被忽略，为了消除特征间单位和尺度差异的影响，以对每维特征同等看待，需要对特征进行归一化。
- 原始特征下，因尺度差异，其损失函数的等高线图可能是椭圆形，梯度方向垂直于等高线，下降会走zigzag路线，而不是指向local minimum。通过对特征进行zero-mean and unit-variance变换后，其损失函数的等高线图更接近圆形，梯度下降的方向震荡更小，收敛更快，如下图所示
![](https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-0.png)
feature scaling的方法可以分成2类，逐行进行和逐列进行。逐行是对每一维特征操作，逐列是对每个样本操作，上图为逐行操作中特征标准化的示例。

具体地，常用feature scaling方法如下
![](https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-1.png)
上述4种feature scaling方式，前3种为逐行操作，最后1种为逐列操作。

## When do we need feature scaling?
涉及或隐含距离计算的算法，比如K-means、KNN、PCA、SVM等，一般需要feature scaling，因为
- zero-mean一般可以增加样本间余弦距离或者内积结果的差异，区分力更强，假设数据集集中分布在第一象限遥远的右上角，将其平移到原点处，可以想象样本间余弦距离的差异被放大了。在模版匹配中，zero-mean可以明显提高响应结果的区分度。
- 就欧式距离而言，增大某个特征的尺度，相当于增加了其在距离计算中的权重，如果有明确的先验知识表明某个特征很重要，那么适当增加其权重可能有正向效果，但如果没有这样的先验，或者目的就是想知道哪些特征更重要，那么就需要先feature scaling，对各维特征等而视之。
- 增大尺度的同时也增大了该特征维度上的方差，PCA算法倾向于关注方差较大的特征所在的坐标轴方向，其他特征可能会被忽视，因此，在PCA前做Standardization效果可能更好，
![](https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-2.png)
- 要用到gradient descent方法的算法，e.g.，神经网络。
    - 注意，文章开篇的椭圆形和圆形等高线图，仅在采用均方误差(mean square error)的线性模型上适用，其他损失函数或更复杂的模型，如深度神经网络，损失函数的error surface可能很复杂，并不能简单地用椭圆和圆来刻画，所以用它来解释feature scaling对所有损失函数的梯度下降的作用是一个简化后的解释，
- bactch normalization: 对于传统的神经网络，对输入做feature scaling也很重要，因为采用sigmoid等有饱和区的激活函数，如果输入分布范围很广，参数初始化时没有适配好，很容易直接陷入饱和区，导致梯度消失，所以，需要对输入做Standardization或映射[0,1]、[-1,1]，配合精心设计的参数初始化方法，对值域进行控制。但自从有了Batch Normalization，每次线性变换改变特征分布后，都会重新进行Normalization，似乎可以不太需要对网络的输入进行feature scaling了？但习惯上还是会做feature scaling。


## When do we not need feature scaling?
- 与距离计算无关的概率模型，不需要feature scaling，比如Naive Bayes；
- 与距离计算无关的基于树的模型，不需要feature scaling，比如决策树、随机森林等，树中节点的选择只关注当前特征在哪里切分对分类更好，即只在意特征内部的相对大小，而与特征间的相对大小无关。（因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率）
