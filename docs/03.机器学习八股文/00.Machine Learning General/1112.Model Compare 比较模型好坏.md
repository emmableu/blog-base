 ## [random forest](/pages/63f233/#éšæœºæ£®æ—ç®—æ³•-random-forest) v.s. gradient boost
- [random forest](/pages/63f233/#éšæœºæ£®æ—ç®—æ³•-random-forest): the process to train an ensemble where, each constituent model (a decision tree) trains on a random subset of training examples, sampled with replacement. 
- gradient boosting: 
	- A training algorithm where weak models are trained to iteratively improve the quality (reduce the loss) of a strong model. For example, a weak model could be a linear or small decision tree model. The strong model becomes the sum of all the previously trained weak models.
	- at each iteration, a weak model is trained to predict the loss gradient of the strong model. Then, the strong model's output is updated by subtracting the predicted gradient, similar to gradient descent.
- åŒï¼š ensembling methods, combine the outputs from individual trees. 
- å¼‚ï¼š
	- ä»classification ç»“æœæ¥è¯´ï¼š
		 - comparing to RF, gradient boost have a lot more modeling capacity. They can model very complex relationships and decision boundaries.
		 - gradient boost: low bias, high variance, can lead to overfitting
		 - random forest: high bias, low variance, can cause underfitting. 
	 - ä»classification è¿‡ç¨‹æ¥è¯´ï¼š
		 - [/pages/63f233/#bagging-å’Œ-boosting-çš„4-ç‚¹å·®åˆ«](/pages/63f233/#bagging-å’Œ-boosting-çš„4-ç‚¹å·®åˆ«)

## Generative v.s. Discrimitive Model
from [csdn](https://blog.csdn.net/Oh_MyBug/article/details/104343641)


**generative mode**l:
-   a model that does either of the following:
    -   Creates (generates) new sample from the training dataset. e.g., create poetry after training on a dataset of poems. (e.g., GAN)
    -   Determines the probability that a new example comes from the training set, or was created from the same mechanism that created the training set. e.g., after training on a dataset consisting of English sentences, a generative model could determine the probability that new input is a valid English sentence.(e.g., Naive Bayes)
-   A generative model can understand the distribution of examples or particular features in a dataset. 

- **discriminative model**: A model that predicts labels from a set of one or more features. The goal of a discriminative model is to understand the conditional probability of an output given the features and weights; that is: `p(output | features, weights)`
	- For example, a model that predicts whether an email is spam from features and weights 
	- Contrast with generative model.

<img src="https://raw.githubusercontent.com/emmableu/image/master/generative-model-0.png" width="100%">
simple generative model includes:
- naive bayes 
- LDA (Linear Discrimitative Analysis)
<img src="https://raw.githubusercontent.com/emmableu/image/master/generative-model-1.png" width="100%">

### generative model pros and cons
#### pros
- å®é™…ä¸Šå¸¦çš„ä¿¡æ¯æ¯”åˆ¤åˆ«æ¨¡å‹ä¸°å¯Œ
- ç ”ç©¶å•ç±»é—®é¢˜æ¯”åˆ¤åˆ«æ¨¡å‹çµæ´»æ€§å¼º
- èƒ½ç”¨äºæ•°æ®ä¸å®Œæ•´æƒ…å†µ, åŸºäºæ¦‚ç‡åˆ†å¸ƒçš„å‡è®¾ï¼Œæ‰€éœ€çš„training dataè¾ƒå°‘
- å¾ˆå®¹æ˜“å°†å…ˆéªŒçŸ¥è¯†è€ƒè™‘è¿›å»
- ç¨³å¥å‹å¥½ï¼Œå½“æ•°æ®å‘ˆç°ä¸åŒç‰¹ç‚¹æ—¶ï¼Œåˆ†ç±»æ€§èƒ½ä¸ä¼šå‡ºç°å¤ªå¤§çš„å·®å¼‚å¯¹noiseæ¯”è¾ƒrobust
#### cons
- å®¹æ˜“äº§ç”Ÿé”™è¯¯åˆ†ç±»:Naive Bayesé‡Œé¢å‡è®¾æ¯ä¸ªäº‹ä»¶éƒ½æ˜¯independentçš„ï¼Œæ¯”å¦‚00|01|10 & 11çš„åˆ†ç±»ï¼Œæ ·æœ¬ä¸å‡çš„æ—¶å€™å¯èƒ½ä¼šåˆ†é”™ï¼Œå› ä¸ºmodelå¯èƒ½ä¼šè„‘è¡¥ä¸å­˜åœ¨çš„æƒ…å†µ
- å­¦ä¹ å’Œè®¡ç®—è¿‡ç¨‹æ¯”è¾ƒå¤æ‚


### discrimitive model pros and cons
pros
- åˆ†ç±»è¾¹ç•Œæ›´çµæ´»ï¼Œæ¯”ä½¿ç”¨çº¯æ¦‚ç‡æ–¹æ³•æˆ–ç”Ÿæˆæ¨¡å‹å¾—åˆ°çš„æ›´é«˜çº§
- èƒ½æ¸…æ™°çš„åˆ†è¾¨å‡ºå¤šç±»æˆ–æŸä¸€ç±»ä¸å…¶å®ƒç±»ä¹‹é—´çš„å·®å¼‚ç‰¹å¾
- å¯¹äºå¤šfeatureçš„æƒ…å†µï¼Œfeatureä¹‹é—´å¤šæœ‰correlationï¼Œæ¯”èµ·naive bayesï¼Œmodels such as logistic regression is much more robust with correlated features. 
- åˆ¤åˆ«æ¨¡å‹çš„æ€§èƒ½æ¯”ç”Ÿæˆæ¨¡å‹è¦ç®€å•ï¼Œæ¯”è¾ƒå®¹æ˜“å­¦ä¹ 
cons
- ä¸èƒ½ååº”è®­ç»ƒæ•°æ®æœ¬èº«çš„ç‰¹æ€§ï¼Œåªèƒ½å‘Šè¯‰ä½ çš„æ˜¯1è¿˜æ˜¯2ï¼Œä¸èƒ½æŠŠæ•´ä¸ªåœºæ™¯æè¿°å‡ºæ¥






### å’ŒDiscrimitiveæ¨¡å‹æ¯”èµ·æ¥ï¼ŒGenerative æ›´å®¹æ˜“overfittingè¿˜æ˜¯underfitting
æ›´å®¹æ˜“underfittingã€‚
this [stack exchange](https://stats.stackexchange.com/questions/91484/do-discriminative-models-overfit-more-than-generative-models) has some very math explanations.  
æ¯”è¾ƒç®€å•çš„è§£é‡Šï¼š 

A generative model is typically underfitting because it allows the user to put in more side information in the form of class conditionals.

Consider a generative model ğ‘(ğ‘|ğ‘¥)=ğ‘(ğ‘)ğ‘(ğ‘¥|ğ‘). If the class conditionals are mulitvariate normals with shared covariance, this will have a linear decision boundary. Thus, the model by itself is just as powerful as a linear SVM or logistic regression.

However, a discriminative classifier is much more free in the choice of decision function: it just has to find an appropriate hyperplane. The generative classifier however will need much less samples to find good parameters if the assumptions are valid.

Sorry, this is rather handwavy and there is no hard math behind it. But it is an intuition.

## GMM v.s. K-Means
k-means:
-   A clustering algorithm that clusters samples by:
    1. Assigns each sample to the closest center points.
    2. Iteratively determines the best k center points.
-   So, k-means algorithm picks center points to minimize the cumulative square of the distances from each sample to its closest center points

GMM: 
-   A clustering algorithm that clusters samples by iteratively:
	- assign the probablilities each sample belong to each cluster, and
	- iteratively determine the best miu, and covariance big sigma for each of the clusters
- keep iterating until it reaches convergence. 

ç›¸åŒç‚¹  
éƒ½æ˜¯è¿­ä»£æ‰§è¡Œçš„ç®—æ³•ï¼Œä¸”è¿­ä»£çš„ç­–ç•¥ä¹Ÿç›¸åŒï¼šç®—æ³•å¼€å§‹æ‰§è¡Œæ—¶å…ˆå¯¹éœ€è¦è®¡ç®—çš„å‚æ•°èµ‹åˆå€¼ï¼Œç„¶åäº¤æ›¿æ‰§è¡Œä¸¤ä¸ªæ­¥éª¤ï¼Œä¸€ä¸ªæ­¥éª¤æ˜¯å¯¹æ•°æ®çš„ä¼°è®¡ï¼ˆk-meansæ˜¯ä¼°è®¡æ¯ä¸ªç‚¹æ‰€å±ç°‡ï¼›GMMæ˜¯è®¡ç®—éšå«å˜é‡çš„æœŸæœ›ï¼›ï¼‰;ç¬¬äºŒæ­¥æ˜¯ç”¨ä¸Šä¸€æ­¥ç®—å‡ºçš„ä¼°è®¡å€¼é‡æ–°è®¡ç®—å‚æ•°å€¼ï¼Œæ›´æ–°ç›®æ ‡å‚æ•°ï¼ˆk-meansæ˜¯è®¡ç®—ç°‡å¿ƒä½ç½®ï¼›GMMæ˜¯è®¡ç®—å„ä¸ªé«˜æ–¯åˆ†å¸ƒçš„ä¸­å¿ƒä½ç½®å’Œåæ–¹å·®çŸ©é˜µï¼‰

ä¸åŒç‚¹   
1ï¼‰éœ€è¦è®¡ç®—çš„å‚æ•°ä¸åŒï¼šk-meansæ˜¯ç°‡å¿ƒä½ç½®ï¼›GMMæ˜¯å„ä¸ªé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°   
2ï¼‰è®¡ç®—ç›®æ ‡å‚æ•°çš„æ–¹æ³•ä¸åŒï¼šk-meansæ˜¯è®¡ç®—å½“å‰ç°‡ä¸­æ‰€æœ‰å…ƒç´ çš„ä½ç½®çš„å‡å€¼ï¼›GMMæ˜¯åŸºäºæ¦‚ç‡çš„ç®—æ³•ï¼Œæ˜¯é€šè¿‡è®¡ç®—ä¼¼ç„¶å‡½æ•°çš„æœ€å¤§å€¼å®ç°åˆ†å¸ƒå‚æ•°çš„æ±‚è§£çš„ã€‚



## gradient boost å’Œ adaboost åŒºåˆ«
1. ç»„æˆéƒ¨åˆ†ï¼š
	1) adaboost starts by building a very short tree, called a **stump**, from the training data.
	2) gradient boost starts by making a small leaf, instead of a tree or stump, this leaf represents an initial guess for the weights of all of the samples. ç„¶ågradient boost ä¼šå»ºæ ‘ï¼Œä½†æ˜¯ä¼šæ¯”ä¸€ä¸ªstumpå¤§
2. prediction target:
	1. adaboost predicts the final value
	2. gradient boost predicts the residuals