 ## [random forest](/pages/63f233/#随机森林算法-random-forest) v.s. gradient boost
- [random forest](/pages/63f233/#随机森林算法-random-forest): the process to train an ensemble where, each constituent model (a decision tree) trains on a random subset of training examples, sampled with replacement. 
- gradient boosting: 
	- A training algorithm where weak models are trained to iteratively improve the quality (reduce the loss) of a strong model. For example, a weak model could be a linear or small decision tree model. The strong model becomes the sum of all the previously trained weak models.
	- at each iteration, a weak model is trained to predict the loss gradient of the strong model. Then, the strong model's output is updated by subtracting the predicted gradient, similar to gradient descent.
- 同： ensembling methods, combine the outputs from individual trees. 
- 异：
	- 从classification 结果来说：
		 - comparing to RF, gradient boost have a lot more modeling capacity. They can model very complex relationships and decision boundaries.
		 - gradient boost: low bias, high variance, can lead to overfitting
		 - random forest: high bias, low variance, can cause underfitting. 
	 - 从classification 过程来说：
		 - [/pages/63f233/#bagging-和-boosting-的4-点差别](/pages/63f233/#bagging-和-boosting-的4-点差别)

## Generative v.s. Discrimitive Model
from [csdn](https://blog.csdn.net/Oh_MyBug/article/details/104343641)


**generative mode**l:
-   a model that does either of the following:
    -   Creates (generates) new sample from the training dataset. e.g., create poetry after training on a dataset of poems. (e.g., GAN)
    -   Determines the probability that a new example comes from the training set, or was created from the same mechanism that created the training set. e.g., after training on a dataset consisting of English sentences, a generative model could determine the probability that new input is a valid English sentence.(e.g., Naive Bayes)
-   A generative model can understand the distribution of examples or particular features in a dataset. 

- **discriminative model**: A model that predicts labels from a set of one or more features. The goal of a discriminative model is to understand the conditional probability of an output given the features and weights; that is: `p(output | features, weights)`
	- For example, a model that predicts whether an email is spam from features and weights 
	- Contrast with generative model.

<img src="https://raw.githubusercontent.com/emmableu/image/master/generative-model-0.png" width="100%">
simple generative model includes:
- naive bayes 
- LDA (Linear Discrimitative Analysis)
<img src="https://raw.githubusercontent.com/emmableu/image/master/generative-model-1.png" width="100%">

### generative model pros and cons
#### pros
- 实际上带的信息比判别模型丰富
- 研究单类问题比判别模型灵活性强
- 能用于数据不完整情况, 基于概率分布的假设，所需的training data较少
- 很容易将先验知识考虑进去
- 稳健型好，当数据呈现不同特点时，分类性能不会出现太大的差异对noise比较robust
#### cons
- 容易产生错误分类:Naive Bayes里面假设每个事件都是independent的，比如00|01|10 & 11的分类，样本不均的时候可能会分错，因为model可能会脑补不存在的情况
- 学习和计算过程比较复杂


### discrimitive model pros and cons
pros
- 分类边界更灵活，比使用纯概率方法或生成模型得到的更高级
- 能清晰的分辨出多类或某一类与其它类之间的差异特征
- 对于多feature的情况，feature之间多有correlation，比起naive bayes，models such as logistic regression is much more robust with correlated features. 
- 判别模型的性能比生成模型要简单，比较容易学习
cons
- 不能反应训练数据本身的特性，只能告诉你的是1还是2，不能把整个场景描述出来






### 和Discrimitive模型比起来，Generative 更容易overfitting还是underfitting
更容易underfitting。
this [stack exchange](https://stats.stackexchange.com/questions/91484/do-discriminative-models-overfit-more-than-generative-models) has some very math explanations.  
比较简单的解释： 

A generative model is typically underfitting because it allows the user to put in more side information in the form of class conditionals.

Consider a generative model 𝑝(𝑐|𝑥)=𝑝(𝑐)𝑝(𝑥|𝑐). If the class conditionals are mulitvariate normals with shared covariance, this will have a linear decision boundary. Thus, the model by itself is just as powerful as a linear SVM or logistic regression.

However, a discriminative classifier is much more free in the choice of decision function: it just has to find an appropriate hyperplane. The generative classifier however will need much less samples to find good parameters if the assumptions are valid.

Sorry, this is rather handwavy and there is no hard math behind it. But it is an intuition.

## GMM v.s. K-Means
k-means:
-   A clustering algorithm that clusters samples by:
    1. Assigns each sample to the closest center points.
    2. Iteratively determines the best k center points.
-   So, k-means algorithm picks center points to minimize the cumulative square of the distances from each sample to its closest center points

GMM: 
-   A clustering algorithm that clusters samples by iteratively:
	- assign the probablilities each sample belong to each cluster, and
	- iteratively determine the best miu, and covariance big sigma for each of the clusters
- keep iterating until it reaches convergence. 

相同点  
都是迭代执行的算法，且迭代的策略也相同：算法开始执行时先对需要计算的参数赋初值，然后交替执行两个步骤，一个步骤是对数据的估计（k-means是估计每个点所属簇；GMM是计算隐含变量的期望；）;第二步是用上一步算出的估计值重新计算参数值，更新目标参数（k-means是计算簇心位置；GMM是计算各个高斯分布的中心位置和协方差矩阵）

不同点   
1）需要计算的参数不同：k-means是簇心位置；GMM是各个高斯分布的参数   
2）计算目标参数的方法不同：k-means是计算当前簇中所有元素的位置的均值；GMM是基于概率的算法，是通过计算似然函数的最大值实现分布参数的求解的。



## gradient boost 和 adaboost 区别
1. 组成部分：
	1) adaboost starts by building a very short tree, called a **stump**, from the training data.
	2) gradient boost starts by making a small leaf, instead of a tree or stump, this leaf represents an initial guess for the weights of all of the samples. 然后gradient boost 会建树，但是会比一个stump大
2. prediction target:
	1. adaboost predicts the final value
	2. gradient boost predicts the residuals