---
title: SoftMax and Cross-Entropy Loss Function
date: 2022-09-29 13:35:13
permalink: /pages/710eb0/
categories:
  - 机器学习八股文
  - Machine Learning General
tags:
  - 
---
[source](https://raw.githubusercontent.com/parasdahal/deepnotes/master/_posts/2017-05-28-softmax-crossentropy.md)

*Note: Complete source code can be found here [https://github.com/parasdahal/deepnet](https://github.com/parasdahal/deepnet)*

### The Softmax Function

Softmax function takes an N-dimensional vector of real numbers and transforms it into a vector of real number in range (0,1) which add upto 1. 
$$
p_j = \frac{e^{a_i}}{\sum_{k=1}^N e^a_k}
$$


As the name suggests, softmax function is a "soft" version of max function. Instead of selecting one maximum value, it breaks the whole (1) with maximal element getting the largest portion of the distribution, but other smaller elements getting some of it as well. 

This property of softmax function that it outputs a probability distribution makes it suitable for probabilistic interpretation in classification tasks.

In python, we the code for softmax function as follows:

```python
def softmax(X):
    exps = np.exp(X)
    return exps / np.sum(exps)
```


### Cross Entropy Loss

Cross entropy indicates the distance between what the model believes the output distribution should be, and what the original distribution really is. It is defined as,
$$
H(y,p) = - \sum_i y_i log(p_i)
$$
Cross entropy measure is a widely used alternative of squared error. It is used when node activations can be understood as representing the probability that each hypothesis might be true, i.e. when the output is a probability distribution. Thus it is used as a loss function in neural networks which have softmax activations in the output layer.

```python
def cross_entropy(X,y):
    """
    X is the output from fully connected layer (num_examples x num_classes)
    y is labels (num_examples x 1)
    """
    m = y.shape[0]
    p = softmax(X)
    log_likelihood = -np.log(p[range(m),y])
    loss = np.sum(log_likelihood) / m
    return loss
```

