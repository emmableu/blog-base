---
title: Task 1 - Sentiment Analysis
date: 2022-04-10 12:02:23
permalink: /pages/5cca96/
categories:
  - 机器学习八股文
  - FudanNLP
tags:
  - 
---

## Requirements
实现基于logistic/softmax regression的文本分类

1. 参考
   1. 《[神经网络与深度学习](https://nndl.github.io/)》 第2/3章
2. 数据集：[Classify the sentiment of sentences from the Rotten Tomatoes dataset](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)
3. 实现要求：NumPy
4. 需要了解的知识点：
   1. 文本特征表示：Bag-of-Word，N-gram
   2. 分类器：logistic/softmax  regression，损失函数、（随机）梯度下降、特征选择
   3. 数据集：训练集/验证集/测试集的划分
5. 实验：
   1. 分析不同的特征、损失函数、学习率对最终分类性能的影响
   2. shuffle 、batch、mini-batch 
6. 时间：两周


## API knowledge 
### `fit(), transform(), fit_transform()`
各种不同的learner具体执行的内容不一样，但是返回的格式类似：
- `fit()`: 返回parameter, 例如：
	- `CountVectorizer().fit(['a, b','a','a'])`: Learn a vocabulary dictionary of all tokens in the raw documents.
	- `StandardScaler().fit(x)`： 返回mean 和 variance
- `fit_transform()` == `fit().transform()`, 返回parameter, 以及用parameter转化后的x
- `transform()`: 对于类似 `CountVectorizer()` 这种learner，就是只返回转化后的x

常见的fit 和 transform的写法：在train上做fit，然后用fit得到的parameter给train 和 test一起做transform
```python
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_transformer = TfidfVectorizer(analyzer='word',max_features=50000)
tfidf_transformer.fit(x_train)
x_train_tfidf_word = tfidf_transformer.transform(x_train)
x_test_tfidf_word = tfidf_transformer.transform(x_test)
```
### `hstack`
`np.hstack([[1,2,3], [4,5,6]])` = `[1,2,3,4,5,6]`
注意 [1,2,3], [4,5,6] 外面还有一个中括号。

## code
github: https://github.com/emmableu/my-nlp-beginner-solution/blob/master/Task1/task1.py

same as below: 
```python
import pandas as pd  
  
from sklearn.model_selection import train_test_split  
from sklearn.feature_extraction.text import CountVectorizer  
from sklearn.feature_extraction.text import TfidfVectorizer  
from sklearn.metrics import confusion_matrix  
from scipy.sparse import hstack  
from sklearn.linear_model import SGDClassifier  
from sklearn.metrics import precision_recall_fscore_support  
from sklearn.metrics import accuracy_score  
  
dir_all_data = 'data/task1_all_data.tsv'  
data_all = pd.read_csv(dir_all_data, sep='\t')  
x_all = data_all['Phrase']  
y_all = data_all['Sentiment']  
x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2)  
  
"""接下来要提取几个特征：文本计数特征、word级别的TF-IDF特征、ngram级别的TF-IDF特征"""  
# 提取文本计数特征 -- 每个单词的数量  
# 对文本的单词进行计数，包括文本的预处理, 分词以及过滤停用词  
count_transformer = CountVectorizer().fit(x_train)  
x_train_count = count_transformer.transform(x_train)  
x_test_count = count_transformer.transform(x_test)  
print(x_train_count.shape, x_test_count.shape)  
# 在词汇表中一个单词的索引值对应的是该单词在整个训练的文集中出现的频率。  
# print(count_vect.vocabulary_.get(u'good'))    #5812     count_vect.vocabulary_是一个词典：word-id  
  
  
# 提取TF-IDF特征 -- word级别的TF-IDF  
# 将各文档中每个单词的出现次数除以该文档中所有单词的总数：这些新的特征称之为词频tf。  
tfidf_transformer = TfidfVectorizer(analyzer='word').fit(x_train)  
x_train_tfidf = tfidf_transformer.transform(x_train)  
x_test_tfidf = tfidf_transformer.transform(x_test)  
print(x_train_tfidf.shape, x_test_tfidf.shape)  
  
# 提取TF-IDF特征 - ngram级别的TF-IDF  
# 将各文档中每个单词的出现次数除以该文档中所有单词的总数：这些新的特征称之为词频tf。  
ngram_tfidf_transformer = TfidfVectorizer(analyzer='word', ngram_range=(2, 3), max_features=50000)  
ngram_tfidf_transformer.fit(x_train)  
x_train_tfidf_ngram = ngram_tfidf_transformer.transform(x_train)  
x_test_tfidf_ngram = ngram_tfidf_transformer.transform(x_test)  
print(x_train_tfidf_ngram.shape, x_test_tfidf_ngram.shape)  
  
x_train = hstack([x_train_count, x_train_tfidf, x_train_tfidf_ngram])  
x_test = hstack([x_test_count, x_test_tfidf, x_test_tfidf_ngram]) 

model = SGDClassifier().fit(x_train, y_train)  
y_pred = model.predict(x_test)  
print(confusion_matrix(y_test, y_pred))  
print(precision_recall_fscore_support(y_test, y_pred,average="macro"))  
print(accuracy_score(y_test, y_pred))
```