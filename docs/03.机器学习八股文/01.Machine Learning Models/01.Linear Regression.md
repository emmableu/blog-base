---
title: Linear Regression
date: 2021-10-21 17:14:00
permalink: /pages/963e9f/
categories:
  - 机器学习八股文
  - Machine Learning Problems
tags:
  - 
---
## Regression Metrics
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-11.png)

## Linear Regression的基础假设
以下是对于 Gaussian-noise simple linear regression model
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-0.png)
最后还有： ![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-6.png)
如果不是Gaussian-noise， 而是普通的simple linear regression model:  
残差只要t mean zero with constant variance 就好。


## 假设失效的影响
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-1.png)
## 假设检验方法
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-2.png)
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-3.png)
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-4.png)
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-5.png)
what will happen when we have correlated variables, how to solve
explain regression coefficient

## Least Squares Estimation
试图找到一条直线，使得所有样本到直线上的欧式距离之和最小， 也就是对残差平方和求导，使得残差平方和最小。
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-10.png)


## MLE(Maximum Likelihood Estimation) for linear regression
仅限于对：Gaussian-noise simple linear regression model：
因为假设了残差遵循正态分布，所以因为正态分布的pdf(概率密度函数)是
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-7.png)
所以 y 的conditional pdf就是
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-8.png)
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-9.png)

## When do we need residuals to be normally distributed?
- OLS:
  - 如果只是用ols做参数估计，那么就不需要残差正态分布。
  - 但是，如果要算beta的p value 和置信区间，那么就一定要残差正态分布。因为否则的话beta不满足t distribution
- MLE:
  - MLE估计无论如何都需要残差正态分布。

## Least Square Method v.s. Gradient Descent Method 概念区别
- 最小二乘：一种特殊的求优化问题的解的方法，目的是使得残差的平方和最小，可以通过计算得到。
- 梯度下降：一种通过近似得到优化问题的答案的方法,The benefit is that it can be applied to any objective function, not just squared distances. 前提是得是convex function，

## Problem with Root Mean Square Error (RMSE)
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-14.png)
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-12.png)
![](https://raw.githubusercontent.com/emmableu/image/master/linear-regression-13.png)


if the relationship between y and x is no linear, can linear regression solve that

why use interaction variables
