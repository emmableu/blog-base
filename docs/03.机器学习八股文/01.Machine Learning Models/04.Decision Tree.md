---
title: Decision Tree
date: 2021-10-21 17:14:42
permalink: /pages/05b850/
categories:
  - 机器学习八股文
  - Machine Learning Problems
tags:
  - 
---

[youtube](https://www.youtube.com/watch?v=_L39rN6gz7Y)

## decision tree 建树过程example
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-0.png)
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-1.png)
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-2.png)

### 1.Gini Imperity (Information Gain)
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-3.png)
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-4.png)
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-5.png)

### 2.Calculate Gini Imperity for numerical (non-categorical) data
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-6.png)
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-7.png)
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-8.png)

### 3.Use Gini Imperity to find who to put at the top of the tree
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-9.png)
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-10.png)

### 4.Recalculate gini impurity on the each subset of the data after the root node split. 
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-11.png)
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-12.png)
![](https://raw.githubusercontent.com/emmableu/image/master/decision-tree-13.png)
## How to prevent overfitting in Decision Tree?
1. early stopping (pre-pruning)， 在完全长成以前停止，以防止过拟合。
    1. 限制树的高度，可以利用交叉验证选择
    2. 利用分类指标，如果下一次切分没有降低误差，则停止切分
    3. 限制树的节点个数，比如某个节点小于100个样本，停止对该节点切分

2. post pruning  
它首先构造完整的决策树，允许树过度拟合训练数据，然后对那些置信度不够的结点子树用叶子结点来代替，该叶子的类标号用该结点子树中最频繁的类标记。


(incomplete) How to do regularization in DT? 
