---
title: EM (Expectation Maximization) Algorithm
date: 2021-11-08 10:59:46
permalink: /pages/3f9e2c/
categories:
  - 机器学习八股文
  - Machine Learning Models
tags:
  - 
---
EM算法是什么
GMM是什么，和Kmeans的关系

## EM Algorithm: to estimate the **local optima** for MLE when there's an unknown variable z. 
### Jensen's Inequality 
![](https://raw.githubusercontent.com/emmableu/image/master/em-0.png)
![](https://raw.githubusercontent.com/emmableu/image/master/em-1.png)

### Target function to optimize: (Equation (3))
![](https://raw.githubusercontent.com/emmableu/image/master/em-2.png)
![](https://raw.githubusercontent.com/emmableu/image/master/em-3.png)

(If z were continuous, then Qi would be a density, and the summations over z in our discussion are replaced with integrals over z.)

### The E Step: Find the optimal distribution Q(z) of the unknown variable z, so that the Jensen's inquality takes the equal sign
![](https://raw.githubusercontent.com/emmableu/image/master/em-4.png)
![](https://raw.githubusercontent.com/emmableu/image/master/em-5.png)

### The M Step: Find theta that maximize the Equation (3) with the new theta's. 
如题

### Repeately do E and M until convergence 
![](https://raw.githubusercontent.com/emmableu/image/master/em-8.png)
![](https://raw.githubusercontent.com/emmableu/image/master/em-6.png)
![](https://raw.githubusercontent.com/emmableu/image/master/em-7.png)

### Prove that EM converges to local optima 
> EM causes the likelihood to converge monotomically.
![](https://raw.githubusercontent.com/emmableu/image/master/em-9.png)
![](https://raw.githubusercontent.com/emmableu/image/master/em-10.png)
