---
title: Naive Bayes and Generative Model
date: 2021-10-21 17:17:25
permalink: /pages/ee42b0/
categories:
  - 机器学习八股文
  - Machine Learning Problems
tags:
  - 
---
## Generative v.s. Discrimitive Model
from [csdn](https://blog.csdn.net/Oh_MyBug/article/details/104343641)
<img src="https://raw.githubusercontent.com/emmableu/image/master/generative-model-0.png" width="100%">
simple generative model includes:
- naive bayes 
- LDA (Linear Discrimitative Analysis)
<img src="https://raw.githubusercontent.com/emmableu/image/master/generative-model-1.png" width="100%">

### generative model pros and cons
#### pros
- 实际上带的信息比判别模型丰富
- 研究单类问题比判别模型灵活性强
- 能用于数据不完整情况, 基于概率分布的假设，所需的training data较少
- 很容易将先验知识考虑进去
- 稳健型好，当数据呈现不同特点时，分类性能不会出现太大的差异对noise比较robust
#### cons
- 容易产生错误分类:Naive Bayes里面假设每个事件都是independent的，比如00|01|10 & 11的分类，样本不均的时候可能会分错，因为model可能会脑补不存在的情况
- 学习和计算过程比较复杂


### discrimitive model pros and cons
#### pros
- 分类边界更灵活，比使用纯概率方法或生成模型得到的更高级
- 能清晰的分辨出多类或某一类与其它类之间的差异特征
- 对于多feature的情况，feature之间多有correlation，比起naive bayes，models such as logistic regression is much more robust with correlated features. 
- 判别模型的性能比生成模型要简单，比较容易学习
#### cons
- 不能反应训练数据本身的特性，只能告诉你的是1还是2，不能把整个场景描述出来


## 和Discrimitive模型比起来，Generative 更容易overfitting还是underfitting
更容易overfitting。
this [stack exchange](https://stats.stackexchange.com/questions/91484/do-discriminative-models-overfit-more-than-generative-models) has some very math explanations.  
比较简单的解释： 

A generative model is typically overfitting less because it allows the user to put in more side information in the form of class conditionals.

Consider a generative model 𝑝(𝑐|𝑥)=𝑝(𝑐)𝑝(𝑥|𝑐). If the class conditionals are mulitvariate normals with shared covariance, this will have a linear decision boundary. Thus, the model by itself is just as powerful as a linear SVM or logistic regression.

However, a discriminative classifier is much more free in the choice of decision function: it just has to find an appropriate hyperplane. The generative classifier however will need much less samples to find good parameters if the assumptions are valid.

Sorry, this is rather handwavy and there is no hard math behind it. But it is an intuition.

## Naïve Bayes 算法
![](https://raw.githubusercontent.com/emmableu/image/master/202209211513598.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209211515403.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209211515884.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209211516316.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209211516595.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209211517623.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209211518639.png)


## Gaussian Naive Bayes
![](https://raw.githubusercontent.com/emmableu/image/master/202209211519354.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209211555737.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209211609031.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209211616842.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209211617475.png)

## Naive bayes q&a 
![](https://raw.githubusercontent.com/emmableu/image/master/202209211618798.png)
![](https://raw.githubusercontent.com/emmableu/image/master/202209211619300.png)


## Naïve Bayes 原理
![](https://raw.githubusercontent.com/emmableu/image/master/202209211632512.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209211619428.png)


## Naive Bayes 基础假设
假设数据集属性之间是相对独立的。

