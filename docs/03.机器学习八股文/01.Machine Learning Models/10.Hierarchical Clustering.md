---
title: Hierarchical Clustering
date: 2021-11-08 10:59:34
permalink: /pages/4d184b/
categories:
  - 机器学习八股文
  - Machine Learning Models
tags:
  - 
---
[notes from MIT](http://web.mit.edu/6.S097/www/resources/Hierarchical.pdf)
## hierarchical clustering v.s. K-means
K-means tend to have the following issues:
1. Convergence time is poor. For example K-means takes worst case exponential number (2^(Ω(n))) of iterations.
2. The final clusters depend heavily on the initialization. Usually a k random points are chosen.
3. The number of clusters is a huge issue. You are forced to specify the number of clusters in the beginning.

Hierarchical clustering solves all these issues and even allows you a metric by which to cluster.   
Hierarchical clustering is polynomial time, the final clusters are always the same depending on your metric, and the number of clusters is not at all a problem.

## Metrics
![](https://raw.githubusercontent.com/emmableu/image/master/hierarchical-0.png)
![](https://raw.githubusercontent.com/emmableu/image/master/hierarchical-1.png)

## Lance-Williams Algorithm
![](https://raw.githubusercontent.com/emmableu/image/master/hierarchical-2.png)
![](https://raw.githubusercontent.com/emmableu/image/master/hierarchical-3.png)
