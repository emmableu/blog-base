---
title: Ensemble Learning
date: 2021-10-21 17:17:15
permalink: /pages/63f233/
categories:
  - 机器学习八股文
  - Machine Learning Problems
tags:
  - 
---
## Concept of ensemble learning
集成学习归属于机器学习，他是一种「训练思路」，并不是某种具体的方法或者算法。
集成学习会挑选一些简单的基础模型进行组装，组装这些基础模型的思路主要有 2 种方法：
1. bagging（bootstrap aggregating的缩写，也称作“套袋法”）
2. boosting

## bagging (bootstrap aggregating) concept:
[youtube](https://www.youtube.com/watch?v=2Mg8QD0F1dQ)  
Bagging 的核心思路是 — — 民主。  
Bagging 的思路是所有基础模型都一致对待，每个基础模型手里都只有一票。然后使用民主投票的方式得到最终的结果。   
大部分情况下，经过 bagging 得到的结果方差（variance）更小。

步骤：
1. create a number of subset of data, train each of them. 
    - e.g., grab n' of the data from the original n dataset, randomly, with replacement (同一个bag里面可能会有重复的data)
2. use each of them to predict, and then take their mean. 
![](https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-0.png)

举例：
- 在 bagging 的方法中，最广为熟知的就是随机森林了：bagging + 决策树 = 随机森林

## Boosting concept:
![](https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-1.png)
Boosting 的核心思路是 — — 挑选精英。    
Boosting 和 bagging 最本质的差别在于他对基础模型不是一致对待的，而是经过不停的考验和筛选来挑选出「精英」，然后给精英更多的投票权，表现不好的基础模型则给较少的投票权，然后综合所有人的投票得到最终结果。      
大部分情况下，经过 boosting 得到的结果偏差（bias）更小      

### 例子：adaboost （adaptive boost)
红色的点： 被当前model错误predict的data
![](https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-2.png)

### Adaboost procedure


![](https://raw.githubusercontent.com/emmableu/image/master/202209212212265.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209212213216.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209212213224.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209212214479.png)


![](https://raw.githubusercontent.com/emmableu/image/master/202209212214446.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209212214462.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209212215085.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209212216231.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209212217762.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209212218473.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209212218551.png)





## Bagging 和 Boosting 的4 点差别
### 样本选择上：
- Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
- Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
### 样例权重：
- Bagging：使用均匀取样，每个样例的权重相等 （random with replacement)
- Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。in the current training data, choose the sample that's mean predicted wrongly in the previous model
### 预测函数：
- Bagging：所有预测函数的权重相等。
- Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
### 并行计算：
- Bagging：各个预测函数可以并行生成
- Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果

## Ensemble learning 中的 Base Classifier (基分类器)
最常用的是decision tree，原因：
- 决策树做判断不受样本imbalance影响
- 数据样本的额扰动对决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性比较大，这样的“不稳定学习器”更适合作为基分类器。此外，在决策树节点分裂的时候，随机的选择一个特征子集，从中找出最优分裂属性，很好的引入了随机性。

除了决策树外，神经网络模型也适合作为base classifier，主要由于神经网络模型也比较不确定，而且还可以通过调整神经元数量，连接方式，网络层数，初始权值等方式引入随机性。

## Can we replace the base classifier in Random Forest to be linear classifier or K-means classifier? Why?
This is not necessary. 
![](https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-3.png)


## Why bagging reduces variance and tends to underfit
![](https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-5.png)
![](https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-7.png)

## Why boosting reduces bias and tends to overfit
![](https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-8.png)

## gbdt v.s. random forest
### GBDT (MART):
Gradient-boosting-based Decision Tree (or Multiple Additive Regression Tree)

### GBDT pros and cons
![](https://raw.githubusercontent.com/emmableu/image/master/ensemble-learning-9.png)
