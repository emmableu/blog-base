---
title: K-Means Clustering
date: 2021-10-21 17:14:20
permalink: /pages/aed8f8/
categories:
  - 机器学习八股文
  - Machine Learning Models
tags:
  - 
---
## K-Means Clustering Algorithm
![](https://raw.githubusercontent.com/emmableu/image/master/k-means-0.png)
![](https://raw.githubusercontent.com/emmableu/image/master/k-means-1.png)

## K-means pros and cons
### cons:
- 受初始值和outlier的影响，每次结果不稳定
- 结果不是全局最优而是局部最优
- 无法很好地解决数据簇分布差别较大的情况（比如一类是另一类样本数量的100倍）
- 不太适用于sparse data的分类

### pros:
- 对于大数据集，k-means是相对可伸缩和高效的。
    - 他的计算复杂度是 O(NKt) 接近于线性，其中N是数据对象的数目， K是聚类的族数， t是迭代的轮数。

## How to improve K-Means
### 1.data scaling
![](https://raw.githubusercontent.com/emmableu/image/master/k-means-5.png)
### 2.choose a good value for k:
#### Elbow method
![](https://raw.githubusercontent.com/emmableu/image/master/k-means-3.png)
#### Gap Statistic
![](https://raw.githubusercontent.com/emmableu/image/master/k-means-4.png)

## Why K-Means converges to local optima: loss function decreases monotonically.
![](https://raw.githubusercontent.com/emmableu/image/master/k-means-6.png)
![](https://raw.githubusercontent.com/emmableu/image/master/k-means-7.png)


## how to determine convergence (stop) in practice:
Ideally, if the values in the last two consequent iterations are same then the algorithm is said to have converged. But often people use a less strict criteria for convergence, like, the difference in the values of last two iterations is less than a particular threshold etc,.

