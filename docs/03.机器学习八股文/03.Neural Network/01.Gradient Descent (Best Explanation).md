---
title:  Gradient Descent (Best Explanation)
date: 2022-04-10 23:16:42
permalink: /pages/1f2a0f/
categories:
  - 机器学习八股文
  - Machine Learning General
tags:
  - 
---
**这里用一个例子来讲下列知识**
- gradient descent
- step size, learning rate, maximum number of steps


### Summary of Gradient Descent Procedure
1. Take the derivative of the **Loss Function** for each parameter in it. (i.e., take the Gradient of the Loss Function).
2. Pick random values for the parameters.
3. Plug the parameter values into the derivatives (i.e., the Gradient)
4. Calculate the Step Sizes: **Step Sizes = Slope * Learning Rate**
5. Calculate the New Parameters: **New Parameter = Old Parameter - Step Size**
6. go back to **Step 3** and repeat until **Step Size** is very small, or you reach the **Maximum Number of Steps**.



假设这里目标是拟合一个y = wx + i, 假设 weight为一个定值0.64, 这里用gradient descent(梯度下降)来得到一个最佳的i intercept的值。

<img  width="70%" src="https://raw.githubusercontent.com/emmableu/image/master/202204102320021.png"/>


首先，假设intercept为0，设weight为一个定值0.64，目标是计算使 SSR 最小的intercept的值

> Evaluate how good the fit is using SSR: Sum of Squared Residuals, 这个ssr 其实也是一种 loss function
		
e.g., random value for weight = 0.64
		<img  width="70%" src="https://raw.githubusercontent.com/emmableu/image/master/202204102326207.png"/>
		
		
则 residual = 1.4 - 0.32 = 1.1
同理， 得到剩下两个点 SSR = $1.1^2 +0.4^2 +1.3^2 = 3.1$

所以，在下图的右边的曲线上，标上 (0, 3.1) 这个点，因为当intercept = 0的时候，SSR = 3.1。
同理，用不同的intercept value，可以得到不同的点, 然后找到使SSR最小的intercept的值

<img  width="70%" src="https://raw.githubusercontent.com/emmableu/image/master/202204102329687.png"/>


把上边的0用参数intercept取代，得到
<img width="70%"  src="https://raw.githubusercontent.com/emmableu/image/master/202204102345342.png">

 下一步，也就是体现Least squares v.s. gradient descent 的区别的一步：
 
### Least squares 最小二乘 v.s. gradient descent 梯度下降 的区别
- 如果是最小二乘法 least squares 估计，就是直接对 SSR 关于 intercept 求导， 求导数为0的点
 $$
 SSR = (1.4 - (intercept + 0.64*0.5))^2 + \\
 (1.9 - (intercept + 0.64*2.3))^2 + \\
 (3.2 - (intercept + 0.64*2.9))^2
 $$
 
 
 $$
 \frac{d}{d \space intercept} SSR = -2 *(1.4 - (intercept + 0.64*0.5)) + \\
 (-2) *(1.9 - (intercept + 0.64*2.3)) + \\
(-2) *(3.2 - (intercept + 0.64*2.9)) 
$$


如果是最小二乘的方法，只要求$\frac{d}{d \space intercept} SSR = 0$  的解，
如果是gradiant descent的估计，则是不停地猜intercept的值，一直到这个猜到的值接近最小值
所以
-   最小二乘：一种特殊的求优化问题的解的方法，目的是使得残差的平方和最小，可以通过计算得到。
-   梯度下降：一种通过近似得到优化问题的答案的方法,The benefit is that it can be applied to any objective function, not just squared distances. 前提是得是convex function，

#### 以本题为例，接下来做Gradient Descent 步骤：
根据这个公式：
  $$
\frac{d}{d \space intercept} SSR = -2 *(1.4 - (intercept + 0.64*0.5)) + \\
 (-2) *(1.9 - (intercept + 0.64*2.3)) + \\
(-2) *(3.2 - (intercept + 0.64*2.9))  \\
 $$
 1. 代入intercept = 0， 得到derivative（也就是 $\frac{d}{d \space intercept} SSR$ ) = - 5.7
 2. 假设**learning rate** = 0.1, 则 **step size** = -5.7 * 0.1 = -0.57 根据  **new intercept = old intercept - step size**,  得到 new intercept = 0.57, 
 3. 代入intercept = 0.57， 得到新的gradient
 4. 重复2-3，直到step size 很小很小，或者max step到达了。


### Step size,
step size: 下一次猜的x会比当前猜的值大多少
step size =  ssr曲线上对应当前x的slope * learning rate
**gradient descend 会在 step size 和0非常接近 或者设置的 max step 到达的时候停止**。


#### How to use Gradient Descent to estimate both the intercept and the slope

 $$
 SSR = (1.4 - (intercept + slope*0.5))^2 + \\
 (1.9 - (intercept + slope*2.3))^2 + \\
 (3.2 - (intercept + slope*2.9))^2
 $$

**goal: 找到 intercept 和 slope的值， 使 SSR 最小** 

**1. 对intercept求导**
 $$
 \frac{d}{d \space intercept} SSR = -2 *(1.4 - (intercept + slope*0.5)) + \\
 (-2) *(1.9 - (intercept + slope*2.3)) + \\
(-2) *(3.2 - (intercept + slope*2.9)) 
$$
在这里，因为我们是在对intercept求导，所以把slope看作是常数


**2. 对slope求导**
 $$
 \frac{d}{d \space slope} SSR = -2 *0.5*(1.4 - (intercept + slope*0.5)) + \\
 (-2) *2.3*(1.9 - (intercept + slope*2.3)) + \\
(-2) *2.9 **(3.2 - (intercept + slope*2.9)) 
$$
在这里，因为我们是在对slope求导，所以把intercept看作是常数


**步骤**：
1. 初始值，设intercept = 0， slope = 1
2. 设 learning rate = 0.01， 计算得到 step size (intercept) = -1.6 * 0.01 = -0.016, step size (slope) = -0.8 * 0.01 = -0.008
3. 得到new intercept =0.016， new slope = 1.008，重复2，3




#### the definition of a gradient ：
多个derivative叫做一个 gradient： when you have two or more derivatives of the same function, they are called a Gradient. 

