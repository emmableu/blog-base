---
title:  Gradient Descent (Best Explanation)
date: 2022-04-10 23:16:42
permalink: /pages/1f2a0f/
categories:
  - 机器学习八股文
  - Machine Learning General
tags:
  - 
---
**这里用一个例子来讲下列知识**
- gradient descent
- step size, learning rate, maximum number of steps


假设这里目标是拟合一个y = wx + i, 假设 weight为一个定值0.64, 这里用gradient descent(梯度下降)来得到一个最佳的i intercept的值。

<img  width="70%" src="https://raw.githubusercontent.com/emmableu/image/master/202204102320021.png"/>


首先，假设intercept为0，设weight为一个定值0.64，目标是计算使 SSR 最小的intercept的值

> Evaluate how good the fit is using SSR: Sum of Squared Residuals, 这个ssr 其实也是一种 loss function
		
e.g., random value for weight = 0.64
		<img  width="70%" src="https://raw.githubusercontent.com/emmableu/image/master/202204102326207.png"/>
		
		
则 residual = 1.4 - 0.32 = 1.1
同理， 得到剩下两个点 SSR = $1.1^2 +0.4^2 +1.3^2 = 3.1 $

所以，在下图的右边的曲线上，标上 (0, 3.1) 这个点，因为当intercept = 0的时候，SSR = 3.1。
同理，用不同的intercept value，可以得到不同的点, 然后找到使SSR最小的intercept的值

<img  width="70%" src="https://raw.githubusercontent.com/emmableu/image/master/202204102329687.png"/>


把上边的0用参数intercept取代，得到
<img width="70%"  src="https://raw.githubusercontent.com/emmableu/image/master/202204102345342.png">

 下一步，也就是体现Least squares v.s. gradient descent 的区别的一步：
 
### Least squares 最小二乘 v.s. gradient descent 梯度下降 的区别
- 如果是最小二乘法 least squares 估计，就是直接对 SSR 关于 intercept 求导， 求导数为0的点
<img  width="70%" src="https://raw.githubusercontent.com/emmableu/image/master/202204102348100.png">

如果是gradiant descent的估计，则是不停地猜intercept的值，一直到这个猜到的值接近最小值
所以
-   最小二乘：一种特殊的求优化问题的解的方法，目的是使得残差的平方和最小，可以通过计算得到。
-   梯度下降：一种通过近似得到优化问题的答案的方法,The benefit is that it can be applied to any objective function, not just squared distances. 前提是得是convex function，


### Step size,
step size: 下一次猜的x会比当前猜的值大多少
step size =  ssr曲线上对应当前x的slope * learning rate
**gradient descend 会在 slope == 0 或者设置的 max step 到达的时候停止**。



