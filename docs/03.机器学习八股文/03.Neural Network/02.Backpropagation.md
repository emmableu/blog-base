---
title: Backpropagation
date: 2022-04-17 19:24:54
permalink: /pages/2a7490/
categories:
  - 机器学习八股文
  - Neural Network
tags:
  - 
---

### The Main Idea of Backpropagation:
1. Calculate derivatives
2. Plug the derivatives into Gradient Descent to optimize parameters

Using an example:
![](https://raw.githubusercontent.com/emmableu/image/master/202204171940141.png)
从理论上来说，backpropagation是从最后一个parameter b3 开始，然后一个一个往前估计 （到 w1，w2）
但是本部分就是讲主要思路，这里只讲怎么估计b3

assume w1,w2,b1,b2,w3,w4 都已知，这里讲怎么估计b3:
根据已知的w1,w2,b1,b2,w3,w4， 得到一条这样的曲线 （具体见 neural network intro章节）：
![](https://raw.githubusercontent.com/emmableu/image/master/202204110114878.png)

已知的数据点(observed data) 有3个： (dosage = low, efficacy = 0; dosage = medium, efficacy = 1; dosage = high, efficacy = 0)
计算b3:
- 首先，设b3为0， 得到一个和上图一样的绿色曲线，根据这个绿色曲线计算SSR (sum of squared residuals)， 得到SSR = (0 - predicted value of dosage = low, when b3=0)^2 + (1 - predicted value of dosage = medium, when b3=0)^2 + (0 - predicted value of dosage = high, when b3=0)^2
- 同理，如果设b3 为1，得到一个往上平移1的绿色曲线，得到 SSR = 7.8

最后，如果设很多b3，会得到一个这样的曲线：
![](https://raw.githubusercontent.com/emmableu/image/master/202204171955390.png)

 
但是，我们这里不画这个曲线来找最低点，而是直接用Gradient Descent 来计算什么b3的值能让SSR最小, 也就是计算 $\frac{d \space SSR}{d \space b_3}$。

根据chain rule：
$$
\frac{d \space SSR}{d \space b_3} = \frac{d \space SSR}{d \space predicted} *  \frac{d \space predicted}{d \space b_3} 
$$
因为 $SSR = \sum_{i=1}^{n=3} (observed_i - predicted_i)^2$，

$$
\frac{d \space SSR}{d \space predicted} = \sum_{i=1}^{n=3} * (-2)*(observed_i - predicted_i)
$$

因为$predicted_i = green \space squiggle_i = blue + orange + b_3$， $\frac{d \space predicted}{d \space b_3}$ = 1,
所以
$$
\frac{d \space SSR}{d \space b_3} = \sum_{i=1}^{n=3} * (-2)*(observed_i - predicted_i)
$$
然后用gradient descent的方法，来估计 $b_3$ 的值 使得 $\frac{d \space SSR}{d \space b_3}$ 接近0

### Backpropagation Details Part 1: Optimizing 3 parameters simultaneously
![](https://raw.githubusercontent.com/emmableu/image/master/202204172020804.png)

assume w3, w4, b3 均未知， 而 w1,b1, w2, b2 已知。
在估计b3 的derivative 时， w3，w4 为常数，所以和上面的算法完全一样，

$\frac{d \space SSR}{d \space b_3} = \sum_{i=1}^{n=3} * (-2)*(observed_i - predicted_i)$ 

估计 w3， w4 时， $x_{1,i} = input_i * w_1 + b_1$,  $x_{2,i} = input_i * w_2 + b_2$

**softmax function**: $y_{1, i} = log(1 + e^x_{1,i})$, $y_{2,i}$ 也是一样的，

$predicted_i = y_{1,i}*w_3 +  y_{2,i}*w_4 + b_3$

因为w4 的估计方法完全一样，下面以 w_3 为例 ：
$$
\frac{d \space SSR}{d \space w_3} = \frac{d \space SSR}{d \space predicted} *  \frac{d \space predicted}{d \space w_3}  = \sum_{i=1}^{n=3} * (-2)*(observed_i - predicted_i) * y_{1,i}
$$

接下来只要对这个derivative 求gradient descent即可。

### Backpropagation Details Part 2: 估计所有参数
因为别的参数在估计的时候看作是常数，w3,w4,b3的derivative 和上面计算的是一样的。现在计算 w1 和 b1
![](https://raw.githubusercontent.com/emmableu/image/master/202204172034642.png)

对于w1,从w1到SSR经过了下面的function：

- $SSR = \sum_{i=1}^{n=3} (observed_i - predicted_i)^2$ ;

- $predicted_i = y_{1,i}*w_3 +  y_{2,i}*w_4 + b_3$ ;

- activation function: $y_{1, i} = log(1 + e^x_{1,i})$  ;

- $x_{1,i} = input_i * w_1 + b_1$ ;


所以 根据 chain rule:
$$
\frac{d \space SSR}{d \space w_3} = \frac{d \space SSR}{d \space predicted} *  \frac{d \space predicted}{d \space b_3}  * \frac{d \space y_{1,i}}{d \space {x_{1,i}}} * \frac{d \space x_1}{d \space w_1}
$$
即
$$
\frac{d \space SSR}{d \space w_3} =\sum_{i=1}^{n=3} * (-2)*(observed_i - predicted_i) * w_3 * \frac{e^x}{1 + e^x} * input_i
$$

类似的，对于 b1:
$$
\frac{d \space SSR}{d \space b_1} =\sum_{i=1}^{n=3} * (-2)*(observed_i - predicted_i) * w_3 * \frac{e^x}{1 + e^x} * 1
$$

估计w2， b2的方法和上面的方法一样。

> 注意：在initialize weights的时候可以使用 standard normal distribution 来取一个random sample



