---
title: Backpropagation
date: 2022-04-17 19:24:54
permalink: /pages/2a7490/
categories:
  - 机器学习八股文
  - Neural Network
tags:
  - 
---

### The Main Idea of Backpropagation:
1. Calculate derivatives
2. Plug the derivatives into Gradient Descent to optimize parameters

Using an example:
![](https://raw.githubusercontent.com/emmableu/image/master/202204171940141.png)
从理论上来说，backpropagation是从最后一个parameter b3 开始，然后一个一个往前估计 （到 w1，w2）
但是本部分就是讲主要思路，这里只讲怎么估计b3

assume w1,w2,b1,b2,w3,w4 都已知，这里讲怎么估计b3:
根据已知的w1,w2,b1,b2,w3,w4， 得到一条这样的曲线 （具体见 neural network intro章节）：
![](https://raw.githubusercontent.com/emmableu/image/master/202204110114878.png)

已知的数据点(observed data) 有3个： (dosage = low, efficacy = 0; dosage = medium, efficacy = 1; dosage = high, efficacy = 0)
计算b3:
- 首先，设b3为0， 得到一个和上图一样的绿色曲线，根据这个绿色曲线计算SSR (sum of squared residuals)， 得到SSR = (0 - predicted value of dosage = low, when b3=0)^2 + (1 - predicted value of dosage = medium, when b3=0)^2 + (0 - predicted value of dosage = high, when b3=0)^2
- 同理，如果设b3 为1，得到一个往上平移1的绿色曲线，得到 SSR = 7.8

最后，如果设很多b3，会得到一个这样的曲线：
![](https://raw.githubusercontent.com/emmableu/image/master/202204171955390.png)

 
但是，我们这里不画这个曲线来找最低点，而是直接用Gradient Descent 来计算什么b3的值能让SSR最小, 也就是计算 $\frac{d \space SSR}{d \space b_3}$。

根据chain rule：
$$
\frac{d \space SSR}{d \space b_3} = \frac{d \space SSR}{d \space predicted} *  \frac{d \space predicted}{d \space b_3} 
$$
因为 $SSR = \sum_{i=1}^{n=3} (observed_i - predicted_i)^2$，

$$
\frac{d \space SSR}{d \space predicted} = \sum_{i=1}^{n=3} * (-2)*(observed_i - predicted_i)
$$

因为$predicted_i = green \space squiggle_i = blue + orange + b_3$， $\frac{d \space predicted}{d \space b_3}$ = 1,
所以
$$
\frac{d \space SSR}{d \space b_3} = \sum_{i=1}^{n=3} * (-2)*(observed_i - predicted_i)
$$
然后用gradient descent的方法，来估计 $b_3$ 的值 使得 $\frac{d \space SSR}{d \space b_3}$ 接近0

### Backpropagation Details Part 1: Optimizing 3 parameters simultaneously
![](https://raw.githubusercontent.com/emmableu/image/master/202204172020804.png)

assume w3, w4, b3 均未知， 而 w1,b1, w2, b2 已知。
在估计b3 的derivative 时， w3，w4 为常数，所以和上面的算法完全一样，

$\frac{d \space SSR}{d \space b_3} = \sum_{i=1}^{n=3} * (-2)*(observed_i - predicted_i)$ 

估计 w3， w4 时， $x_{1,i} = input_i * w_1 + b_1$,  $x_{2,i} = input_i * w_2 + b_2$

**softmax function**: $y_{1, i} = log(1 + e^x_{1,i})$, $y_{2,i}$ 也是一样的，

$predicted_i = y_{1,i}*w_3 +  y_{2,i}*w_4 + b_3$

因为w4 的估计方法完全一样，下面以 w_3 为例 ：
$$
\frac{d \space SSR}{d \space w_3} = \frac{d \space SSR}{d \space predicted} *  \frac{d \space predicted}{d \space w_3}  = \sum_{i=1}^{n=3} * (-2)*(observed_i - predicted_i) * y_{1,i}
$$

接下来只要对这个derivative 求gradient descent即可。

### Backpropagation Details Part 2: 估计所有参数
因为别的参数在估计的时候看作是常数，w3,w4,b3的derivative 和上面计算的是一样的。现在计算 w1 和 b1
![](https://raw.githubusercontent.com/emmableu/image/master/202204172034642.png)

对于w1,从w1到SSR经过了下面的function：

- $SSR = \sum_{i=1}^{n=3} (observed_i - predicted_i)^2$ ;

- $predicted_i = y_{1,i}*w_3 +  y_{2,i}*w_4 + b_3$ ;

- activation function: $y_{1, i} = log(1 + e^x_{1,i})$  ;

- $x_{1,i} = input_i * w_1 + b_1$ ;


所以 根据 chain rule:
$$
\frac{d \space SSR}{d \space w_3} = \frac{d \space SSR}{d \space predicted} *  \frac{d \space predicted}{d \space b_3}  * \frac{d \space y_{1,i}}{d \space {x_{1,i}}} * \frac{d \space x_1}{d \space w_1}
$$
即
$$
\frac{d \space SSR}{d \space w_3} =\sum_{i=1}^{n=3} * (-2)*(observed_i - predicted_i) * w_3 * \frac{e^x}{1 + e^x} * input_i
$$

类似的，对于 b1:
$$
\frac{d \space SSR}{d \space b_1} =\sum_{i=1}^{n=3} * (-2)*(observed_i - predicted_i) * w_3 * \frac{e^x}{1 + e^x} * 1
$$

估计w2， b2的方法和上面的方法一样。

> 注意：在initialize weights的时候可以使用 standard normal distribution 来取一个random sample



## Implement Backpropagation from scratch


### Requirements
```python
# 最后一个是y
dataset = [[2.7810836,2.550537003,0],
	[1.465489372,2.362125076,0],
	[3.396561688,4.400293529,0],
	[1.38807019,1.850220317,0],
	[3.06407232,3.005305973,0],
	[7.627531214,2.759262235,1],
	[5.332441248,2.088626775,1],
	[6.922596716,1.77106367,1],
	[8.675418651,-0.242068655,1],
	[7.673756466,3.508563011,1]]
```

- 1 个 hidden layer
- 2 个 x attribute
- 2 个output type (用 y = 0， y = 1 表示)， 其实这个写成1个也可以，但是2个更容易generalize


![](https://raw.githubusercontent.com/emmableu/image/master/202209151223924.png)






```python
from math import exp
from random import seed
from random import random

# Initialize a network
def initialize_network(n_inputs, n_hidden, n_outputs):
	network = list()
	hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]
	network.append(hidden_layer)
	output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]
	network.append(output_layer)
	return network

# Calculate neuron activation for an input
def activate(weights, inputs):
	activation = weights[-1]
	for i in range(len(weights)-1):
		activation += weights[i] * inputs[i]
	return activation

# Transfer neuron activation
def transfer(activation):
	return 1.0 / (1.0 + exp(-activation))

# Forward propagate input to a network output
def forward_propagate(network, row):
	inputs = row
	for layer in network:
		new_inputs = []
		for neuron in layer:
			activation = activate(neuron['weights'], inputs)
			neuron['output'] = transfer(activation)
			new_inputs.append(neuron['output'])
		inputs = new_inputs
	return inputs

# Calculate the derivative of an neuron output
def transfer_derivative(output):
	return output * (1.0 - output)

# Backpropagate error and store in neurons
def backward_propagate_error(network, expected):
	for i in reversed(range(len(network))):
		layer = network[i]
		errors = list()
		if i != len(network)-1:
			for j in range(len(layer)):
				error = 0.0
				for neuron in network[i + 1]:
					error += (neuron['weights'][j] * neuron['delta'])
				errors.append(error)
		else:
			for j in range(len(layer)):
				neuron = layer[j]
				errors.append(neuron['output'] - expected[j])
		for j in range(len(layer)):
			neuron = layer[j]
			neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])

# Update network weights with error
def update_weights(network, row, l_rate):
	for i in range(len(network)):
		inputs = row[:-1]
		if i != 0:
			inputs = [neuron['output'] for neuron in network[i - 1]]
		for neuron in network[i]:
			for j in range(len(inputs)):
				neuron['weights'][j] -= l_rate * neuron['delta'] * inputs[j]
			neuron['weights'][-1] -= l_rate * neuron['delta']

# Train a network for a fixed number of epochs
def train_network(network, train, l_rate, n_epoch, n_outputs):
	for epoch in range(n_epoch):
		sum_error = 0
		for row in train:
			outputs = forward_propagate(network, row)
			expected = [0 for i in range(n_outputs)]
			expected[row[-1]] = 1
			sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])
			backward_propagate_error(network, expected)
			update_weights(network, row, l_rate)
		print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))

# Test training backprop algorithm
seed(1)
dataset = [[2.7810836,2.550537003,0],
	[1.465489372,2.362125076,0],
	[3.396561688,4.400293529,0],
	[1.38807019,1.850220317,0],
	[3.06407232,3.005305973,0],
	[7.627531214,2.759262235,1],
	[5.332441248,2.088626775,1],
	[6.922596716,1.77106367,1],
	[8.675418651,-0.242068655,1],
	[7.673756466,3.508563011,1]]
n_inputs = len(dataset[0]) - 1
n_outputs = len(set([row[-1] for row in dataset]))
network = initialize_network(n_inputs, 2, n_outputs)
train_network(network, dataset, 0.5, 20, n_outputs)
for layer in network:
	print(layer)
```

Running the example first prints the sum squared error each training epoch. We can see a trend of this error decreasing with each epoch.

Once trained, the network is printed, showing the learned weights. Also still in the network are output and delta values that can be ignored. We could update our training function to delete these data if we wanted.


```
>epoch=0, lrate=0.500, error=6.350
>epoch=1, lrate=0.500, error=5.531
>epoch=2, lrate=0.500, error=5.221
>epoch=3, lrate=0.500, error=4.951
>epoch=4, lrate=0.500, error=4.519
>epoch=5, lrate=0.500, error=4.173
>epoch=6, lrate=0.500, error=3.835
>epoch=7, lrate=0.500, error=3.506
>epoch=8, lrate=0.500, error=3.192
>epoch=9, lrate=0.500, error=2.898
>epoch=10, lrate=0.500, error=2.626
>epoch=11, lrate=0.500, error=2.377
>epoch=12, lrate=0.500, error=2.153
>epoch=13, lrate=0.500, error=1.953
>epoch=14, lrate=0.500, error=1.774
>epoch=15, lrate=0.500, error=1.614
>epoch=16, lrate=0.500, error=1.472
>epoch=17, lrate=0.500, error=1.346
>epoch=18, lrate=0.500, error=1.233
>epoch=19, lrate=0.500, error=1.132
[{'weights': [-1.4688375095432327, 1.850887325439514, 1.0858178629550297], 'output': 0.029980305604426185, 'delta': 0.0059546604162323625}, {'weights': [0.37711098142462157, -0.0625909894552989, 0.2765123702642716], 'output': 0.9456229000211323, 'delta': -0.0026279652850863837}]
[{'weights': [2.515394649397849, -0.3391927502445985, -0.9671565426390275], 'output': 0.23648794202357587, 'delta': 0.04270059278364587}, {'weights': [-2.5584149848484263, 1.0036422106209202, 0.42383086467582715], 'output': 0.7790535202438367, 'delta': -0.03803132596437354}]
```


### 5. Predict

Making predictions with a trained neural network is easy enough.

We have already seen how to forward-propagate an input pattern to get an output. This is all we need to do to make a prediction. We can use the output values themselves directly as the probability of a pattern belonging to each output class.

It may be more useful to turn this output back into a crisp class prediction. We can do this by selecting the class value with the larger probability. This is also called the [arg max function](https://en.wikipedia.org/wiki/Arg_max).

Below is a function named **predict()** that implements this procedure. It returns the index in the network output that has the largest probability. It assumes that class values have been converted to integers starting at 0.

```python
# Make a prediction with a network
def predict(network, row):
	outputs = forward_propagate(network, row)
	return outputs.index(max(outputs))
```


We can put this together with our code above for forward propagating input and with our small contrived dataset to test making predictions with an already-trained network. The example hardcodes a network trained from the previous step.

The complete example is listed below.

```python
from math import exp

# Calculate neuron activation for an input
def activate(weights, inputs):
	activation = weights[-1]
	for i in range(len(weights)-1):
		activation += weights[i] * inputs[i]
	return activation

# Transfer neuron activation
def transfer(activation):
	return 1.0 / (1.0 + exp(-activation))

# Forward propagate input to a network output
def forward_propagate(network, row):
	inputs = row
	for layer in network:
		new_inputs = []
		for neuron in layer:
			activation = activate(neuron['weights'], inputs)
			neuron['output'] = transfer(activation)
			new_inputs.append(neuron['output'])
		inputs = new_inputs
	return inputs

# Make a prediction with a network
def predict(network, row):
	outputs = forward_propagate(network, row)
	return outputs.index(max(outputs))

# Test making predictions with the network
dataset = [[2.7810836,2.550537003,0],
	[1.465489372,2.362125076,0],
	[3.396561688,4.400293529,0],
	[1.38807019,1.850220317,0],
	[3.06407232,3.005305973,0],
	[7.627531214,2.759262235,1],
	[5.332441248,2.088626775,1],
	[6.922596716,1.77106367,1],
	[8.675418651,-0.242068655,1],
	[7.673756466,3.508563011,1]]
network = [[{'weights': [-1.482313569067226, 1.8308790073202204, 1.078381922048799]}, {'weights': [0.23244990332399884, 0.3621998343835864, 0.40289821191094327]}],
	[{'weights': [2.5001872433501404, 0.7887233511355132, -1.1026649757805829]}, {'weights': [-2.429350576245497, 0.8357651039198697, 1.0699217181280656]}]]
for row in dataset:
	prediction = predict(network, row)
	print('Expected=%d, Got=%d' % (row[-1], prediction))
```

Running the example prints the expected output for each record in the training dataset, followed by the crisp prediction made by the network.

It shows that the network achieves 100% accuracy on this small dataset.

```
Expected=0, Got=0
Expected=0, Got=0
Expected=0, Got=0
Expected=0, Got=0
Expected=0, Got=0
Expected=1, Got=1
Expected=1, Got=1
Expected=1, Got=1
Expected=1, Got=1
Expected=1, Got=1
```