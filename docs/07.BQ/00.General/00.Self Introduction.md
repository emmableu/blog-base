---
title: Self Introduction
date: 2022-08-31 00:03:30
permalink: /pages/b6b3e3/
categories:
  - BQ
tags:
  - 
---

## Docs To Open
- [Meta Intern Presentation PDF](https://drive.google.com/file/d/1y_CQtUKQ7BSh6zZztLV1EjDMAxYok-Ar/view)
- Oral prelim presentation (last deck)
- my own feature extraction comparison paper slides
- 

## Self Introduction
### general Version
I'm a 4th-year Ph.D. student at NC State University (expected graduation in Jan 2023).

In my PhD work I research  ways to generate intelligent support (e.g., code examples, hints) for kids and colleage students to learn programming. For example, when they are working on designing and building a programming artifact, for example, a pong or a mario game, can we auto-generate code examples or hints that helps them achieve their goals. 

What I'm excited about is that kids can use my tool to make things *open_ended*
- build things that they themselves feel meaningful
- that can enpower them
- make them feel confident, interested in CS, feel that they could use CS as a tool to build things that matters to them the most. 
- and I could provide support to them during the process. 


This summer I also did an internship as an Machine Learning Engineer in Meta, so I have a little bit of industry experience on how to apply ML methods in, for example, the feed ranking system that I was working on.

But in school I write papers mostly in the HCI - human computer interaction area, meaning that all of the tools or softwares I built is focused on people - so they ends up being used by programming students - usually college students in computer science classrooms, but also some middle schoolers and high schoolers. So, yeah, I'm mainly interested in building things that can be used by real people, or make those things better. Like I did in my internship. 


### ML Version
I'm a 4th-year Ph.D. student at NC State University.

I do learning analytics and HCI research, so
- how to detect students' cognitive activities based on their coding problem solutions
- how to generate intelligent support (e.g., code examples, hints) for kids and colleage students to learn programming. 

- For example, when they are working on designing and building a programming artifact, for example, a pong or a mario game,
	- can we understand what they are struggling at
	- can we auto-generate code examples or hints that helps them achieve their goals. 

the first part of my phd time (first 2 years) is very ML focused, I researched ways
- extracting code features from complex project
- predicting the meaning of a sub-component of a programming code
- clustering types of misconceptions from students' programming projects. 


Afterwards I worked on traditional code analysis, parsing, compiler related work, and also pure HCI work, such as building real softwares and running human studies.

the focus of my research right now is to support kids doing *open_ended* programming
- build things that they themselves feel meaningful
- that can enpower them
- make them feel confident, interested in CS, to feel that they could use CS as a tool to build things that matters to them the most. 
- and I could provide support to them during the process. 





## Summer Intern Project

During my internship at Meta, I worked on improving our team's feed ranking model - the goal of this model, which is called PNUF (Personalized negative user feedback) is to improve users'  experience on the news feeds by reducing, or demoting contents 
- low quality content
- or content the users do not like. - i.e., contents that they have chose to hide, x-out, or report before, we will demote similar stuff when they might see them again. 

![](https://raw.githubusercontent.com/emmableu/image/master/202208311041637.png)

So the goal of my project was pretty open-ended, I needed to find issues on the current model first, and then find ways to address them. 

![](https://raw.githubusercontent.com/emmableu/image/master/202209262139218.png)

(overall ROC_AUC : 85%)
![](https://raw.githubusercontent.com/emmableu/image/master/202209262141896.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209262142145.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209262143677.png)

**overall ROC_AUC**: 5% increase, 0.85 -> 0.90
**ROC_AUC on cold users** (users with < 1year facebookage): 8% increase, 0.80 -> 0.88
**ROC_AUC users with less frequent NUF events** (users with < 1year facebookage): 7% increase, 0.82 -> 0.89

in 7 days, decreased dislike through rate (dislike over percentage view point view, dislike includes the 9 events I've mentioned) by 4.8%

as a result increased the like through rate by 3%


[use the presentation if needed](https://drive.google.com/file/d/1y_CQtUKQ7BSh6zZztLV1EjDMAxYok-Ar/view)



## Code2Vec Project Experience

Tell me about a deep learning project you've worked on

**motivation**: 

in my area of research, learning analytics, there's also a big research space for applying ML -     
learning analytics means

understanding students, -  
including understanding students' thoughts, ideas, and misconceptions, which sometimes resembles a lot with NLP tasks - deriving meanings from texts, in our case, codes

So the project I've worked on, I got the idea from the code2vec paper published in 2019, it is about learning code embeddings and use them to predict function names

I saw this idea and I wanted to apply it in my work - in novice programming tasks.  Because:
- it is at that time the state-of-the-art work in predicting method names based on functions.
- It uses simple but interesting feature extraction - AST tree, leaf-to-leaf paths
- It uses attention mechanism to find the part of the code that's most predictive of its function name. For example, in a sorting algorithm,...
- It creates code-embeddings in the layer before the attention layer. While the paper did not dig deep into that I believe just like word embeddings, those code embeddings could be rather useful - like the misconception clustering task that I have mentioned. 

But different from code2vec, 
- we had much less data, code2vec has about 2 million pieces of code as training dataset, we have 207 from 4 semesters.
- we don't predict method names - students write script-like functions that we only has labeled as right or wrong. 


input: 207 student programs, AST leaf-to-leaf paths

output: right/wrong

![](https://raw.githubusercontent.com/emmableu/image/master/202209262256120.png)

![](https://raw.githubusercontent.com/emmableu/image/master/202209270023838.png)


F1 score: 81%, 30% higher than SVM, 12% higher than traditional NN

misconceptions

embedding -> PCA -> k-means clustering -> expert interpretations

some of the big clusters include: 
	- cannot detect by program execution - these are logic errors. 
- variable misuse - supposed to use x, but uses y
- index off by 1 error, so when you iterative through indexes, it is easy to loop one index out of the loop, or return one index before or after the target index, for example, in a binary search problem
- don't know how to write a loop - so unroll a loop by repeating code inside of the loop 

